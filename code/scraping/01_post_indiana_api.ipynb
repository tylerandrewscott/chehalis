{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ea9bcb-dbf8-4241-a25c-b3beeee40e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://secure.in.gov/apps/idoa/contractsearch/api/contracts/search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wvabzfj3x1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for incremental update\n",
    "def load_existing_contracts(filepath):\n",
    "    \"\"\"Load existing contracts and return both data and set of existing records (as tuples)\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Create a set of tuples representing unique records\n",
    "            # Using key fields that together identify a unique record\n",
    "            existing_records = set()\n",
    "            for record in data:\n",
    "                if isinstance(record, dict):\n",
    "                    # Create a tuple of key fields that uniquely identify a record\n",
    "                    record_tuple = (\n",
    "                        record.get('id'),\n",
    "                        record.get('pdfUrl'),\n",
    "                        record.get('actionType'),\n",
    "                        record.get('vendorName'),\n",
    "                        record.get('amount'),\n",
    "                        record.get('startDate'),\n",
    "                        record.get('endDate')\n",
    "                    )\n",
    "                    existing_records.add(record_tuple)\n",
    "            \n",
    "            print(f\"Successfully loaded {len(data)} records from {filepath}\")\n",
    "            print(f\"Found {len(existing_records)} unique records\")\n",
    "            return data, existing_records\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error reading JSON file: {e}\")\n",
    "            print(\"File may be corrupted. Returning empty data.\")\n",
    "            return [], set()\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error reading file: {e}\")\n",
    "            return [], set()\n",
    "    else:\n",
    "        print(f\"File {filepath} does not exist. Will create new file.\")\n",
    "        return [], set()\n",
    "\n",
    "def get_record_tuple(record):\n",
    "    \"\"\"Convert a record dict to a tuple for comparison\"\"\"\n",
    "    return (\n",
    "        record.get('id'),\n",
    "        record.get('pdfUrl'),\n",
    "        record.get('actionType'),\n",
    "        record.get('vendorName'),\n",
    "        record.get('amount'),\n",
    "        record.get('startDate'),\n",
    "        record.get('endDate')\n",
    "    )\n",
    "\n",
    "def scrape_all_new_records(url, existing_records, contract_type_flags=None, page_size=1000):\n",
    "    \"\"\"\n",
    "    Scrape all pages and collect only new records that don't exist in the dataset.\n",
    "    \"\"\"\n",
    "    new_records = []\n",
    "    page = 1\n",
    "    \n",
    "    # Get first page to determine total pages\n",
    "    payload = {\n",
    "        \"pageNumber\": page,\n",
    "        \"pageSize\": page_size\n",
    "    }\n",
    "    \n",
    "    if contract_type_flags is not None:\n",
    "        payload[\"contractTypeFlags\"] = contract_type_flags\n",
    "    \n",
    "    # Make initial request\n",
    "    resp = requests.post(url, json=payload)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Error: {resp.status_code}\")\n",
    "        return new_records\n",
    "    \n",
    "    js_result = resp.json()\n",
    "    pagination = js_result.get('pagination', {})\n",
    "    total_pages = pagination.get('totalPages', 0)\n",
    "    \n",
    "    print(f\"Total pages to scan: {total_pages}\")\n",
    "    \n",
    "    # Process all pages\n",
    "    while page <= total_pages:\n",
    "        if page > 1:  # We already have page 1 data\n",
    "            payload[\"pageNumber\"] = page\n",
    "            resp = requests.post(url, json=payload)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"Error on page {page}: {resp.status_code}\")\n",
    "                page += 1\n",
    "                continue\n",
    "            js_result = resp.json()\n",
    "        \n",
    "        results = js_result.get('results', [])\n",
    "        \n",
    "        # Check each record\n",
    "        new_on_page = 0\n",
    "        for record in results:\n",
    "            record_tuple = get_record_tuple(record)\n",
    "            if record_tuple not in existing_records:\n",
    "                new_records.append(record)\n",
    "                new_on_page += 1\n",
    "        \n",
    "        print(f\"Page {page}/{total_pages}: Found {new_on_page} new records\")\n",
    "        \n",
    "        page += 1\n",
    "        if page <= total_pages:\n",
    "            time.sleep(0.25)  # Rate limiting\n",
    "    \n",
    "    print(f\"\\nTotal new records found: {len(new_records)}\")\n",
    "    return new_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ogxfyvb2t4q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Set CLOBBER = True to scrape everything and overwrite existing file\n",
    "# Set CLOBBER = False to only scrape until existing IDs are found and combine with existing data\n",
    "CLOBBER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "q66jpob308h",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_contracts(url, contract_type_flags=None, start_date='2000-01-01T00:00:00.0000000', page_size=1000):\n",
    "    \"\"\"Scrape all contracts from the start date\"\"\"\n",
    "    # Build initial request\n",
    "    payload = {\n",
    "        \"pageNumber\": 1,\n",
    "        \"pageSize\": page_size,\n",
    "        \"startDate\": start_date\n",
    "    }\n",
    "    \n",
    "    if contract_type_flags is not None:\n",
    "        payload[\"contractTypeFlags\"] = contract_type_flags\n",
    "    \n",
    "    # Get first page\n",
    "    resp = requests.post(url, json=payload)\n",
    "    js_result = resp.json()\n",
    "    paginate = js_result['pagination']\n",
    "    total_pages = paginate['totalPages']\n",
    "    all_records = js_result['results']\n",
    "    \n",
    "    print(f\"Total pages to scrape: {total_pages}\")\n",
    "    \n",
    "    # Get remaining pages\n",
    "    for p in range(2, total_pages + 1):\n",
    "        print(f\"Processing page {p}/{total_pages}\")\n",
    "        payload[\"pageNumber\"] = p\n",
    "        resp = requests.post(url, json=payload)\n",
    "        js_result = resp.json()\n",
    "        all_records.extend(js_result['results'])\n",
    "        time.sleep(0.25)\n",
    "    \n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "681f4a0b-f300-4ca1-8f2c-0fd6e3d5412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 181000 records from ../../data/raw/indiana_contracts.json\n",
      "Found 180445 unique records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Correct usage:\n",
    "filepath = '../../data/raw/indiana_contracts.json'\n",
    "data, existing_ids = load_existing_contracts(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "jzb0ihh0739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 181000 records from ../../data/raw/indiana_contracts.json\n",
      "Found 180445 unique records\n",
      "\n",
      "First contract structure:\n",
      "Keys: ['actionType', 'agencyName', 'amendment', 'amount', 'businessUnit', 'contractTypeFlags', 'id', 'endDate', 'pdfUrl', 'startDate', 'vendorName', 'zipCode', 'approvals']\n",
      "\n",
      "Example record:\n",
      "ID: A70-5-2020\n",
      "PDF URL: https://contracts.idoa.in.gov/idoacontractsweb/PUBLIC/10778-001.pdf\n",
      "Vendor: GENESIS SYSTEMS INC\n",
      "Amount: N/A\n"
     ]
    }
   ],
   "source": [
    "# Example: How to use load_existing_contracts function correctly\n",
    "# The function returns a tuple: (data, existing_urls)\n",
    "\n",
    "# Correct usage:\n",
    "filepath = '../../data/raw/indiana_contracts.json'\n",
    "data, existing_urls = load_existing_contracts(filepath)\n",
    "\n",
    "# If you want to inspect the data structure:\n",
    "if data:\n",
    "    print(\"\\nFirst contract structure:\")\n",
    "    print(f\"Keys: {list(data[0].keys())}\")\n",
    "    print(f\"\\nExample record:\")\n",
    "    print(f\"ID: {data[0].get('id')}\")\n",
    "    print(f\"PDF URL: {data[0].get('pdfUrl')}\")\n",
    "    print(f\"Vendor: {data[0].get('vendorName')}\")\n",
    "    print(f\"Amount: ${data[0].get('amount'):,.2f}\" if data[0].get('amount') else \"Amount: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b9f706-fd25-4d7d-9e16-792792bc3a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental mode: Scanning for new contracts across all pages...\n",
      "Successfully loaded 197991 records from ../../data/raw/indiana_contracts.json\n",
      "Found 197263 unique records\n",
      "Scanning all pages for new contracts...\n",
      "Total pages to scan: 195\n",
      "Page 1/195: Found 0 new records\n",
      "Page 2/195: Found 0 new records\n",
      "Page 3/195: Found 2 new records\n",
      "Page 4/195: Found 9 new records\n",
      "Page 5/195: Found 0 new records\n",
      "Page 6/195: Found 0 new records\n",
      "Page 7/195: Found 0 new records\n",
      "Page 8/195: Found 1 new records\n",
      "Page 9/195: Found 0 new records\n",
      "Page 10/195: Found 0 new records\n",
      "Page 11/195: Found 0 new records\n",
      "Page 12/195: Found 0 new records\n",
      "Page 13/195: Found 12 new records\n",
      "Page 14/195: Found 24 new records\n",
      "Page 15/195: Found 5 new records\n",
      "Page 16/195: Found 4 new records\n",
      "Page 17/195: Found 0 new records\n",
      "Page 18/195: Found 0 new records\n",
      "Page 19/195: Found 0 new records\n",
      "Page 20/195: Found 0 new records\n",
      "Page 21/195: Found 0 new records\n",
      "Page 22/195: Found 1 new records\n",
      "Page 23/195: Found 0 new records\n",
      "Page 24/195: Found 3 new records\n",
      "Page 25/195: Found 6 new records\n",
      "Page 26/195: Found 5 new records\n",
      "Page 27/195: Found 0 new records\n",
      "Page 28/195: Found 0 new records\n",
      "Page 29/195: Found 0 new records\n",
      "Page 30/195: Found 0 new records\n",
      "Page 31/195: Found 0 new records\n",
      "Page 32/195: Found 0 new records\n",
      "Page 33/195: Found 0 new records\n",
      "Page 34/195: Found 0 new records\n",
      "Page 35/195: Found 0 new records\n",
      "Page 36/195: Found 0 new records\n",
      "Page 37/195: Found 0 new records\n",
      "Page 38/195: Found 0 new records\n",
      "Page 39/195: Found 0 new records\n",
      "Page 40/195: Found 0 new records\n",
      "Page 41/195: Found 0 new records\n",
      "Page 42/195: Found 0 new records\n",
      "Page 43/195: Found 2 new records\n",
      "Page 44/195: Found 5 new records\n",
      "Page 45/195: Found 4 new records\n",
      "Page 46/195: Found 1 new records\n",
      "Page 47/195: Found 0 new records\n",
      "Page 48/195: Found 0 new records\n",
      "Page 49/195: Found 0 new records\n",
      "Page 50/195: Found 0 new records\n",
      "Page 51/195: Found 0 new records\n",
      "Page 52/195: Found 0 new records\n",
      "Page 53/195: Found 0 new records\n",
      "Page 54/195: Found 0 new records\n",
      "Page 55/195: Found 0 new records\n",
      "Page 56/195: Found 0 new records\n",
      "Page 57/195: Found 0 new records\n",
      "Page 58/195: Found 2 new records\n",
      "Page 59/195: Found 0 new records\n",
      "Page 60/195: Found 0 new records\n",
      "Page 61/195: Found 0 new records\n",
      "Page 62/195: Found 0 new records\n",
      "Page 63/195: Found 0 new records\n",
      "Page 64/195: Found 0 new records\n",
      "Page 65/195: Found 0 new records\n",
      "Page 66/195: Found 0 new records\n",
      "Page 67/195: Found 0 new records\n",
      "Page 68/195: Found 1 new records\n",
      "Page 69/195: Found 6 new records\n",
      "Page 70/195: Found 4 new records\n",
      "Page 71/195: Found 5 new records\n",
      "Page 72/195: Found 24 new records\n",
      "Page 73/195: Found 0 new records\n",
      "Page 74/195: Found 0 new records\n",
      "Page 75/195: Found 1 new records\n",
      "Page 76/195: Found 0 new records\n",
      "Page 77/195: Found 0 new records\n",
      "Page 78/195: Found 4 new records\n",
      "Page 79/195: Found 0 new records\n",
      "Page 80/195: Found 0 new records\n",
      "Page 81/195: Found 3 new records\n",
      "Page 82/195: Found 0 new records\n",
      "Page 83/195: Found 2 new records\n",
      "Page 84/195: Found 2 new records\n",
      "Page 85/195: Found 0 new records\n",
      "Page 86/195: Found 0 new records\n",
      "Page 87/195: Found 0 new records\n",
      "Page 88/195: Found 1 new records\n",
      "Page 89/195: Found 6 new records\n",
      "Page 90/195: Found 0 new records\n",
      "Page 91/195: Found 0 new records\n",
      "Page 92/195: Found 3 new records\n",
      "Page 93/195: Found 0 new records\n",
      "Page 94/195: Found 0 new records\n",
      "Page 95/195: Found 0 new records\n",
      "Page 96/195: Found 0 new records\n",
      "Page 97/195: Found 0 new records\n",
      "Page 98/195: Found 0 new records\n",
      "Page 99/195: Found 0 new records\n",
      "Page 100/195: Found 2 new records\n",
      "Page 101/195: Found 55 new records\n",
      "Page 102/195: Found 51 new records\n",
      "Page 103/195: Found 5 new records\n",
      "Page 104/195: Found 1 new records\n",
      "Page 105/195: Found 0 new records\n",
      "Page 106/195: Found 0 new records\n",
      "Page 107/195: Found 0 new records\n",
      "Page 108/195: Found 0 new records\n",
      "Page 109/195: Found 0 new records\n",
      "Page 110/195: Found 0 new records\n",
      "Page 111/195: Found 0 new records\n",
      "Page 112/195: Found 0 new records\n",
      "Page 113/195: Found 0 new records\n",
      "Page 114/195: Found 0 new records\n",
      "Page 115/195: Found 0 new records\n",
      "Page 116/195: Found 0 new records\n",
      "Page 117/195: Found 0 new records\n",
      "Page 118/195: Found 0 new records\n",
      "Page 119/195: Found 1 new records\n",
      "Page 120/195: Found 0 new records\n",
      "Page 121/195: Found 0 new records\n",
      "Page 122/195: Found 3 new records\n",
      "Page 123/195: Found 2 new records\n",
      "Page 124/195: Found 6 new records\n",
      "Page 125/195: Found 8 new records\n",
      "Page 126/195: Found 28 new records\n",
      "Page 127/195: Found 3 new records\n",
      "Page 128/195: Found 0 new records\n",
      "Page 129/195: Found 0 new records\n",
      "Page 130/195: Found 0 new records\n",
      "Page 131/195: Found 0 new records\n",
      "Page 132/195: Found 0 new records\n",
      "Page 133/195: Found 0 new records\n",
      "Page 134/195: Found 4 new records\n",
      "Page 135/195: Found 1 new records\n",
      "Page 136/195: Found 12 new records\n",
      "Page 137/195: Found 0 new records\n",
      "Page 138/195: Found 0 new records\n",
      "Page 139/195: Found 0 new records\n",
      "Page 140/195: Found 0 new records\n",
      "Page 141/195: Found 0 new records\n",
      "Page 142/195: Found 0 new records\n",
      "Page 143/195: Found 0 new records\n",
      "Page 144/195: Found 1 new records\n",
      "Page 145/195: Found 2 new records\n",
      "Page 146/195: Found 1 new records\n",
      "Page 147/195: Found 2 new records\n",
      "Page 148/195: Found 0 new records\n",
      "Page 149/195: Found 2 new records\n",
      "Page 150/195: Found 19 new records\n",
      "Page 151/195: Found 0 new records\n",
      "Page 152/195: Found 1 new records\n",
      "Page 153/195: Found 0 new records\n",
      "Page 154/195: Found 0 new records\n",
      "Page 155/195: Found 7 new records\n",
      "Page 156/195: Found 0 new records\n",
      "Page 157/195: Found 0 new records\n",
      "Page 158/195: Found 0 new records\n",
      "Page 159/195: Found 0 new records\n",
      "Page 160/195: Found 0 new records\n",
      "Page 161/195: Found 0 new records\n",
      "Page 162/195: Found 0 new records\n",
      "Page 163/195: Found 0 new records\n",
      "Page 164/195: Found 0 new records\n",
      "Page 165/195: Found 0 new records\n",
      "Page 166/195: Found 0 new records\n",
      "Page 167/195: Found 0 new records\n",
      "Page 168/195: Found 0 new records\n",
      "Page 169/195: Found 3 new records\n",
      "Page 170/195: Found 4 new records\n",
      "Page 171/195: Found 3 new records\n",
      "Page 172/195: Found 0 new records\n",
      "Page 173/195: Found 3 new records\n",
      "Page 174/195: Found 1 new records\n",
      "Page 175/195: Found 2 new records\n",
      "Page 176/195: Found 0 new records\n",
      "Page 177/195: Found 2 new records\n",
      "Page 178/195: Found 1 new records\n",
      "Page 179/195: Found 7 new records\n",
      "Page 180/195: Found 2 new records\n",
      "Page 181/195: Found 0 new records\n",
      "Page 182/195: Found 6 new records\n",
      "Page 183/195: Found 1 new records\n",
      "Page 184/195: Found 9 new records\n",
      "Page 185/195: Found 5 new records\n",
      "Page 186/195: Found 4 new records\n",
      "Page 187/195: Found 4 new records\n",
      "Page 188/195: Found 6 new records\n",
      "Page 189/195: Found 22 new records\n",
      "Page 190/195: Found 8 new records\n",
      "Page 191/195: Found 4 new records\n",
      "Page 192/195: Found 9 new records\n",
      "Page 193/195: Found 51 new records\n",
      "Page 194/195: Found 2 new records\n",
      "Page 195/195: Found 0 new records\n",
      "\n",
      "Total new records found: 524\n",
      "Found 524 new contracts\n",
      "Total contracts after update: 198515\n"
     ]
    }
   ],
   "source": [
    "# Update all contracts\n",
    "all_contracts_file = '../../data/raw/indiana_contracts.json'\n",
    "\n",
    "if CLOBBER:\n",
    "    print(\"CLOBBER mode: Scraping all contracts and overwriting existing file...\")\n",
    "    all_data = scrape_all_contracts(url)\n",
    "    print(f\"Scraped {len(all_data)} total contracts\")\n",
    "    \n",
    "    # Save all data (overwrite)\n",
    "    with open(all_contracts_file, 'w') as write_file:\n",
    "        json.dump(all_data, write_file)\n",
    "        \n",
    "    print(f\"Saved {len(all_data)} contracts (overwrote existing file)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Incremental mode: Scanning for new contracts across all pages...\")\n",
    "    \n",
    "    # Load existing data\n",
    "    existing_data, existing_records = load_existing_contracts(all_contracts_file)\n",
    "    \n",
    "    # Scrape all pages looking for new records\n",
    "    if existing_records:\n",
    "        print(\"Scanning all pages for new contracts...\")\n",
    "        new_records = scrape_all_new_records(url, existing_records)\n",
    "        print(f\"Found {len(new_records)} new contracts\")\n",
    "        \n",
    "        # Combine new and existing data (new records first)\n",
    "        updated_data = new_records + existing_data\n",
    "        \n",
    "        # Save updated data\n",
    "        with open(all_contracts_file, 'w') as write_file:\n",
    "            json.dump(updated_data, write_file)\n",
    "            \n",
    "        print(f\"Total contracts after update: {len(updated_data)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No existing data found. Performing full scrape...\")\n",
    "        all_data = scrape_all_contracts(url)\n",
    "        \n",
    "        # Save all data\n",
    "        with open(all_contracts_file, 'w') as write_file:\n",
    "            json.dump(all_data, write_file)\n",
    "            \n",
    "        print(f\"Saved {len(all_data)} contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad2bb240-61d8-464b-a91e-b627e0f33fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental mode: Scanning for new professional services contracts across all pages...\n",
      "Successfully loaded 34557 records from ../../data/raw/indiana_prof_services_contracts.json\n",
      "Found 34552 unique records\n",
      "Scanning all pages for new professional services contracts...\n",
      "Total pages to scan: 35\n",
      "Page 1/35: Found 12 new records\n",
      "Page 2/35: Found 0 new records\n",
      "Page 3/35: Found 7 new records\n",
      "Page 4/35: Found 0 new records\n",
      "Page 5/35: Found 0 new records\n",
      "Page 6/35: Found 4 new records\n",
      "Page 7/35: Found 10 new records\n",
      "Page 8/35: Found 5 new records\n",
      "Page 9/35: Found 0 new records\n",
      "Page 10/35: Found 0 new records\n",
      "Page 11/35: Found 17 new records\n",
      "Page 12/35: Found 0 new records\n",
      "Page 13/35: Found 0 new records\n",
      "Page 14/35: Found 0 new records\n",
      "Page 15/35: Found 0 new records\n",
      "Page 16/35: Found 2 new records\n",
      "Page 17/35: Found 0 new records\n",
      "Page 18/35: Found 2 new records\n",
      "Page 19/35: Found 5 new records\n",
      "Page 20/35: Found 2 new records\n",
      "Page 21/35: Found 0 new records\n",
      "Page 22/35: Found 23 new records\n",
      "Page 23/35: Found 1 new records\n",
      "Page 24/35: Found 1 new records\n",
      "Page 25/35: Found 15 new records\n",
      "Page 26/35: Found 0 new records\n",
      "Page 27/35: Found 14 new records\n",
      "Page 28/35: Found 1 new records\n",
      "Page 29/35: Found 11 new records\n",
      "Page 30/35: Found 0 new records\n",
      "Page 31/35: Found 0 new records\n",
      "Page 32/35: Found 0 new records\n",
      "Page 33/35: Found 0 new records\n",
      "Page 34/35: Found 12 new records\n",
      "Page 35/35: Found 0 new records\n",
      "\n",
      "Total new records found: 144\n",
      "Found 144 new professional services contracts\n",
      "Total professional services contracts after update: 34701\n"
     ]
    }
   ],
   "source": [
    "# Update professional services contracts\n",
    "prof_services_file = '../../data/raw/indiana_prof_services_contracts.json'\n",
    "\n",
    "# Contract type flags\n",
    "# 128 = professional services\n",
    "contract_type_flags = 128\n",
    "\n",
    "if CLOBBER:\n",
    "    print(\"CLOBBER mode: Scraping all professional services contracts and overwriting existing file...\")\n",
    "    all_data = scrape_all_contracts(url, contract_type_flags=contract_type_flags)\n",
    "    print(f\"Scraped {len(all_data)} total professional services contracts\")\n",
    "    \n",
    "    # Save all data (overwrite)\n",
    "    with open(prof_services_file, 'w') as write_file:\n",
    "        json.dump(all_data, write_file)\n",
    "        \n",
    "    print(f\"Saved {len(all_data)} professional services contracts (overwrote existing file)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Incremental mode: Scanning for new professional services contracts across all pages...\")\n",
    "    \n",
    "    # Load existing data\n",
    "    existing_data, existing_records = load_existing_contracts(prof_services_file)\n",
    "    \n",
    "    # Scrape all pages looking for new records\n",
    "    if existing_records:\n",
    "        print(\"Scanning all pages for new professional services contracts...\")\n",
    "        new_records = scrape_all_new_records(url, existing_records, contract_type_flags=contract_type_flags)\n",
    "        print(f\"Found {len(new_records)} new professional services contracts\")\n",
    "        \n",
    "        # Combine new and existing data (new records first)\n",
    "        updated_data = new_records + existing_data\n",
    "        \n",
    "        # Save updated data\n",
    "        with open(prof_services_file, 'w') as write_file:\n",
    "            json.dump(updated_data, write_file)\n",
    "            \n",
    "        print(f\"Total professional services contracts after update: {len(updated_data)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No existing data found. Performing full scrape...\")\n",
    "        all_data = scrape_all_contracts(url, contract_type_flags=contract_type_flags)\n",
    "        \n",
    "        # Save all data\n",
    "        with open(prof_services_file, 'w') as write_file:\n",
    "            json.dump(all_data, write_file)\n",
    "            \n",
    "        print(f\"Saved {len(all_data)} professional services contracts\")\n",
    "\n",
    "# Other contract type flags for reference:\n",
    "# 0 = all\n",
    "# 1 = attorney\n",
    "# 4 = grant\n",
    "# 8 = lease\n",
    "# 64 = MOU\n",
    "# 128 = professional services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
