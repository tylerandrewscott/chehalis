{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c6313e-f6b5-4797-9660-cf62d419a5d8",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages available\n",
      "‚úÖ All libraries imported successfully!\n",
      "üîß Ready for Amazon Textract processing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Required packages for Textract\n",
    "required_packages = [\n",
    "    'boto3', 'pandas', 'numpy', 'tqdm', 'matplotlib', 'seaborn', \n",
    "    'PIL', 'pdf2image', 'json', 'pathlib'\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        if pkg == 'PIL':\n",
    "            __import__('PIL')\n",
    "        else:\n",
    "            __import__(pkg)\n",
    "    except ImportError:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing packages: {', '.join(missing)}\")\n",
    "    print(\"Install with: pip install boto3 pandas numpy tqdm matplotlib seaborn Pillow pdf2image\")\n",
    "else:\n",
    "    print(\"‚úÖ All packages available\")\n",
    "\n",
    "# Import everything\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pdf2image\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üîß Ready for Amazon Textract processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c3e356-42f6-4a73-b8e8-09b41b8938fb",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration set for Amazon Textract\n",
      "Region: us-west-2\n",
      "Batch size: 10\n",
      "Features: Forms=True, Tables=True\n",
      "\n",
      "üìã AWS Setup Required:\n",
      "1. Install AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\n",
      "2. Configure credentials: aws configure\n",
      "3. Or set environment variables:\n",
      "   export AWS_ACCESS_KEY_ID=your_key\n",
      "   export AWS_SECRET_ACCESS_KEY=your_secret\n",
      "   export AWS_DEFAULT_REGION=us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Textract processing\n",
    "CONFIG = {\n",
    "    \"aws_region\": \"us-west-2\",  # Textract region\n",
    "    \"batch_size\": 10,  # Documents to process in parallel\n",
    "    \"max_pages_per_doc\": 1,  # Textract supports multi-page\n",
    "    \"image_dpi\": 300,  # Higher DPI for better accuracy\n",
    "    \"timeout\": 120,  # 2 minutes max per document\n",
    "    \"cache_dir\": \"./textract_cache\",  # Cache directory\n",
    "    \"confidence_threshold\": 0.8,  # Minimum confidence for field extraction\n",
    "    \"use_forms_analysis\": True,  # Enable forms feature\n",
    "    \"use_tables_analysis\": True,  # Enable tables feature\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG[\"cache_dir\"], exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration set for Amazon Textract\")\n",
    "print(f\"Region: {CONFIG['aws_region']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Features: Forms={CONFIG['use_forms_analysis']}, Tables={CONFIG['use_tables_analysis']}\")\n",
    "\n",
    "# AWS Setup Instructions\n",
    "print(\"\\nüìã AWS Setup Required:\")\n",
    "print(\"1. Install AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\")\n",
    "print(\"2. Configure credentials: aws configure\")\n",
    "print(\"3. Or set environment variables:\")\n",
    "print(\"   export AWS_ACCESS_KEY_ID=your_key\")\n",
    "print(\"   export AWS_SECRET_ACCESS_KEY=your_secret\")\n",
    "print(\"   export AWS_DEFAULT_REGION=us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecde33-3520-435f-9d4e-b21925db161b",
   "metadata": {},
   "source": [
    "## 3. Amazon Textract Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processor-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TextractContractProcessor\n",
      "AWS Region: us-west-2\n",

      "‚úì TextractContractProcessor initialized with exact form field mappings\n",
      "‚úì Special handling for complex sections:\n",
      "  - Contracts & Leases (field 3): contract types with fill-ins\n",
      "  - New total amount fiscal years (field 10): year/amount table\n",
      "  - Method of source selection (field 13): checkbox section\n",
      "  - Primary Vendor M/WBE (field 29): Minority/Women/IN-Veteran options\n",
      "  - Sub Vendor M/WBE (field 31): Minority/Women/IN-Veteran options\n"
     ]
    }
   ],

   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TextractContractProcessor:\n",
    "    \"\"\"Government contract processor using Amazon Textract\"\"\"\n",
    "    \n",
    "    def __init__(self, aws_region: str = None):\n",
    "        \"\"\"Initialize the Textract processor\"\"\"\n",
    "        \n",
    "        self.aws_region = aws_region or CONFIG[\"aws_region\"]\n",
    "        self.textract_client = None\n",
    "        \n",
    "        print(f\"Initializing TextractContractProcessor\")\n",
    "        print(f\"AWS Region: {self.aws_region}\")\n",
    "    \n",
    "    def initialize_textract(self):\n",
    "        \"\"\"Initialize AWS Textract client\"\"\"\n",
    "        if self.textract_client is not None:\n",
    "            return  # Already initialized\n",
    "            \n",
    "        try:\n",
    "            self.textract_client = boto3.client('textract', region_name=self.aws_region)\n",
    "            \n",
    "            # Test connection\n",
    "            response = self.textract_client.get_document_text_detection(JobId='test')\n",
    "        except Exception as e:\n",
    "            if 'InvalidJobIdException' in str(e):\n",
    "                print(\"‚úì AWS Textract client initialized successfully\")\n",
    "            else:\n",
    "                print(f\"‚ùå AWS configuration issue: {e}\")\n",
    "                print(\"Please check your AWS credentials and region\")\n",
    "                raise\n",
    "    \n",
    "    def convert_pdf_to_images(self, pdf_path: str, max_pages: int = None) -> List[Image.Image]:\n",
    "        \"\"\"Convert PDF to list of PIL Images for Textract\"\"\"\n",
    "        try:\n",
    "            max_pages = max_pages or CONFIG[\"max_pages_per_doc\"]\n",
    "            \n",
    "            images = pdf2image.convert_from_path(\n",
    "                pdf_path,\n",
    "                dpi=CONFIG[\"image_dpi\"],\n",
    "                first_page=1,\n",
    "                last_page=max_pages,\n",
    "                fmt='PNG'  # Textract prefers PNG\n",
    "            )\n",
    "            \n",
    "            return images\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting PDF {pdf_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def image_to_bytes(self, image: Image.Image) -> bytes:\n",
    "        \"\"\"Convert PIL Image to bytes for Textract\"\"\"\n",
    "        import io\n",
    "        \n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        return img_byte_arr.getvalue()\n",
    "    \n",
    "    def analyze_document_with_textract(self, image_bytes: bytes) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze document using Textract with forms and tables\"\"\"\n",
    "        try:\n",
    "            # Determine which features to use\n",
    "            feature_types = []\n",
    "            if CONFIG[\"use_forms_analysis\"]:\n",
    "                feature_types.append('FORMS')\n",
    "            if CONFIG[\"use_tables_analysis\"]:\n",
    "                feature_types.append('TABLES')\n",
    "            \n",
    "            if feature_types:\n",
    "                # Use analyze_document for forms/tables\n",
    "                response = self.textract_client.analyze_document(\n",
    "                    Document={'Bytes': image_bytes},\n",
    "                    FeatureTypes=feature_types\n",
    "                )\n",
    "            else:\n",
    "                # Use basic text detection\n",
    "                response = self.textract_client.detect_document_text(\n",
    "                    Document={'Bytes': image_bytes}\n",
    "                )\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Textract analysis failed: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def extract_key_value_pairs(self, textract_response: Dict) -> Dict[str, str]:\n",
    "        \"\"\"Extract key-value pairs from Textract forms analysis\"\"\"\n",
    "        key_value_pairs = {}\n",
    "        \n",
    "        if 'Blocks' not in textract_response:\n",
    "            return key_value_pairs\n",
    "        \n",
    "        # Create block map for reference lookup\n",
    "        block_map = {}\n",
    "        for block in textract_response['Blocks']:\n",
    "            block_map[block['Id']] = block\n",
    "        \n",
    "        # Extract key-value pairs\n",
    "        for block in textract_response['Blocks']:\n",
    "            if block['BlockType'] == 'KEY_VALUE_SET':\n",
    "                if 'KEY' in block.get('EntityTypes', []):\n",
    "                    # This is a key block\n",
    "                    key_text = self._get_text_from_block(block, block_map)\n",
    "                    \n",
    "                    # Find associated value\n",
    "                    value_text = \"\"\n",
    "                    if 'Relationships' in block:\n",
    "                        for relationship in block['Relationships']:\n",
    "                            if relationship['Type'] == 'VALUE':\n",
    "                                for value_id in relationship['Ids']:\n",
    "                                    if value_id in block_map:\n",
    "                                        value_block = block_map[value_id]\n",
    "                                        value_text = self._get_text_from_block(value_block, block_map)\n",
    "                    \n",
    "                    if key_text and block.get('Confidence', 0) >= CONFIG['confidence_threshold'] * 100:\n",
    "                        key_value_pairs[key_text.strip()] = value_text.strip()\n",
    "        \n",
    "        return key_value_pairs\n",
    "    \n",
    "    def extract_checkboxes(self, textract_response: Dict) -> Dict[str, str]:\n",
    "        \"\"\"Extract checkbox selections from Textract response\"\"\"\n",
    "        checkboxes = {}\n",
    "        \n",
    "        if 'Blocks' not in textract_response:\n",
    "            return checkboxes\n",
    "        \n",
    "        # Create block map for reference lookup\n",
    "        block_map = {}\n",
    "        for block in textract_response['Blocks']:\n",
    "            block_map[block['Id']] = block\n",
    "        \n",
    "        # Extract selection elements (checkboxes)\n",
    "        for block in textract_response['Blocks']:\n",
    "            if block['BlockType'] == 'SELECTION_ELEMENT':\n",
    "                selection_status = block.get('SelectionStatus', 'NOT_SELECTED')\n",
    "                confidence = block.get('Confidence', 0)\n",
    "                \n",
    "                if confidence >= CONFIG['confidence_threshold'] * 100:\n",
    "                    # Try to find associated text/label\n",
    "                    checkbox_label = self._find_checkbox_label(block, block_map)\n",
    "                    if checkbox_label:\n",
    "                        checkboxes[checkbox_label] = selection_status\n",
    "        \n",
    "        return checkboxes\n",
    "    \n",
    "    def _find_checkbox_label(self, checkbox_block: Dict, block_map: Dict) -> str:\n",
    "        \"\"\"Find the text label associated with a checkbox\"\"\"\n",
    "        # This is a simplified approach - in practice, you might need more\n",
    "        # sophisticated logic to associate checkboxes with their labels\n",
    "        checkbox_geometry = checkbox_block.get('Geometry', {})\n",
    "        checkbox_bbox = checkbox_geometry.get('BoundingBox', {})\n",
    "        \n",
    "        # Look for nearby text blocks\n",
    "        nearby_text = []\n",
    "        for block_id, block in block_map.items():\n",
    "            if block['BlockType'] == 'LINE':\n",
    "                block_bbox = block.get('Geometry', {}).get('BoundingBox', {})\n",
    "                # Simple proximity check (this could be improved)\n",
    "                if abs(block_bbox.get('Top', 0) - checkbox_bbox.get('Top', 0)) < 0.05:\n",
    "                    nearby_text.append(block.get('Text', ''))\n",
    "        \n",
    "        return ' '.join(nearby_text) if nearby_text else 'Unknown checkbox'\n",
    "    \n",
    "    def _get_text_from_block(self, block: Dict, block_map: Dict) -> str:\n",
    "        \"\"\"Extract text from a block using relationships\"\"\"\n",
    "        text = \"\"\n",
    "        \n",
    "        if 'Relationships' in block:\n",
    "            for relationship in block['Relationships']:\n",
    "                if relationship['Type'] == 'CHILD':\n",
    "                    for child_id in relationship['Ids']:\n",
    "                        if child_id in block_map:\n",
    "                            child_block = block_map[child_id]\n",
    "                            if child_block['BlockType'] == 'WORD':\n",
    "                                text += child_block.get('Text', '') + ' '\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_all_text(self, textract_response: Dict) -> str:\n",
    "        \"\"\"Extract all text from Textract response\"\"\"\n",
    "        text_blocks = []\n",
    "        \n",
    "        if 'Blocks' not in textract_response:\n",
    "            return \"\"\n",
    "        \n",
    "        for block in textract_response['Blocks']:\n",
    "            if block['BlockType'] == 'LINE':\n",
    "                text_blocks.append(block.get('Text', ''))\n",
    "        \n",
    "        return '\\n'.join(text_blocks)\n",
    "    \n",
    "    def extract_tables(self, textract_response: Dict) -> List[List[List[str]]]:\n",
    "        \"\"\"Extract tables from Textract response\"\"\"\n",
    "        tables = []\n",
    "        \n",
    "        if 'Blocks' not in textract_response:\n",
    "            return tables\n",
    "        \n",
    "        # Create block map\n",
    "        block_map = {}\n",
    "        for block in textract_response['Blocks']:\n",
    "            block_map[block['Id']] = block\n",
    "        \n",
    "        # Find table blocks\n",
    "        for block in textract_response['Blocks']:\n",
    "            if block['BlockType'] == 'TABLE':\n",
    "                table = self._extract_table_data(block, block_map)\n",
    "                if table:\n",
    "                    tables.append(table)\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    def _extract_table_data(self, table_block: Dict, block_map: Dict) -> List[List[str]]:\n",
    "        \"\"\"Extract data from a single table block\"\"\"\n",
    "        table_data = []\n",
    "        \n",
    "        if 'Relationships' not in table_block:\n",
    "            return table_data\n",
    "        \n",
    "        # Get all cells\n",
    "        cells = {}\n",
    "        for relationship in table_block['Relationships']:\n",
    "            if relationship['Type'] == 'CHILD':\n",
    "                for cell_id in relationship['Ids']:\n",
    "                    if cell_id in block_map:\n",
    "                        cell = block_map[cell_id]\n",
    "                        if cell['BlockType'] == 'CELL':\n",
    "                            row_index = cell.get('RowIndex', 0) - 1\n",
    "                            col_index = cell.get('ColumnIndex', 0) - 1\n",
    "                            cell_text = self._get_text_from_block(cell, block_map)\n",
    "                            cells[(row_index, col_index)] = cell_text\n",
    "        \n",
    "        # Convert to 2D array\n",
    "        if cells:\n",
    "            max_row = max(pos[0] for pos in cells.keys()) + 1\n",
    "            max_col = max(pos[1] for pos in cells.keys()) + 1\n",
    "            \n",
    "            table_data = [[\"\" for _ in range(max_col)] for _ in range(max_row)]\n",
    "            \n",
    "            for (row, col), text in cells.items():\n",
    "                table_data[row][col] = text\n",
    "        \n",
    "        return table_data\n",
    "    \n",
    "    def extract_fiscal_year_table(self, tables: List[List[List[str]]], full_text: str) -> str:\n",
    "        \"\"\"Extract the fiscal year table (field 10) with year and amount columns\"\"\"\n",
    "        \n",
    "        # Look for tables that might be the fiscal year table\n",
    "        fiscal_year_data = []\n",
    "        \n",
    "        for table in tables:\n",
    "            if len(table) < 2:  # Need at least header + 1 data row\n",
    "                continue\n",
    "                \n",
    "            # Check if this looks like a fiscal year table\n",
    "            # Look for year-like patterns in first column and amount-like patterns in second\n",
    "            is_fiscal_table = False\n",
    "            \n",
    "            for row_idx, row in enumerate(table):\n",
    "                if len(row) >= 2:  # Need at least 2 columns\n",
    "                    col1, col2 = row[0].strip(), row[1].strip()\n",
    "                    \n",
    "                    # Skip empty rows\n",
    "                    if not col1 and not col2:\n",
    "                        continue\n",
    "                        \n",
    "                    # Check if first column looks like a year (4 digits) or fiscal year\n",
    "                    if re.search(r'\\b(20\\d{2}|FY\\s*20\\d{2}|\\d{4})\\b', col1, re.IGNORECASE):\n",
    "                        is_fiscal_table = True\n",
    "                        \n",
    "                        # Extract year\n",
    "                        year_match = re.search(r'\\b(20\\d{2}|\\d{4})\\b', col1)\n",
    "                        year = year_match.group(1) if year_match else col1\n",
    "                        \n",
    "                        # Extract amount (remove currency symbols, commas)\n",
    "                        amount = re.sub(r'[^\\d\\.\\-]', '', col2) if col2 else '0'\n",
    "                        \n",
    "                        if year and amount:\n",
    "                            fiscal_year_data.append(f\"{year}: ${amount}\")\n",
    "            \n",
    "            # If we found fiscal year data in this table, stop looking\n",
    "            if is_fiscal_table and fiscal_year_data:\n",
    "                break\n",
    "        \n",
    "        # If no table found, try to extract from text patterns\n",
    "        if not fiscal_year_data:\n",
    "            # Look for patterns like \"2023: $50000\" or \"FY 2023 $50,000\" in text\n",
    "            patterns = [\n",
    "                r'(?:FY\\s*)?(\\d{4})[:\\s]+\\$?([\\d,]+\\.?\\d*)',\n",
    "                r'(\\d{4})\\s+(\\d+[,\\d]*\\.?\\d*)',\n",
    "                r'Year\\s+(\\d{4})[:\\s]+\\$?([\\d,]+\\.?\\d*)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, full_text, re.IGNORECASE)\n",
    "                for year, amount in matches:\n",
    "                    # Clean up amount\n",
    "                    clean_amount = re.sub(r'[^\\d\\.\\-]', '', amount)\n",
    "                    if clean_amount:\n",
    "                        fiscal_year_data.append(f\"{year}: ${clean_amount}\")\n",
    "        \n",
    "        return '; '.join(fiscal_year_data) if fiscal_year_data else ''\n",
    "    \n",
    "    def extract_contracts_leases_section(self, checkboxes: Dict[str, str], key_value_pairs: Dict[str, str], full_text: str) -> str:\n",
    "        \"\"\"Extract the contracts & leases section which is a complex checkbox area with fill-ins\"\"\"\n",
    "        \n",
    "        # Common contract/lease types to look for\n",
    "        contract_types = [\n",
    "            'lease', 'grant', 'professional services', 'personal services', \n",
    "            'maintenance', 'amendment', 'renewal', 'other'\n",
    "        ]\n",
    "        \n",
    "        selected_types = []\n",
    "        \n",
    "        # Check checkboxes for selected contract types\n",
    "        for checkbox_label, selection_status in checkboxes.items():\n",
    "            if selection_status == 'SELECTED':\n",
    "                label_lower = checkbox_label.lower()\n",
    "                for contract_type in contract_types:\n",
    "                    if contract_type in label_lower:\n",
    "                        # For amendment and renewal, try to extract the number\n",
    "                        if 'amendment' in label_lower:\n",
    "                            # Look for amendment number in nearby text or key-value pairs\n",
    "                            amendment_num = self._extract_fill_in_value(key_value_pairs, full_text, 'amendment')\n",
    "                            if amendment_num:\n",
    "                                selected_types.append(f\"Amendment #{amendment_num}\")\n",
    "                            else:\n",
    "                                selected_types.append(\"Amendment\")\n",
    "                        elif 'renewal' in label_lower:\n",
    "                            # Look for renewal number\n",
    "                            renewal_num = self._extract_fill_in_value(key_value_pairs, full_text, 'renewal')\n",
    "                            if renewal_num:\n",
    "                                selected_types.append(f\"Renewal #{renewal_num}\")\n",
    "                            else:\n",
    "                                selected_types.append(\"Renewal\")\n",
    "                        elif 'other' in label_lower:\n",
    "                            # Look for \"Other\" fill-in text\n",
    "                            other_text = self._extract_fill_in_value(key_value_pairs, full_text, 'other')\n",
    "                            if other_text:\n",
    "                                selected_types.append(f\"Other: {other_text}\")\n",
    "                            else:\n",
    "                                selected_types.append(\"Other\")\n",
    "                        else:\n",
    "                            selected_types.append(contract_type.title())\n",
    "                        break\n",
    "        \n",
    "        # If no checkboxes found, try to infer from text patterns\n",
    "        if not selected_types:\n",
    "            text_lower = full_text.lower()\n",
    "            for contract_type in contract_types:\n",
    "                if contract_type in text_lower:\n",
    "                    selected_types.append(contract_type.title())\n",
    "        \n",
    "        return ', '.join(selected_types) if selected_types else ''\n",
    "    \n",
    "    def extract_method_source_selection(self, checkboxes: Dict[str, str], full_text: str) -> str:\n",
    "        \"\"\"Extract the method of source selection checkbox section (field 13)\"\"\"\n",
    "        \n",
    "        # Common source selection methods\n",
    "        selection_methods = [\n",
    "            'competitive bid', 'competitive bidding', 'competitive sealed bid',\n",
    "            'competitive proposal', 'competitive negotiation', \n",
    "            'sole source', 'single source', 'emergency',\n",
    "            'cooperative contract', 'state contract', \n",
    "            'idt service', 'interagency', 'small purchase',\n",
    "            'other'\n",
    "        ]\n",
    "        \n",
    "        selected_methods = []\n",
    "        \n",
    "        # Check checkboxes for selected methods\n",
    "        for checkbox_label, selection_status in checkboxes.items():\n",
    "            if selection_status == 'SELECTED':\n",
    "                label_lower = checkbox_label.lower()\n",
    "                \n",
    "                # Check if this checkbox is related to source selection\n",
    "                if 'source' in label_lower or 'selection' in label_lower or 'method' in label_lower:\n",
    "                    for method in selection_methods:\n",
    "                        if method in label_lower:\n",
    "                            selected_methods.append(method.title())\n",
    "                            break\n",
    "                    else:\n",
    "                        # If it's clearly a source selection checkbox but doesn't match known methods\n",
    "                        # Add the label itself (cleaned up)\n",
    "                        if any(word in label_lower for word in ['competitive', 'sole', 'emergency', 'bid', 'proposal']):\n",
    "                            selected_methods.append(checkbox_label.strip())\n",
    "        \n",
    "        # If no checkboxes found, try to infer from text patterns\n",
    "        if not selected_methods:\n",
    "            text_lower = full_text.lower()\n",
    "            for method in selection_methods:\n",
    "                if method in text_lower:\n",
    "                    selected_methods.append(method.title())\n",
    "        \n",
    "        return ', '.join(selected_methods) if selected_methods else ''\n",
    "    \n",
    "    def extract_mwbe_section(self, checkboxes: Dict[str, str], section_name: str) -> str:\n",
    "        \"\"\"Extract M/WBE checkbox section (Primary Vendor or Sub Vendor) with Minority/Women/IN-Veteran options\"\"\"\n",
    "        \n",
    "        mwbe_types = []\n",
    "        \n",
    "        # Look for checkboxes related to this M/WBE section\n",
    "        for checkbox_label, selection_status in checkboxes.items():\n",
    "            if selection_status == 'SELECTED':\n",
    "                label_lower = checkbox_label.lower()\n",
    "                \n",
    "                # Check if this checkbox is in the right section (primary vs sub vendor)\n",
    "                section_match = False\n",
    "                if 'primary' in section_name.lower() and 'primary' in label_lower:\n",
    "                    section_match = True\n",
    "                elif 'sub' in section_name.lower() and 'sub' in label_lower:\n",
    "                    section_match = True\n",
    "                elif section_name.lower() in label_lower:\n",
    "                    section_match = True\n",
    "                \n",
    "                if section_match:\n",
    "                    # Check for specific M/WBE categories\n",
    "                    if 'minority' in label_lower:\n",
    "                        mwbe_types.append('Minority')\n",
    "                    elif 'women' in label_lower:\n",
    "                        mwbe_types.append('Women')\n",
    "                    elif 'in-veteran' in label_lower or 'veteran' in label_lower:\n",
    "                        mwbe_types.append('IN-Veteran')\n",
    "        \n",
    "        if mwbe_types:\n",
    "            return ', '.join(mwbe_types)\n",
    "        else:\n",
    "            return 'No'  # Default to No if no M/WBE categories are selected\n",
    "    \n",
    "    def _extract_fill_in_value(self, key_value_pairs: Dict[str, str], full_text: str, field_type: str) -> str:\n",
    "        \"\"\"Extract fill-in values for Amendment #, Renewal #, or Other fields\"\"\"\n",
    "        \n",
    "        # First check key-value pairs\n",
    "        for key, value in key_value_pairs.items():\n",
    "            key_lower = key.lower()\n",
    "            if field_type in key_lower and value:\n",
    "                return value.strip()\n",
    "        \n",
    "        # Then try regex patterns on full text\n",
    "        patterns = {\n",
    "            'amendment': r'amendment\\s*#?\\s*(\\d+|[A-Z]\\d+)',\n",
    "            'renewal': r'renewal\\s*#?\\s*(\\d+|[A-Z]\\d+)',\n",
    "            'other': r'other\\s*[:\\-]?\\s*([^,\\n]{1,50})'\n",
    "        }\n",
    "        \n",
    "        if field_type in patterns:\n",
    "            match = re.search(patterns[field_type], full_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    def map_to_contract_fields(self, key_value_pairs: Dict[str, str], checkboxes: Dict[str, str], full_text: str, tables: List[List[List[str]]]) -> Dict[str, str]:\n",
    "        \"\"\"Map extracted key-value pairs, checkboxes, and tables to contract fields using exact form labels\"\"\"\n",
    "        \n",
    "        fields = {\n",
    "            'eds_number': '',\n",
    "            'date_prepared': '',\n",
    "            'contracts_leases': '',\n",
    "            'account_number': '',\n",
    "            'account_name': '',\n",
    "            'total_amount_this_action': '',\n",
    "            'new_contract_total': '',\n",
    "            'revenue_generated_this_action': '',\n",
    "            'revenue_generated_total_contract': '',\n",
    "            'new_total_amount_fiscal_years': '',  # Field 10 - table\n",
    "            'from_date': '',\n",
    "            'to_date': '',\n",
    "            'method_source_selection': '',  # Field 13 - checkbox section\n",
    "            'email_address': '',\n",
    "            'vendor_id': '',\n",
    "            'vendor_name': '',\n",
    "            'primary_vendor_mwbe': '',\n",
    "            'sub_vendor_mwbe': '',\n",
    "            'renewal_language': '',\n",
    "            'termination_convenience_clause': '',\n",
    "            'description_work_justification': ''\n",
    "        }\n",
    "        \n",
    "        # Regular text field mapping patterns\n",
    "        field_mappings = {\n",
    "            'eds_number': ['EDS Number'],\n",
    "            'date_prepared': ['Date prepared'],\n",
    "            'account_number': ['Account Number'],\n",
    "            'account_name': ['Account Name'],\n",
    "            'total_amount_this_action': ['Total amount this action'],\n",
    "            'new_contract_total': ['New contract total'],\n",
    "            'revenue_generated_this_action': ['Revenue generated this action'],\n",
    "            'revenue_generated_total_contract': ['Revenue generated total contract'],\n",
    "            'from_date': ['From (month, day, year)', 'From'],\n",
    "            'to_date': ['To (month, day, year)', 'To'],\n",
    "            'email_address': ['E-email address:', 'E-email address'],\n",
    "            'vendor_id': ['Vendor ID #', 'Vendor ID'],\n",
    "            'vendor_name': ['Name'],\n",
    "            'description_work_justification': ['Description of work and justification for spending money']\n",
    "        }\n",
    "        \n",
    "        # Yes/No checkbox field mappings (simple checkboxes)\n",
    "        simple_checkbox_mappings = {\n",
    "            'renewal_language': ['Is there Renewal Language in the document?', 'Renewal Language'],\n",
    "            'termination_convenience_clause': ['Is there a \"Termination for Convenience\" clause in the document?', 'Termination for Convenience']\n",
    "        }\n",
    "        \n",
    "        # Map regular text fields\n",
    "        for field_name, possible_keys in field_mappings.items():\n",
    "            for key, value in key_value_pairs.items():\n",
    "                key_clean = key.strip().rstrip(':')\n",
    "                \n",
    "                # Try exact match first\n",
    "                if key_clean in possible_keys:\n",
    "                    if value and len(value.strip()) > 0:\n",
    "                        fields[field_name] = value.strip()\n",
    "                        break\n",
    "                \n",
    "                # Try partial match\n",
    "                for possible_key in possible_keys:\n",
    "                    if possible_key.lower() in key_clean.lower():\n",
    "                        if value and len(value.strip()) > 0:\n",
    "                            fields[field_name] = value.strip()\n",
    "                            break\n",
    "                \n",
    "                if fields[field_name]:  # Found a match, move to next field\n",
    "                    break\n",
    "        \n",
    "        # Map simple Yes/No checkbox results\n",
    "        for field_name, possible_labels in simple_checkbox_mappings.items():\n",
    "            for checkbox_label, selection_status in checkboxes.items():\n",
    "                for possible_label in possible_labels:\n",
    "                    if possible_label.lower() in checkbox_label.lower():\n",
    "                        # Convert selection status to more readable format\n",
    "                        fields[field_name] = 'Yes' if selection_status == 'SELECTED' else 'No'\n",
    "                        break\n",
    "                if fields[field_name]:  # Found a match, move to next field\n",
    "                    break\n",
    "        \n",
    "        # Special handling for complex sections\n",
    "        \n",
    "        # 1. Contracts & Leases (field 3) - complex checkbox section with fill-ins\n",
    "        fields['contracts_leases'] = self.extract_contracts_leases_section(checkboxes, key_value_pairs, full_text)\n",
    "        \n",
    "        # 2. New total amount for each fiscal year (field 10) - table extraction\n",
    "        fields['new_total_amount_fiscal_years'] = self.extract_fiscal_year_table(tables, full_text)\n",
    "        \n",
    "        # 3. Method of source selection (field 13) - checkbox section\n",
    "        fields['method_source_selection'] = self.extract_method_source_selection(checkboxes, full_text)\n",
    "        \n",
    "        # 4. Primary Vendor M/WBE (field 29) - checkbox section with Minority/Women/IN-Veteran options\n",
    "        fields['primary_vendor_mwbe'] = self.extract_mwbe_section(checkboxes, 'Primary Vendor')\n",
    "        \n",
    "        # 5. Sub Vendor M/WBE (field 31) - checkbox section with Minority/Women/IN-Veteran options\n",
    "        fields['sub_vendor_mwbe'] = self.extract_mwbe_section(checkboxes, 'Sub Vendor')\n",
    "        \n",
    "        # Fallback to regex patterns on full text for missing critical fields\n",
    "        regex_patterns = {\n",
    "            'email_address': r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})',\n",
    "            'eds_number': r'([A-Z]\\d{2}[A-Z]?-\\d+-\\d{4})',\n",
    "            'date_prepared': r'(\\d{1,2}/\\d{1,2}/\\d{4})',\n",
    "        }\n",
    "        \n",
    "        for field_name, pattern in regex_patterns.items():\n",
    "            if not fields[field_name]:  # Only if not already found\n",
    "                match = re.search(pattern, full_text)\n",
    "                if match:\n",
    "                    fields[field_name] = match.group(1)\n",
    "        \n",
    "        return fields\n",
    "    \n",
    "    def process_document(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single contract document using Amazon Textract\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'file_path': file_path,\n",
    "            'filename': os.path.basename(file_path),\n",
    "            'status': 'processing',\n",
    "            'processing_time': 0,\n",
    "            'error': None,\n",
    "            'pages_processed': 0,\n",
    "            'extracted_fields': {},\n",
    "            'key_value_pairs': {},\n",
    "            'checkboxes': {},\n",
    "            'tables': [],\n",
    "            'extraction_confidence': 0.0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Initialize Textract if needed\n",
    "            if self.textract_client is None:\n",
    "                self.initialize_textract()\n",
    "            \n",
    "            # Handle different file types\n",
    "            if file_path.lower().endswith('.pdf'):\n",
    "                images = self.convert_pdf_to_images(file_path)\n",
    "            else:\n",
    "                # Assume image file\n",
    "                images = [Image.open(file_path)]\n",
    "            \n",
    "            if not images:\n",
    "                raise ValueError(\"No images extracted from document\")\n",
    "            \n",
    "            # Process all pages with Textract\n",
    "            all_key_value_pairs = {}\n",
    "            all_checkboxes = {}\n",
    "            all_tables = []\n",
    "            full_text = \"\"\n",
    "            total_confidence = 0\n",
    "            confidence_count = 0\n",
    "            \n",
    "            for i, image in enumerate(images[:CONFIG[\"max_pages_per_doc\"]]):\n",
    "                # Convert image to bytes\n",
    "                image_bytes = self.image_to_bytes(image)\n",
    "                \n",
    "                # Analyze with Textract\n",
    "                textract_response = self.analyze_document_with_textract(image_bytes)\n",
    "                \n",
    "                if not textract_response:\n",
    "                    continue\n",
    "                \n",
    "                # Extract key-value pairs\n",
    "                page_kv_pairs = self.extract_key_value_pairs(textract_response)\n",
    "                all_key_value_pairs.update(page_kv_pairs)\n",
    "                \n",
    "                # Extract checkboxes\n",
    "                page_checkboxes = self.extract_checkboxes(textract_response)\n",
    "                all_checkboxes.update(page_checkboxes)\n",
    "                \n",
    "                # Extract tables\n",
    "                page_tables = self.extract_tables(textract_response)\n",
    "                all_tables.extend(page_tables)\n",
    "                \n",
    "                # Extract all text\n",
    "                page_text = self.extract_all_text(textract_response)\n",
    "                full_text += f\"\\n=== Page {i+1} ===\\n{page_text}\"\n",
    "                \n",
    "                # Calculate confidence (from blocks)\n",
    "                if 'Blocks' in textract_response:\n",
    "                    page_confidences = [block.get('Confidence', 0) for block in textract_response['Blocks'] \n",
    "                                      if 'Confidence' in block]\n",
    "                    if page_confidences:\n",
    "                        avg_confidence = sum(page_confidences) / len(page_confidences)\n",
    "                        total_confidence += avg_confidence\n",
    "                        confidence_count += 1\n",
    "            \n",
    "            # Map to contract fields (now includes tables parameter)\n",
    "            extracted_fields = self.map_to_contract_fields(all_key_value_pairs, all_checkboxes, full_text, all_tables)\n",
    "            \n",
    "            # Calculate overall confidence\n",
    "            overall_confidence = (total_confidence / confidence_count) if confidence_count > 0 else 0\n",
    "            \n",
    "            # Adjust confidence based on field extraction success\n",
    "            filled_fields = sum(1 for v in extracted_fields.values() if v)\n",
    "            field_success_rate = filled_fields / len(extracted_fields)\n",
    "            adjusted_confidence = (overall_confidence * 0.8) + (field_success_rate * 100 * 0.2)\n",
    "            \n",
    "            # Update result\n",
    "            result.update({\n",
    "                'status': 'success',\n",
    "                'extracted_fields': extracted_fields,\n",
    "                'key_value_pairs': all_key_value_pairs,\n",
    "                'checkboxes': all_checkboxes,\n",
    "                'tables': all_tables,\n",
    "                'pages_processed': len(images),\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'extraction_confidence': round(adjusted_confidence, 2)\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Successfully processed {file_path} in {result['processing_time']:.2f}s\")\n",
    "            logger.info(f\"Fields extracted: {filled_fields}/{len(extracted_fields)}, Confidence: {adjusted_confidence:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result.update({\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'processing_time': time.time() - start_time\n",
    "            })\n",
    "            logger.error(f\"Failed to process {file_path}: {e}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_batch(self, file_paths: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a batch of documents with progress bar\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        with tqdm(total=len(file_paths), desc=\"Processing contracts with Textract\") as pbar:\n",
    "            for file_path in file_paths:\n",
    "                result = self.process_document(file_path)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update progress bar\n",
    "                status_icon = \"‚úì\" if result['status'] == 'success' else \"‚ùå\"\n",
    "                pbar.set_postfix({\n",
    "                    'file': os.path.basename(file_path)[:20],\n",
    "                    'status': status_icon\n",
    "                })\n",
    "                pbar.update(1)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# %%\n",
    "# Initialize the Textract processor\n",
    "processor = TextractContractProcessor()\n",
    "print(\"‚úì TextractContractProcessor initialized with exact form field mappings\")\n",
    "print(\"‚úì Special handling for complex sections:\")\n",
    "print(\"  - Contracts & Leases (field 3): contract types with fill-ins\")  \n",
    "print(\"  - New total amount fiscal years (field 10): year/amount table\")\n",
    "print(\"  - Method of source selection (field 13): checkbox section\")\n",
    "print(\"  - Primary Vendor M/WBE (field 29): Minority/Women/IN-Veteran options\")\n",
    "print(\"  - Sub Vendor M/WBE (field 31): Minority/Women/IN-Veteran options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332494a8-0ff8-46a4-b802-cd3383fc8615",
   "metadata": {},
   "source": [
    "## 4. Test on Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "test-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Before running: Ensure AWS credentials are configured\n",
      "Run: aws configure\n",
      "Or set environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\n",
      "\n",
      "üîÑ Testing Textract on: ../../data/raw/_exampleforms/83501-000.pdf\n",
      "==================================================\n",
      "‚úì AWS Textract client initialized successfully\n",
      "Status: success\n",
      "Processing time: 7.58 seconds\n",
      "Pages processed: 1\n",
      "Confidence: 91.5%\n",
      "\n",
      "üìã Extracted Contract Fields:\n",
      "  eds_number: A70-5-008060\n",
      "  date_prepared: 12/9/2014\n",
      "  contracts_leases: Lease, Grant, Personal Services, Maintenance, Amendment, Renewal, Other\n",
      "  account_number: 61910-30400.573100\n",
      "  account_name: ISDH DHHS Fund\n",
      "  total_amount_this_action: $65,000.00\n",
      "  new_contract_total: 65,000.00\n",
      "  revenue_generated_this_action: $0.00\n",
      "  revenue_generated_total_contract: $0.00\n",
      "  new_total_amount_fiscal_years: 6204: $3.; 8060: $12; 2383: $24.; 4900: $61910; 6208: $8.; 2015: $65000.00; 2014: $8; 2015: $39.; 7115: $44.; 6204: $3.; 8060: $12; 2383: $24.; 4900: $61910; 6208: $8.; 2014: $8; 2015: $39.; 7115: $44.; 2015: $65000.00\n",
      "  from_date: 9/29/2014\n",
      "  to_date: $65,000.00\n",
      "  method_source_selection: Emergency, Other\n",
      "  email_address: aholt@isdh.in.gov\n",
      "  vendor_id: 0000012383\n",
      "  vendor_name: Mike P. Mendyk\n",
      "  primary_vendor_mwbe: No\n",
      "  sub_vendor_mwbe: No\n",
      "\n",
      "üîë Key-Value Pairs Found: 52\n",
      "  Sample pairs:\n",
      "    40. Anthony: Em approval\n",
      "    Maintenance: \n",
      "    Amendment#: \n",
      "    No: \n",
      "    Emergency: \n",
      "\n",
      "üìä Tables Found: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the Textract processor on a single document\n",
    "def test_single_document(file_path: str):\n",
    "    \"\"\"Test Textract processing on a single document\"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        print(\"\\nüí° To test the processor:\")\n",
    "        print(\"1. Place a sample contract PDF in the '../../data/raw/_exampleforms' folder\")\n",
    "        print(\"2. Update the file_path variable below\")\n",
    "        print(\"3. Ensure AWS credentials are configured\")\n",
    "        print(\"4. Run this cell again\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîÑ Testing Textract on: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    result = processor.process_document(file_path)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    print(f\"Processing time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"Pages processed: {result['pages_processed']}\")\n",
    "    print(f\"Confidence: {result.get('extraction_confidence', 0):.1f}%\")\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        print(\"\\nüìã Extracted Contract Fields:\")\n",
    "        for field, value in result['extracted_fields'].items():\n",
    "            if value:  # Only show non-empty fields\n",
    "                print(f\"  {field}: {value}\")\n",
    "        \n",
    "        print(f\"\\nüîë Key-Value Pairs Found: {len(result['key_value_pairs'])}\")\n",
    "        if result['key_value_pairs']:\n",
    "            print(\"  Sample pairs:\")\n",
    "            for i, (key, value) in enumerate(list(result['key_value_pairs'].items())[:5]):\n",
    "                print(f\"    {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nüìä Tables Found: {len(result['tables'])}\")\n",
    "        \n",
    "        if not any(result['extracted_fields'].values()):\n",
    "            print(\"  ‚ö†Ô∏è No structured fields extracted. Check field mappings.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Error: {result['error']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with a sample file (update path as needed)\n",
    "sample_file = \"../../data/raw/_exampleforms/83501-000.pdf\"\n",
    "\n",
    "print(\"üí° Before running: Ensure AWS credentials are configured\")\n",
    "print(\"Run: aws configure\")\n",
    "print(\"Or set environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\")\n",
    "print()\n",
    "\n",
    "# Uncomment to test with your AWS credentials:\n",
    "test_result = test_single_document(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd25f053-35da-4954-b076-18f7bc3ecc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '../../data/raw/_exampleforms/83501-000.pdf',\n",
       " 'filename': '83501-000.pdf',\n",
       " 'status': 'success',\n",
       " 'processing_time': 7.583702087402344,\n",
       " 'error': None,\n",
       " 'pages_processed': 1,\n",
       " 'extracted_fields': {'eds_number': 'A70-5-008060',\n",
       "  'date_prepared': '12/9/2014',\n",
       "  'contracts_leases': 'Lease, Grant, Personal Services, Maintenance, Amendment, Renewal, Other',\n",
       "  'account_number': '61910-30400.573100',\n",
       "  'account_name': 'ISDH DHHS Fund',\n",
       "  'total_amount_this_action': '$65,000.00',\n",
       "  'new_contract_total': '65,000.00',\n",
       "  'revenue_generated_this_action': '$0.00',\n",
       "  'revenue_generated_total_contract': '$0.00',\n",
       "  'new_total_amount_fiscal_years': '6204: $3.; 8060: $12; 2383: $24.; 4900: $61910; 6208: $8.; 2015: $65000.00; 2014: $8; 2015: $39.; 7115: $44.; 6204: $3.; 8060: $12; 2383: $24.; 4900: $61910; 6208: $8.; 2014: $8; 2015: $39.; 7115: $44.; 2015: $65000.00',\n",
       "  'from_date': '9/29/2014',\n",
       "  'to_date': '$65,000.00',\n",
       "  'method_source_selection': 'Emergency, Other',\n",
       "  'email_address': 'aholt@isdh.in.gov',\n",
       "  'vendor_id': '0000012383',\n",
       "  'vendor_name': 'Mike P. Mendyk',\n",
       "  'primary_vendor_mwbe': 'No',\n",
       "  'sub_vendor_mwbe': 'No',\n",
       "  'renewal_language': '',\n",
       "  'termination_convenience_clause': '',\n",
       "  'description_work_justification': ''},\n",
       " 'key_value_pairs': {'40. Anthony': 'Em approval',\n",
       "  'Maintenance': '',\n",
       "  'Amendment#': '',\n",
       "  'No': '',\n",
       "  'Emergency': '',\n",
       "  'Yes': '',\n",
       "  'Professional/Personal Services': '',\n",
       "  '4. Account Number:': '61910-30400.573100',\n",
       "  '20. Name:': 'Mike P. Mendyk',\n",
       "  'MOU': '',\n",
       "  '23 Vendor ID #': '0000012383',\n",
       "  '5. Account Name:': 'ISDH DHHS Fund',\n",
       "  '6. Total amount this action:': '$65,000.00',\n",
       "  '36. Statutory Authority (Cite applicable Indiana or Federal Codes):': 'TITLE XVII. SECTION 1701(E)(1)',\n",
       "  'Lease': '',\n",
       "  '17. Name:': 'Antoniette Holt',\n",
       "  \"44. Attorney General's Office approval\": 'NMS',\n",
       "  '21. Telephone #:': '317-233-7120',\n",
       "  '43. Date Approved': '117115',\n",
       "  '11. From (month, day. year):': '9/29/2014',\n",
       "  '22. E-mail address:': 'mmendyk@isdh.in.gov',\n",
       "  'Yes: IOT or Delegate has signed off on contract': '',\n",
       "  '14. Name of agency:': 'Department of Health',\n",
       "  '8. Revenue generated this action:': '$0.00',\n",
       "  'I. EDS Number:': 'A70-5-008060',\n",
       "  '12. To ( month, day. year': '8/31/2015',\n",
       "  '7. New contract total:': '65,000.00',\n",
       "  '9. Revenue generated total contract:': '$0.00',\n",
       "  'Special Procurement': '',\n",
       "  'License Agreement': '',\n",
       "  '25. Telephone #:': '317-920-4900',\n",
       "  '15. Requisition Number:': '0000027673',\n",
       "  'Contract for procured Services': '',\n",
       "  '24. Name:': 'INDIANA MINORITY HEALTH',\n",
       "  'Grant': '',\n",
       "  '19. E-mail address:': 'aholt@isdh.in.gov',\n",
       "  'Attorney': '',\n",
       "  '30. Primary Vendor Percentages': '100.0 %',\n",
       "  '47. Date Approved': '',\n",
       "  '26. Address:': '3737 N MERIDIAN ST SUITE 303 INDIANAPOLIS. IN 46208',\n",
       "  '18. Telephone #:': '317/233-3006',\n",
       "  'Renewal #': '',\n",
       "  'Other (specify)': '',\n",
       "  '16. Address:': '2 N. Meridian Street Indianapolis, IN 46204',\n",
       "  '2. Date prepared:': '12/9/2014',\n",
       "  'Bid/Quotation': '',\n",
       "  'Negotiated': '',\n",
       "  'Minority:': '',\n",
       "  '38. Justification of vendor selection and determination of price reasonableness: The Office of Monority Health facilitates and coordinates community-based programs tailored to meet the needs of these populations': 'JAN 08 2015',\n",
       "  'from AG': '',\n",
       "  'fiscal officer Mill': '',\n",
       "  'DEC': '30 2014'},\n",
       " 'checkboxes': {'3. CONTRACTS & LEASES COURIER INFORMATION Professional/Personal Services Contract for procured Services Grant 20. Name: 21. Telephone #: Maintenance Lease Mike P. Mendyk 317-233-7120 License Agreement Attorney Amendment# 22. E-mail address: MOU Renewal # mmendyk@isdh.in.gov QPA Other VENDOR INFORMATION': 'NOT_SELECTED'},\n",
       " 'tables': [[['Em 40. Anthony fiscal officer Mill approval',\n",
       "    '41.',\n",
       "    '12/25/19-52',\n",
       "    '43. Date Approved 117115'],\n",
       "   [\"44. Attorney General's Office approval NMS\",\n",
       "    '45. 1-26-15',\n",
       "    'from AG',\n",
       "    '47. Date Approved']]],\n",
       " 'extraction_confidence': 91.5}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18fc599-205b-47aa-ac51-6deb7a4d4fa7",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_directory(input_dir: str, file_extensions: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Process all documents in a directory using Textract\"\"\"\n",
    "    \n",
    "    if file_extensions is None:\n",
    "        file_extensions = ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.tif']\n",
    "    \n",
    "    # Find all contract files\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in file_extensions):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"‚ùå No files found in {input_dir} with extensions {file_extensions}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üìÅ Found {len(file_paths)} files to process\")\n",
    "    print(f\"üìä Processing with Amazon Textract (batch size: {CONFIG['batch_size']})\")\n",
    "    \n",
    "    # Estimate cost\n",
    "    estimated_cost = len(file_paths) * 0.0015  # $0.0015 per page (forms analysis)\n",
    "    print(f\"üí∞ Estimated cost: ${estimated_cost:.2f} (assuming 1 page per doc)\")\n",
    "    \n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(0, len(file_paths), CONFIG['batch_size']):\n",
    "        batch_files = file_paths[i:i + CONFIG['batch_size']]\n",
    "        batch_num = i // CONFIG['batch_size'] + 1\n",
    "        total_batches = (len(file_paths) + CONFIG['batch_size'] - 1) // CONFIG['batch_size']\n",
    "        \n",
    "        print(f\"\\nüîÑ Processing batch {batch_num}/{total_batches}\")\n",
    "        \n",
    "        batch_results = processor.process_batch(batch_files)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Show batch summary\n",
    "        successful = sum(1 for r in batch_results if r['status'] == 'success')\n",
    "        print(f\"   ‚úì {successful}/{len(batch_results)} successful\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = create_results_dataframe(all_results)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_successful = (df['status'] == 'success').sum()\n",
    "    success_rate = (total_successful / len(df)) * 100\n",
    "    avg_time = df[df['status'] == 'success']['processing_time'].mean()\n",
    "    avg_confidence = df[df['status'] == 'success']['extraction_confidence'].mean()\n",
    "    \n",
    "    print(f\"\\nüìä TEXTRACT PROCESSING COMPLETE\")\n",
    "    print(f\"   Total files: {len(df)}\")\n",
    "    print(f\"   Successful: {total_successful}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   Average time: {avg_time:.2f}s per document\")\n",
    "    print(f\"   Average confidence: {avg_confidence:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_results_dataframe(results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"Convert Textract results list to structured DataFrame\"\"\"\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for result in results:\n",
    "        # Base record\n",
    "        record = {\n",
    "            'filename': result['filename'],\n",
    "            'file_path': result['file_path'],\n",
    "            'status': result['status'],\n",
    "            'processing_time': result['processing_time'],\n",
    "            'pages_processed': result['pages_processed'],\n",
    "            'extraction_confidence': result.get('extraction_confidence', 0),\n",
    "            'key_value_pairs_count': len(result.get('key_value_pairs', {})),\n",
    "            'tables_count': len(result.get('tables', [])),\n",
    "            'error': result.get('error', '')\n",
    "        }\n",
    "        \n",
    "        # Add extracted fields\n",
    "        if result['status'] == 'success':\n",
    "            record.update(result['extracted_fields'])\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage\n",
    "INPUT_DIRECTORY = \"../../data/raw/_exampleforms\"\n",
    "\n",
    "print(\"üöÄ Ready to process documents with Amazon Textract!\")\n",
    "print(f\"Input directory: {INPUT_DIRECTORY}\")\n",
    "print(f\"Configuration: {CONFIG}\")\n",
    "print(\"\\nüí° To process your documents:\")\n",
    "print(\"1. Ensure AWS credentials are configured\")\n",
    "print(\"2. Update INPUT_DIRECTORY above\")\n",
    "print(\"3. Uncomment the processing line below\")\n",
    "print(\"4. Run this cell\")\n",
    "\n",
    "# Uncomment the following line to start processing:\n",
    "# df_results = process_document_directory(INPUT_DIRECTORY)"
   ]
  },
  {

   "cell_type": "code",
   "id": "52kl80l598",
   "source": "# Load zero-shot results and filter for documents with forms\ndef load_form_documents(zero_shot_results_path: str, contracts_dir: str) -> pd.DataFrame:\n    \"\"\"Load and filter documents that contain forms based on zero-shot classification results\"\"\"\n    \n    if not os.path.exists(zero_shot_results_path):\n        print(f\"‚ö†Ô∏è Zero-shot results file not found: {zero_shot_results_path}\")\n        print(\"This file will be created by the zero-shot classification process.\")\n        return pd.DataFrame()\n    \n    # Load zero-shot results\n    print(f\"üìñ Loading zero-shot results from: {zero_shot_results_path}\")\n    df_zero_shot = pd.read_csv(zero_shot_results_path)\n    \n    print(f\"   Total documents in zero-shot results: {len(df_zero_shot)}\")\n    \n    # Filter for documents that contain forms\n    df_forms = df_zero_shot[df_zero_shot['contains_form'] == True].copy()\n    print(f\"   Documents with forms: {len(df_forms)}\")\n    \n    if len(df_forms) == 0:\n        print(\"‚ùå No documents with forms found in zero-shot results\")\n        return pd.DataFrame()\n    \n    # Parse form_pages to get the lowest page number for each document\n    def get_lowest_page(form_pages_str):\n        \"\"\"Extract the lowest page number from form_pages string (e.g., '1,2,3' -> 1)\"\"\"\n        if pd.isna(form_pages_str) or form_pages_str == '':\n            return 1  # Default to page 1 if no page info\n        \n        try:\n            # Handle different formats: \"1,2,3\" or \"1-3\" or just \"1\"\n            pages_str = str(form_pages_str).strip()\n            \n            if ',' in pages_str:\n                # Format: \"1,2,3\"\n                pages = [int(p.strip()) for p in pages_str.split(',') if p.strip().isdigit()]\n            elif '-' in pages_str:\n                # Format: \"1-3\" (extract first number)\n                pages = [int(pages_str.split('-')[0].strip())]\n            else:\n                # Format: \"1\"\n                pages = [int(pages_str)]\n            \n            return min(pages) if pages else 1\n            \n        except (ValueError, AttributeError):\n            print(f\"‚ö†Ô∏è Could not parse form_pages: {form_pages_str}, defaulting to page 1\")\n            return 1\n    \n    df_forms['target_page'] = df_forms['form_pages'].apply(get_lowest_page)\n    \n    # Build full file paths and verify files exist\n    valid_files = []\n    \n    for _, row in df_forms.iterrows():\n        # Get the filename from the zero-shot results\n        filename = row.get('filename', row.get('file', ''))\n        if not filename:\n            continue\n            \n        # Construct full path\n        if not filename.endswith('.pdf'):\n            filename += '.pdf'\n            \n        file_path = os.path.join(contracts_dir, filename)\n        \n        # Check if file exists\n        if os.path.exists(file_path):\n            valid_files.append({\n                'filename': filename,\n                'file_path': file_path,\n                'target_page': row['target_page'],\n                'form_pages': row['form_pages'],\n                'contains_form': row['contains_form']\n            })\n        else:\n            print(f\"‚ö†Ô∏è File not found: {file_path}\")\n    \n    df_valid = pd.DataFrame(valid_files)\n    \n    if len(df_valid) == 0:\n        print(\"‚ùå No valid contract files found\")\n        return df_valid\n        \n    print(f\"‚úÖ Found {len(df_valid)} valid documents with forms to process\")\n    print(f\"   Page distribution: {df_valid['target_page'].value_counts().sort_index().to_dict()}\")\n    \n    return df_valid\n\ndef process_form_documents_with_textract(zero_shot_results_path: str, contracts_dir: str) -> pd.DataFrame:\n    \"\"\"Process documents that contain forms using Amazon Textract\"\"\"\n    \n    # Load and filter documents with forms\n    df_forms = load_form_documents(zero_shot_results_path, contracts_dir)\n    \n    if df_forms.empty:\n        return pd.DataFrame()\n    \n    print(f\"\\nüöÄ Processing {len(df_forms)} documents with Textract\")\n    print(f\"üí∞ Estimated cost: ${len(df_forms) * 0.0015:.2f}\")\n    \n    # Process each document, extracting only the target page\n    all_results = []\n    \n    with tqdm(total=len(df_forms), desc=\"Processing form documents\") as pbar:\n        for _, row in df_forms.iterrows():\n            file_path = row['file_path']\n            target_page = row['target_page']\n            \n            # Modify the processor to extract only the target page\n            result = process_single_form_page(file_path, target_page)\n            \n            # Add metadata from zero-shot results\n            result.update({\n                'target_page': target_page,\n                'original_form_pages': row['form_pages'],\n                'zero_shot_contains_form': row['contains_form']\n            })\n            \n            all_results.append(result)\n            \n            # Update progress\n            status_icon = \"‚úì\" if result['status'] == 'success' else \"‚ùå\"\n            pbar.set_postfix({\n                'file': os.path.basename(file_path)[:15],\n                'page': target_page,\n                'status': status_icon\n            })\n            pbar.update(1)\n    \n    # Convert to DataFrame\n    df_results = create_results_dataframe(all_results)\n    \n    # Summary\n    successful = (df_results['status'] == 'success').sum()\n    success_rate = (successful / len(df_results)) * 100\n    \n    print(f\"\\nüìä FORM PROCESSING COMPLETE\")\n    print(f\"   Documents processed: {len(df_results)}\")\n    print(f\"   Successful: {successful}\")\n    print(f\"   Success rate: {success_rate:.1f}%\")\n    \n    if successful > 0:\n        avg_confidence = df_results[df_results['status'] == 'success']['extraction_confidence'].mean()\n        print(f\"   Average confidence: {avg_confidence:.1f}%\")\n    \n    return df_results\n\ndef process_single_form_page(file_path: str, target_page: int) -> Dict[str, Any]:\n    \"\"\"Process a single page from a PDF document using Textract\"\"\"\n    \n    start_time = time.time()\n    \n    result = {\n        'file_path': file_path,\n        'filename': os.path.basename(file_path),\n        'status': 'processing',\n        'processing_time': 0,\n        'error': None,\n        'pages_processed': 0,\n        'extracted_fields': {},\n        'key_value_pairs': {},\n        'tables': [],\n        'extraction_confidence': 0.0\n    }\n    \n    try:\n        # Initialize Textract if needed\n        if processor.textract_client is None:\n            processor.initialize_textract()\n        \n        # Convert only the target page from PDF\n        if file_path.lower().endswith('.pdf'):\n            images = pdf2image.convert_from_path(\n                file_path,\n                dpi=CONFIG[\"image_dpi\"],\n                first_page=target_page,\n                last_page=target_page,  # Only extract the target page\n                fmt='PNG'\n            )\n        else:\n            # For non-PDF files, just use the whole file\n            images = [Image.open(file_path)]\n        \n        if not images:\n            raise ValueError(f\"No images extracted from page {target_page}\")\n        \n        # Process the single page with Textract\n        image = images[0]  # Should only be one image\n        \n        # Convert image to bytes\n        image_bytes = processor.image_to_bytes(image)\n        \n        # Analyze with Textract\n        textract_response = processor.analyze_document_with_textract(image_bytes)\n        \n        if not textract_response:\n            raise ValueError(\"Empty response from Textract\")\n        \n        # Extract key-value pairs\n        page_kv_pairs = processor.extract_key_value_pairs(textract_response)\n        \n        # Extract tables\n        page_tables = processor.extract_tables(textract_response)\n        \n        # Extract all text\n        page_text = processor.extract_all_text(textract_response)\n        \n        # Calculate confidence\n        page_confidences = [block.get('Confidence', 0) for block in textract_response.get('Blocks', []) \n                          if 'Confidence' in block]\n        avg_confidence = sum(page_confidences) / len(page_confidences) if page_confidences else 0\n        \n        # Map to contract fields\n        extracted_fields = processor.map_to_contract_fields(page_kv_pairs, page_text)\n        \n        # Adjust confidence based on field extraction success\n        filled_fields = sum(1 for v in extracted_fields.values() if v)\n        field_success_rate = filled_fields / len(extracted_fields)\n        adjusted_confidence = (avg_confidence * 0.8) + (field_success_rate * 100 * 0.2)\n        \n        # Update result\n        result.update({\n            'status': 'success',\n            'extracted_fields': extracted_fields,\n            'key_value_pairs': page_kv_pairs,\n            'tables': page_tables,\n            'pages_processed': 1,\n            'processing_time': time.time() - start_time,\n            'extraction_confidence': round(adjusted_confidence, 2)\n        })\n        \n    except Exception as e:\n        result.update({\n            'status': 'failed',\n            'error': str(e),\n            'processing_time': time.time() - start_time\n        })\n    \n    return result\n\n# Configuration for form processing\nZERO_SHOT_RESULTS_PATH = \"../../code/preprocessing/zero_shot_results_full_corpus.csv\"\nCONTRACTS_DIRECTORY = \"../../data/raw/_contracts\"\n\nprint(\"üéØ FORM-SPECIFIC TEXTRACT PROCESSING\")\nprint(\"=\" * 50)\nprint(f\"Zero-shot results: {ZERO_SHOT_RESULTS_PATH}\")\nprint(f\"Contracts directory: {CONTRACTS_DIRECTORY}\")\nprint(\"\\nüí° This will:\")\nprint(\"1. Load zero-shot classification results\")\nprint(\"2. Filter for documents with contains_form = True\")\nprint(\"3. Extract the lowest page number from form_pages\")\nprint(\"4. Process only that specific page with Textract\")\nprint(\"\\nüöÄ To run the processing:\")\nprint(\"1. Ensure the zero-shot results file exists\")\nprint(\"2. Configure AWS credentials\")\nprint(\"3. Uncomment the line below\")\n\n# Uncomment to process form documents:\n# df_form_results = process_form_documents_with_textract(ZERO_SHOT_RESULTS_PATH, CONTRACTS_DIRECTORY)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {

   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## 6. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration (replace with actual results)\n",
    "def create_sample_textract_results():\n",
    "    \"\"\"Create sample Textract results for demonstration purposes\"\"\"\n",
    "    \n",
    "    sample_data = [\n",
    "        {\n",
    "            'filename': 'contract_001.pdf',\n",
    "            'status': 'success',\n",
    "            'processing_time': 8.2,\n",
    "            'pages_processed': 2,\n",
    "            'extraction_confidence': 94.7,\n",
    "            'key_value_pairs_count': 15,\n",
    "            'tables_count': 1,\n",
    "            'eds_number': 'C22-6-0060',\n",
    "            'date_prepared': '6/13/2006',\n",
    "            'contracts_leases': 'Professional/Personal Services',\n",
    "            'account_number': '5120-10660',\n",
    "            'total_amount_this_action': '250000.00',\n",
    "            'from_date': '1/27/2006',\n",
    "            'to_date': '1/26/2009',\n",
    "            'vendor_name': 'PINEBROOK LANDSCAPING INC',\n",
    "            'email_address': 'sstombaugh@idoa.IN.gov',\n",
    "        },\n",
    "        {\n",
    "            'filename': 'contract_002.pdf',\n",
    "            'status': 'success', \n",
    "            'processing_time': 6.1,\n",
    "            'pages_processed': 1,\n",
    "            'extraction_confidence': 97.3,\n",
    "            'key_value_pairs_count': 12,\n",
    "            'tables_count': 0,\n",
    "            'eds_number': 'C45A-6-789',\n",
    "            'date_prepared': '3/15/2023',\n",
    "            'total_amount_this_action': '75000.00',\n",
    "            'vendor_name': 'XYZ Services Inc',\n",
    "            'from_date': '03/15/2023',\n",
    "            'to_date': '03/14/2024',\n",
    "        },\n",
    "        {\n",
    "            'filename': 'contract_003.pdf',\n",
    "            'status': 'failed',\n",
    "            'processing_time': 12.3,\n",
    "            'pages_processed': 0,\n",
    "            'extraction_confidence': 0.0,\n",
    "            'key_value_pairs_count': 0,\n",
    "            'tables_count': 0,\n",
    "            'error': 'AWS credentials not configured'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(sample_data)\n",
    "\n",
    "# Use sample data for now (replace with df_results from actual processing)\n",
    "df_results = create_sample_textract_results()\n",
    "print(\"üìä Sample Textract results loaded for demonstration\")\n",
    "\n",
    "def analyze_textract_results(df: pd.DataFrame):\n",
    "    \"\"\"Analyze and visualize Textract processing results\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìà TEXTRACT RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_docs = len(df)\n",
    "    successful = (df['status'] == 'success').sum()\n",
    "    failed = (df['status'] == 'failed').sum()\n",
    "    success_rate = (successful / total_docs) * 100\n",
    "    \n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if successful > 0:\n",
    "        successful_df = df[df['status'] == 'success']\n",
    "        avg_time = successful_df['processing_time'].mean()\n",
    "        avg_confidence = successful_df['extraction_confidence'].mean()\n",
    "        avg_kv_pairs = successful_df['key_value_pairs_count'].mean()\n",
    "        \n",
    "        print(f\"Average processing time: {avg_time:.2f}s\")\n",
    "        print(f\"Average confidence: {avg_confidence:.1f}%\")\n",
    "        print(f\"Average key-value pairs: {avg_kv_pairs:.1f} per document\")\n",
    "    \n",
    "    # Field extraction rates\n",
    "    print(f\"\\nüìã Field Extraction Rates:\")\n",
    "    field_columns = [\n",
    "        'eds_number', 'date_prepared', 'account_number', 'total_amount_this_action',\n",
    "        'vendor_name', 'from_date', 'to_date', 'email_address'\n",
    "    ]\n",
    "    \n",
    "    for field in field_columns:\n",
    "        if field in df.columns:\n",
    "            non_empty = df[field].notna() & (df[field] != '')\n",
    "            rate = (non_empty.sum() / successful) * 100 if successful > 0 else 0\n",
    "            print(f\"  {field}: {rate:.1f}%\")\n",
    "    \n",
    "    # Visualizations\n",
    "    create_textract_visualizations(df)\n",
    "\n",
    "def create_textract_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations of Textract results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Amazon Textract Processing Results Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Success/Failure distribution\n",
    "    status_counts = df['status'].value_counts()\n",
    "    axes[0, 0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Processing Status Distribution')\n",
    "    \n",
    "    # 2. Processing time vs confidence scatter\n",
    "    successful_df = df[df['status'] == 'success']\n",
    "    if not successful_df.empty:\n",
    "        axes[0, 1].scatter(successful_df['processing_time'], successful_df['extraction_confidence'])\n",
    "        axes[0, 1].set_xlabel('Processing Time (seconds)')\n",
    "        axes[0, 1].set_ylabel('Extraction Confidence (%)')\n",
    "        axes[0, 1].set_title('Processing Time vs Confidence')\n",
    "    \n",
    "    # 3. Confidence distribution\n",
    "    if not successful_df.empty:\n",
    "        axes[1, 0].hist(successful_df['extraction_confidence'], bins=10, alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Extraction Confidence (%)')\n",
    "        axes[1, 0].set_ylabel('Number of Documents')\n",
    "        axes[1, 0].set_title('Confidence Distribution')\n",
    "    \n",
    "    # 4. Key-value pairs found\n",
    "    if not successful_df.empty and 'key_value_pairs_count' in successful_df.columns:\n",
    "        axes[1, 1].hist(successful_df['key_value_pairs_count'], bins=10, alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('Key-Value Pairs Found')\n",
    "        axes[1, 1].set_ylabel('Number of Documents')\n",
    "        axes[1, 1].set_title('Key-Value Pairs Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze the sample results\n",
    "analyze_textract_results(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-cell",
   "metadata": {},
   "outputs": [],

   "source": [
    "def export_textract_results(df: pd.DataFrame, output_dir: str = \"../../data/intermediate_results\"):\n",

  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyterlab-4] *",
   "language": "python",
   "name": "conda-env-jupyterlab-4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}