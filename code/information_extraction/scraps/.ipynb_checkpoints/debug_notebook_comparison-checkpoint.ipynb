{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug: Main Notebook vs Debug Notebook Comparison\n",
    "\n",
    "The debug notebook works correctly (example forms are detected), but the main notebook doesn't.\n",
    "Let's find the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import fitz\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main notebook to analyze\n",
    "notebook_path = \"zeroshot_form_or_contract.ipynb\"\n",
    "with open(notebook_path, 'r') as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "# Find the diagnostic cell (cell 14)\n",
    "cells = notebook['cells']\n",
    "print(f\"Total cells in notebook: {len(cells)}\")\n",
    "\n",
    "# Look for the diagnostic cell\n",
    "diagnostic_cell = None\n",
    "for i, cell in enumerate(cells):\n",
    "    if cell['cell_type'] == 'code' and 'source' in cell:\n",
    "        source = ''.join(cell['source'])\n",
    "        if \"DIAGNOSTIC: Testing detection on example forms\" in source:\n",
    "            diagnostic_cell = i\n",
    "            print(f\"Found diagnostic cell at index {i}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1: Model loading differences\n",
    "print(\"=== CHECKING MODEL LOADING ===\")\n",
    "\n",
    "# Find model loading in main notebook\n",
    "for i, cell in enumerate(cells):\n",
    "    if cell['cell_type'] == 'code' and 'source' in cell:\n",
    "        source = ''.join(cell['source'])\n",
    "        if \"CLIPModel.from_pretrained\" in source and \"clip-vit-large-patch14-336\" in source:\n",
    "            print(f\"\\nCell {i}: CLIP model loading\")\n",
    "            print(\"Key lines:\")\n",
    "            for line in cell['source']:\n",
    "                if 'model_name' in line or 'device' in line or '.to(' in line:\n",
    "                    print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2: Example features loading\n",
    "print(\"=== CHECKING EXAMPLE FEATURES LOADING ===\")\n",
    "\n",
    "# Find where example features are loaded\n",
    "for i, cell in enumerate(cells):\n",
    "    if cell['cell_type'] == 'code' and 'source' in cell:\n",
    "        source = ''.join(cell['source'])\n",
    "        if \"example_features = []\" in source or \"example_features.append\" in source:\n",
    "            print(f\"\\nCell {i}: Example features loading\")\n",
    "            # Check for normalization\n",
    "            has_norm = \"np.linalg.norm\" in source\n",
    "            print(f\"  Has normalization: {has_norm}\")\n",
    "            if has_norm:\n",
    "                for line in cell['source']:\n",
    "                    if \"norm\" in line:\n",
    "                        print(f\"  Normalization line: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: Detection function differences\n",
    "print(\"=== CHECKING DETECTION FUNCTIONS ===\")\n",
    "\n",
    "# Find all detection functions\n",
    "detection_functions = []\n",
    "for i, cell in enumerate(cells):\n",
    "    if cell['cell_type'] == 'code' and 'source' in cell:\n",
    "        source = ''.join(cell['source'])\n",
    "        if \"def detect_form\" in source:\n",
    "            # Extract function name\n",
    "            for line in cell['source']:\n",
    "                if line.strip().startswith(\"def detect_form\"):\n",
    "                    func_name = line.split('(')[0].replace('def ', '').strip()\n",
    "                    detection_functions.append((i, func_name))\n",
    "                    print(f\"\\nCell {i}: {func_name}\")\n",
    "                    # Check if it uses device parameter\n",
    "                    if \"device\" in source:\n",
    "                        print(\"  Uses device parameter: YES\")\n",
    "                    else:\n",
    "                        print(\"  Uses device parameter: NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 4: Variable overwrites\n",
    "print(\"=== CHECKING FOR VARIABLE OVERWRITES ===\")\n",
    "\n",
    "# Track where key variables are assigned\n",
    "key_vars = ['example_features', 'clip_model', 'clip_processor', 'device']\n",
    "var_assignments = {var: [] for var in key_vars}\n",
    "\n",
    "for i, cell in enumerate(cells):\n",
    "    if cell['cell_type'] == 'code' and 'source' in cell:\n",
    "        source = ''.join(cell['source'])\n",
    "        for var in key_vars:\n",
    "            # Look for assignments\n",
    "            if f\"{var} =\" in source or f\"{var}.append\" in source:\n",
    "                var_assignments[var].append(i)\n",
    "\n",
    "for var, cells_list in var_assignments.items():\n",
    "    if len(cells_list) > 1:\n",
    "        print(f\"\\nWARNING: '{var}' is assigned/modified in multiple cells: {cells_list}\")\n",
    "    elif len(cells_list) == 1:\n",
    "        print(f\"'{var}' assigned in cell: {cells_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 5: Analyze the diagnostic cell specifically\n",
    "print(\"=== ANALYZING DIAGNOSTIC CELL ===\")\n",
    "\n",
    "if diagnostic_cell is not None:\n",
    "    diag_source = ''.join(cells[diagnostic_cell]['source'])\n",
    "    \n",
    "    # Which detection function is used?\n",
    "    if \"detect_form_visual_clip\" in diag_source:\n",
    "        print(\"Diagnostic cell uses: detect_form_visual_clip\")\n",
    "    else:\n",
    "        print(\"WARNING: Diagnostic cell doesn't use detect_form_visual_clip!\")\n",
    "        # Find what it does use\n",
    "        for line in cells[diagnostic_cell]['source']:\n",
    "            if \"detect_form\" in line and \"=\" in line:\n",
    "                print(f\"  Found: {line.strip()}\")\n",
    "    \n",
    "    # Check parameters passed\n",
    "    print(\"\\nChecking parameters passed to detection function:\")\n",
    "    for line in cells[diagnostic_cell]['source']:\n",
    "        if \"detect_form\" in line and \"(\" in line:\n",
    "            print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 6: Execution order issue\n",
    "print(\"=== CHECKING EXECUTION ORDER ===\")\n",
    "\n",
    "# Find where detect_form_visual_clip is defined\n",
    "detect_func_cell = None\n",
    "for i, cell in enumerate(cells):\n",
    "    if cell['cell_type'] == 'code' and 'source' in cell:\n",
    "        source = ''.join(cell['source'])\n",
    "        if \"def detect_form_visual_clip\" in source:\n",
    "            detect_func_cell = i\n",
    "            break\n",
    "\n",
    "print(f\"detect_form_visual_clip defined in cell: {detect_func_cell}\")\n",
    "print(f\"Diagnostic cell is at: {diagnostic_cell}\")\n",
    "\n",
    "if detect_func_cell and diagnostic_cell:\n",
    "    if detect_func_cell > diagnostic_cell:\n",
    "        print(\"\\nPROBLEM: Function is defined AFTER it's used!\")\n",
    "    else:\n",
    "        print(\"\\nOK: Function is defined before use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 7: Device handling\n",
    "print(\"=== CHECKING DEVICE HANDLING ===\")\n",
    "\n",
    "# Check if models are moved to device correctly\n",
    "for i, cell in enumerate(cells):\n",
    "    if cell['cell_type'] == 'code' and 'source' in cell:\n",
    "        source = ''.join(cell['source'])\n",
    "        if \".to(device)\" in source or \"to(device)\" in source:\n",
    "            print(f\"\\nCell {i}: Moving to device\")\n",
    "            for line in cell['source']:\n",
    "                if \"to(device)\" in line:\n",
    "                    print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reproduce the exact setup from the main notebook\n",
    "print(\"=== REPRODUCING MAIN NOTEBOOK SETUP ===\")\n",
    "\n",
    "# Load CLIP model exactly as in main notebook\n",
    "model_name = \"openai/clip-vit-large-patch14-336\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONE example exactly as main notebook\n",
    "example_forms_dir = \"../../data/raw/_exampleforms/\"\n",
    "example_files = [f for f in os.listdir(example_forms_dir) if f.endswith('.pdf')]\n",
    "\n",
    "if example_files:\n",
    "    # Load first example\n",
    "    test_file = example_files[0]\n",
    "    pdf_path = os.path.join(example_forms_dir, test_file)\n",
    "    \n",
    "    pdf = fitz.open(pdf_path)\n",
    "    page = pdf[0]\n",
    "    pix = page.get_pixmap()\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    pdf.close()\n",
    "    \n",
    "    # Extract features WITH device handling\n",
    "    print(\"\\nTesting feature extraction with explicit device handling:\")\n",
    "    \n",
    "    # Method 1: As in main notebook\n",
    "    inputs1 = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features1 = clip_model.get_image_features(**inputs1)\n",
    "        features1_cpu = features1.cpu().numpy()\n",
    "        features1_norm = features1_cpu / np.linalg.norm(features1_cpu, axis=1, keepdims=True)\n",
    "    \n",
    "    print(f\"Method 1 - Features shape: {features1_norm.shape}\")\n",
    "    print(f\"Method 1 - Norm: {np.linalg.norm(features1_norm)}\")\n",
    "    \n",
    "    # Method 2: Without explicit to(device) on inputs\n",
    "    inputs2 = clip_processor(images=img, return_tensors=\"pt\")\n",
    "    # Move each tensor to device\n",
    "    inputs2 = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs2.items()}\n",
    "    with torch.no_grad():\n",
    "        features2 = clip_model.get_image_features(**inputs2)\n",
    "        features2_cpu = features2.cpu().numpy()\n",
    "        features2_norm = features2_cpu / np.linalg.norm(features2_cpu, axis=1, keepdims=True)\n",
    "    \n",
    "    print(f\"\\nMethod 2 - Features shape: {features2_norm.shape}\")\n",
    "    print(f\"Method 2 - Norm: {np.linalg.norm(features2_norm)}\")\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\nMethods produce same result: {np.allclose(features1_norm, features2_norm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check: Run the exact detection function from main notebook\n",
    "def detect_form_visual_clip(image, clip_model, clip_processor, device,\n",
    "                           positive_features=None, negative_features=None,\n",
    "                           similarity_threshold=0.7, negative_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Pure visual form detection using CLIP features\n",
    "    No text detection or OCR - just visual similarity\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'is_form': False,\n",
    "        'confidence': 0,\n",
    "        'max_positive_similarity': 0,\n",
    "        'max_negative_similarity': 0,\n",
    "        'positive_similarities': [],\n",
    "        'negative_similarities': []\n",
    "    }\n",
    "    \n",
    "    # Extract visual features from current image\n",
    "    try:\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features.cpu().numpy()\n",
    "            # Normalize features\n",
    "            features = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return result\n",
    "    \n",
    "    # Check similarity to positive examples\n",
    "    if positive_features:\n",
    "        for pos_feat in positive_features:\n",
    "            sim = cosine_similarity(features, pos_feat)[0][0]\n",
    "            result['positive_similarities'].append(sim)\n",
    "        \n",
    "        result['max_positive_similarity'] = max(result['positive_similarities'])\n",
    "        is_like_positive = result['max_positive_similarity'] > similarity_threshold\n",
    "    else:\n",
    "        is_like_positive = False\n",
    "    \n",
    "    # Check similarity to negative examples\n",
    "    if negative_features:\n",
    "        for neg_feat in negative_features:\n",
    "            sim = cosine_similarity(features, neg_feat)[0][0]\n",
    "            result['negative_similarities'].append(sim)\n",
    "        \n",
    "        result['max_negative_similarity'] = max(result['negative_similarities'])\n",
    "        is_not_like_negative = result['max_negative_similarity'] < negative_threshold\n",
    "    else:\n",
    "        is_not_like_negative = True\n",
    "    \n",
    "    # Decision: must be like positive AND not like negative\n",
    "    result['is_form'] = is_like_positive and is_not_like_negative\n",
    "    \n",
    "    # Confidence score\n",
    "    if result['is_form']:\n",
    "        # High positive similarity, low negative similarity\n",
    "        result['confidence'] = result['max_positive_similarity'] * (1 - result['max_negative_similarity'] * 0.5)\n",
    "    else:\n",
    "        result['confidence'] = 0\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with just the one feature\n",
    "print(\"\\nTesting detection with single example feature:\")\n",
    "result = detect_form_visual_clip(\n",
    "    img, clip_model, clip_processor, device,\n",
    "    [features1_norm], None, 0.7, 0.7\n",
    ")\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"\\nThis should show is_form=True with similarity ~1.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}