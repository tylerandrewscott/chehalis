{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Form Detection Issues\n",
    "\n",
    "This notebook investigates why example forms are not being detected as forms when compared against themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on mps\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "model_name = \"openai/clip-vit-large-patch14-336\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: 25581-000.pdf\n",
      "Features shape: (1, 768)\n",
      "Features norm before normalization: 16.98063087463379\n",
      "Features norm after normalization: 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Extract features from a single example form\n",
    "example_forms_dir = \"../../data/raw/_exampleforms/\"\n",
    "example_files = [f for f in os.listdir(example_forms_dir) if f.endswith('.pdf')]\n",
    "test_file = example_files[0]\n",
    "\n",
    "print(f\"Testing with: {test_file}\")\n",
    "\n",
    "# Load the test image\n",
    "pdf_path = os.path.join(example_forms_dir, test_file)\n",
    "pdf = fitz.open(pdf_path)\n",
    "page = pdf[0]\n",
    "pix = page.get_pixmap()\n",
    "img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "pdf.close()\n",
    "\n",
    "# Extract features\n",
    "inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    features1 = clip_model.get_image_features(**inputs)\n",
    "    features1 = features1.cpu().numpy()\n",
    "    \n",
    "print(f\"Features shape: {features1.shape}\")\n",
    "print(f\"Features norm before normalization: {np.linalg.norm(features1)}\")\n",
    "\n",
    "# Normalize\n",
    "features1_norm = features1 / np.linalg.norm(features1, axis=1, keepdims=True)\n",
    "print(f\"Features norm after normalization: {np.linalg.norm(features1_norm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity of the same image with itself: 1.000000\n",
      "This should be 1.0 or very close to 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Compare the same image with itself\n",
    "# Load the same image again\n",
    "pdf = fitz.open(pdf_path)\n",
    "page = pdf[0]\n",
    "pix = page.get_pixmap()\n",
    "img2 = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "pdf.close()\n",
    "\n",
    "# Extract features again\n",
    "inputs2 = clip_processor(images=img2, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    features2 = clip_model.get_image_features(**inputs2)\n",
    "    features2 = features2.cpu().numpy()\n",
    "    features2_norm = features2 / np.linalg.norm(features2, axis=1, keepdims=True)\n",
    "\n",
    "# Compare\n",
    "similarity = cosine_similarity(features1_norm, features2_norm)[0][0]\n",
    "print(f\"\\nSimilarity of the same image with itself: {similarity:.6f}\")\n",
    "print(f\"This should be 1.0 or very close to 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25581-000.pdf: shape (1, 768)\n",
      "Loaded 99171-000.pdf: shape (1, 768)\n",
      "Loaded 13924-002.pdf: shape (1, 768)\n",
      "Loaded 1197-000.pdf: shape (1, 768)\n",
      "Loaded 67419-000.pdf: shape (1, 768)\n",
      "\n",
      "Total example features loaded: 5\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Load all example features as done in the main notebook\n",
    "example_features = []\n",
    "\n",
    "for example_file in example_files[:5]:  # Test with first 5\n",
    "    example_path = os.path.join(example_forms_dir, example_file)\n",
    "    \n",
    "    try:\n",
    "        pdf = fitz.open(example_path)\n",
    "        page = pdf[0]\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        pdf.close()\n",
    "        \n",
    "        # Extract features\n",
    "        inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features.cpu().numpy()\n",
    "            features = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
    "            example_features.append(features)\n",
    "        \n",
    "        print(f\"Loaded {example_file}: shape {features.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {example_file}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal example features loaded: {len(example_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix between first 5 examples:\n",
      "       Ex0    Ex1    Ex2    Ex3    Ex4\n",
      "Ex0  1.000  0.827  0.805  0.872  0.770\n",
      "Ex1  0.827  1.000  0.870  0.841  0.914\n",
      "Ex2  0.805  0.870  1.000  0.878  0.834\n",
      "Ex3  0.872  0.841  0.878  1.000  0.799\n",
      "Ex4  0.770  0.914  0.834  0.799  1.000\n",
      "\n",
      "Diagonal should be 1.0 (same image with itself)\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Check similarity between examples\n",
    "print(\"Similarity matrix between first 5 examples:\")\n",
    "similarity_matrix = np.zeros((min(5, len(example_features)), min(5, len(example_features))))\n",
    "\n",
    "for i in range(min(5, len(example_features))):\n",
    "    for j in range(min(5, len(example_features))):\n",
    "        sim = cosine_similarity(example_features[i], example_features[j])[0][0]\n",
    "        similarity_matrix[i, j] = sim\n",
    "\n",
    "# Display as DataFrame for better readability\n",
    "sim_df = pd.DataFrame(similarity_matrix, \n",
    "                      index=[f\"Ex{i}\" for i in range(similarity_matrix.shape[0])],\n",
    "                      columns=[f\"Ex{i}\" for i in range(similarity_matrix.shape[1])])\n",
    "print(sim_df.round(3))\n",
    "print(\"\\nDiagonal should be 1.0 (same image with itself)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Replicate the exact detection function\n",
    "def detect_form_visual_clip_debug(image, clip_model, clip_processor, device,\n",
    "                                 positive_features=None, negative_features=None,\n",
    "                                 similarity_threshold=0.7, negative_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Debug version with extra logging\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'is_form': False,\n",
    "        'confidence': 0,\n",
    "        'max_positive_similarity': 0,\n",
    "        'max_negative_similarity': 0,\n",
    "        'positive_similarities': [],\n",
    "        'negative_similarities': []\n",
    "    }\n",
    "    \n",
    "    # Extract visual features from current image\n",
    "    try:\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features.cpu().numpy()\n",
    "            print(f\"  Raw features shape: {features.shape}\")\n",
    "            print(f\"  Raw features norm: {np.linalg.norm(features)}\")\n",
    "            \n",
    "            # Normalize features\n",
    "            features = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
    "            print(f\"  Normalized features norm: {np.linalg.norm(features)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return result\n",
    "    \n",
    "    # Check similarity to positive examples\n",
    "    if positive_features:\n",
    "        print(f\"  Checking against {len(positive_features)} positive examples\")\n",
    "        for i, pos_feat in enumerate(positive_features):\n",
    "            sim = cosine_similarity(features, pos_feat)[0][0]\n",
    "            result['positive_similarities'].append(sim)\n",
    "            print(f\"    Example {i}: similarity = {sim:.4f}\")\n",
    "        \n",
    "        result['max_positive_similarity'] = max(result['positive_similarities'])\n",
    "        is_like_positive = result['max_positive_similarity'] > similarity_threshold\n",
    "        print(f\"  Max positive similarity: {result['max_positive_similarity']:.4f}\")\n",
    "        print(f\"  Is like positive (>{similarity_threshold})? {is_like_positive}\")\n",
    "    else:\n",
    "        is_like_positive = False\n",
    "        print(\"  No positive features provided\")\n",
    "    \n",
    "    # For now, ignore negative examples to isolate the issue\n",
    "    is_not_like_negative = True\n",
    "    \n",
    "    # Decision: must be like positive AND not like negative\n",
    "    result['is_form'] = is_like_positive and is_not_like_negative\n",
    "    print(f\"  Final decision: {result['is_form']}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing detection on the first example form:\n",
      "File: 25581-000.pdf\n",
      "\n",
      "  Raw features shape: (1, 768)\n",
      "  Raw features norm: 16.98063087463379\n",
      "  Normalized features norm: 0.9999999403953552\n",
      "  Checking against 5 positive examples\n",
      "    Example 0: similarity = 1.0000\n",
      "    Example 1: similarity = 0.8266\n",
      "    Example 2: similarity = 0.8052\n",
      "    Example 3: similarity = 0.8720\n",
      "    Example 4: similarity = 0.7704\n",
      "  Max positive similarity: 1.0000\n",
      "  Is like positive (>0.7)? True\n",
      "  Final decision: True\n",
      "\n",
      "Result: True\n",
      "This should be True since we're testing an example against itself!\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Test detection on the first example form\n",
    "print(\"Testing detection on the first example form:\")\n",
    "print(f\"File: {example_files[0]}\\n\")\n",
    "\n",
    "# Load the first example\n",
    "pdf = fitz.open(os.path.join(example_forms_dir, example_files[0]))\n",
    "page = pdf[0]\n",
    "pix = page.get_pixmap()\n",
    "test_img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "pdf.close()\n",
    "\n",
    "# Run detection\n",
    "result = detect_form_visual_clip_debug(\n",
    "    test_img, clip_model, clip_processor, device,\n",
    "    example_features, None, 0.7, 0.7\n",
    ")\n",
    "\n",
    "print(f\"\\nResult: {result['is_form']}\")\n",
    "print(f\"This should be True since we're testing an example against itself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking feature dimensions:\n",
      "Example 0: shape = (1, 768), dtype = float32\n",
      "  Min value: -0.3834, Max value: 0.4124\n",
      "  Mean: -0.0008, Std: 0.0361\n",
      "\n",
      "Example 1: shape = (1, 768), dtype = float32\n",
      "  Min value: -0.3543, Max value: 0.3800\n",
      "  Mean: -0.0008, Std: 0.0361\n",
      "\n",
      "Example 2: shape = (1, 768), dtype = float32\n",
      "  Min value: -0.4028, Max value: 0.4316\n",
      "  Mean: -0.0009, Std: 0.0361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 7: Check if the issue is with feature dimensions\n",
    "print(\"Checking feature dimensions:\")\n",
    "for i, feat in enumerate(example_features[:3]):\n",
    "    print(f\"Example {i}: shape = {feat.shape}, dtype = {feat.dtype}\")\n",
    "    print(f\"  Min value: {feat.min():.4f}, Max value: {feat.max():.4f}\")\n",
    "    print(f\"  Mean: {feat.mean():.4f}, Std: {feat.std():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different similarity thresholds:\n",
      "\n",
      "Threshold 0.5: 5/5 examples detected\n",
      "Threshold 0.6: 5/5 examples detected\n",
      "Threshold 0.7: 5/5 examples detected\n",
      "Threshold 0.8: 5/5 examples detected\n",
      "Threshold 0.9: 5/5 examples detected\n"
     ]
    }
   ],
   "source": [
    "# Test 8: Try different similarity thresholds\n",
    "print(\"Testing different similarity thresholds:\\n\")\n",
    "\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for threshold in thresholds:\n",
    "    detected = 0\n",
    "    for i, example_file in enumerate(example_files[:5]):\n",
    "        # Load example\n",
    "        pdf = fitz.open(os.path.join(example_forms_dir, example_file))\n",
    "        page = pdf[0]\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        pdf.close()\n",
    "        \n",
    "        # Extract features\n",
    "        inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features.cpu().numpy()\n",
    "            features = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
    "        \n",
    "        # Check similarity\n",
    "        max_sim = 0\n",
    "        for pos_feat in example_features:\n",
    "            sim = cosine_similarity(features, pos_feat)[0][0]\n",
    "            max_sim = max(max_sim, sim)\n",
    "        \n",
    "        if max_sim > threshold:\n",
    "            detected += 1\n",
    "    \n",
    "    print(f\"Threshold {threshold}: {detected}/5 examples detected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
