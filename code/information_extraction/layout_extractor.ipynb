{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc2ebb-4eb0-4e87-9020-92df3f97798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Government Contract Document Processing Pipeline\n",
    "# \n",
    "# This notebook provides a complete zero-shot solution for extracting structured information from government contract forms using transformer models.\n",
    "# \n",
    "# **Features:**\n",
    "# - Zero-shot extraction (no training required)\n",
    "# - CPU-optimized processing\n",
    "# - Handles PDF and image files\n",
    "# - Batch processing for large datasets\n",
    "# - Results visualization and export\n",
    "# \n",
    "# **Academic Use:** LayoutLMv3 license allows academic research usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deb3c53d-8eb6-4464-8231-cd9f773c0a91",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow.parquet' has no attribute 'ParquetWriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.19.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xgetsize\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_writer.py:618\u001b[0m\n\u001b[1;32m    612\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone writing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes\n\u001b[0;32m--> 618\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mParquetWriter\u001b[39;00m(ArrowWriter):\n\u001b[1;32m    619\u001b[0m     _WRITER_CLASS \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetWriter\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBeamWriter\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_writer.py:619\u001b[0m, in \u001b[0;36mParquetWriter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mParquetWriter\u001b[39;00m(ArrowWriter):\n\u001b[0;32m--> 619\u001b[0m     _WRITER_CLASS \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetWriter\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow.parquet' has no attribute 'ParquetWriter'"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43898f3c-b2fc-425a-8a71-3c7e7512b1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ torch already installed\n",
      "âœ“ transformers>=4.35.0 already installed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow.parquet' has no attribute 'ParquetWriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m             subprocess\u001b[38;5;241m.\u001b[39mcheck_call([sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall\u001b[39m\u001b[38;5;124m\"\u001b[39m, package])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Uncomment to install packages\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m install_packages()\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36minstall_packages\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m package \u001b[38;5;129;01min\u001b[39;00m packages:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28m__import__\u001b[39m(package\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.19.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xgetsize\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_writer.py:618\u001b[0m\n\u001b[1;32m    612\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone writing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes\n\u001b[0;32m--> 618\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mParquetWriter\u001b[39;00m(ArrowWriter):\n\u001b[1;32m    619\u001b[0m     _WRITER_CLASS \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetWriter\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBeamWriter\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_writer.py:619\u001b[0m, in \u001b[0;36mParquetWriter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mParquetWriter\u001b[39;00m(ArrowWriter):\n\u001b[0;32m--> 619\u001b[0m     _WRITER_CLASS \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetWriter\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow.parquet' has no attribute 'ParquetWriter'"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Installation\n",
    "# %%\n",
    "# Install required packages (run once)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages if not already installed\"\"\"\n",
    "    packages = [\n",
    "        \"torch\",\n",
    "        \"transformers>=4.35.0\",\n",
    "        \"datasets\",\n",
    "        \"Pillow\",\n",
    "        \"pdf2image\",\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"tqdm\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.split(\">=\")[0].split(\"==\")[0])\n",
    "            print(f\"âœ“ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Uncomment to install packages\n",
    "install_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae606d26-38b8-4c27-8dca-a0af45bffa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -Y\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow -Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25742631-51fc-46e9-a870-968ad9f86375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Import Libraries and Configuration\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pdf2image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import cv2  # For checkbox detection\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")\n",
    "\n",
    "# %%\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"microsoft/layoutlmv3-base\",  # LayoutLMv3 for form understanding\n",
    "    \"processor_name\": \"microsoft/layoutlmv3-base\",  # Matching processor\n",
    "    \"batch_size\": 5,  # Smaller batches for LayoutLMv3 (more memory intensive)\n",
    "    \"max_pages_per_doc\": 5,  # Limit pages to process per document\n",
    "    \"image_dpi\": 200,  # PDF to image conversion quality\n",
    "    \"timeout\": 300,  # 5 minutes max per document\n",
    "    \"save_raw_text\": False,  # Include raw OCR text in results\n",
    "    \"cache_dir\": \"./model_cache\",  # Model cache directory\n",
    "    \"use_checkbox_detection\": True,  # Enable checkbox detection\n",
    "    \"confidence_threshold\": 0.7,  # Minimum confidence for field extraction\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG[\"cache_dir\"], exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./sample_data\", exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration set\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Document Processing Class\n",
    "\n",
    "# %%\n",
    "class ContractProcessor:\n",
    "    \"\"\"LayoutLMv3-based government contract document processor with checkbox detection\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = None, device: str = \"cpu\"):\n",
    "        \"\"\"Initialize the processor with LayoutLMv3\"\"\"\n",
    "        \n",
    "        self.model_name = model_name or CONFIG[\"model_name\"]\n",
    "        self.processor_name = CONFIG[\"processor_name\"]\n",
    "        self.device = device\n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "        \n",
    "        print(f\"Initializing ContractProcessor with LayoutLMv3\")\n",
    "        print(f\"Model: {self.model_name}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load LayoutLMv3 model and processor\"\"\"\n",
    "        if self.model is not None:\n",
    "            return  # Already loaded\n",
    "            \n",
    "        print(\"Loading LayoutLMv3 model... This may take a few minutes on first run.\")\n",
    "        \n",
    "        try:\n",
    "            # Set cache directory\n",
    "            os.environ[\"TRANSFORMERS_CACHE\"] = CONFIG[\"cache_dir\"]\n",
    "            \n",
    "            # Load LayoutLMv3 processor and model\n",
    "            self.processor = LayoutLMv3Processor.from_pretrained(\n",
    "                self.processor_name,\n",
    "                cache_dir=CONFIG[\"cache_dir\"]\n",
    "            )\n",
    "            \n",
    "            self.model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "                device_map=self.device,\n",
    "                cache_dir=CONFIG[\"cache_dir\"]\n",
    "            )\n",
    "            \n",
    "            print(\"âœ“ LayoutLMv3 model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading LayoutLMv3 model: {e}\")\n",
    "            print(\"ðŸ’¡ Note: LayoutLMv3 requires more memory than simpler OCR models\")\n",
    "            raise\n",
    "    \n",
    "    def detect_checkboxes(self, image: Image.Image) -> Dict[str, bool]:\n",
    "        \"\"\"Detect checked boxes in contract forms using computer vision\"\"\"\n",
    "        \n",
    "        # Convert PIL image to OpenCV format\n",
    "        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        checkboxes = {\n",
    "            'professional_services': False,\n",
    "            'grant': False,\n",
    "            'lease': False,\n",
    "            'attorney': False,\n",
    "            'mou': False,\n",
    "            'qpa': False,\n",
    "            'procured_services': False,\n",
    "            'maintenance': False,\n",
    "            'license_agreement': False,\n",
    "            'amendment': False,\n",
    "            'renewal': False,\n",
    "            'other': False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Define checkbox regions based on typical form layout\n",
    "            # These coordinates may need adjustment based on your specific forms\n",
    "            checkbox_regions = {\n",
    "                'professional_services': (100, 400, 500, 450),  # Approximate region\n",
    "                'grant': (100, 450, 500, 500),\n",
    "                'lease': (100, 500, 500, 550),\n",
    "                # Add more regions as needed\n",
    "            }\n",
    "            \n",
    "            # Look for X marks, checkmarks, or filled boxes\n",
    "            for checkbox_name, (x1, y1, x2, y2) in checkbox_regions.items():\n",
    "                # Extract region of interest\n",
    "                if x2 < gray.shape[1] and y2 < gray.shape[0]:\n",
    "                    roi = gray[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Simple detection: look for dark pixels in checkbox area\n",
    "                    # This is a basic approach - can be enhanced with template matching\n",
    "                    if roi.size > 0:\n",
    "                        dark_pixel_ratio = np.sum(roi < 128) / roi.size\n",
    "                        if dark_pixel_ratio > 0.1:  # Threshold for \"marked\"\n",
    "                            checkboxes[checkbox_name] = True\n",
    "            \n",
    "            # Alternative: Text-based detection for more reliability\n",
    "            # Convert to text and look for patterns like \"X Professional/Personal Services\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Checkbox detection error: {e}\")\n",
    "        \n",
    "        return checkboxes\n",
    "    \n",
    "    def extract_with_layoutlm(self, image: Image.Image) -> Dict[str, Any]:\n",
    "        \"\"\"Extract structured information using LayoutLMv3\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Process image with LayoutLMv3\n",
    "            encoding = self.processor(image, return_tensors=\"pt\")\n",
    "            \n",
    "            # Get model predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoding)\n",
    "            \n",
    "            # Extract predictions (this is a simplified approach)\n",
    "            # In practice, you'd train LayoutLMv3 on your specific forms\n",
    "            predictions = outputs.logits.argmax(-1)\n",
    "            \n",
    "            # For now, we'll use OCR text extraction from LayoutLMv3\n",
    "            # and then apply our parsing logic\n",
    "            text = self.processor.tokenizer.batch_decode(\n",
    "                encoding.input_ids, skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            return {'text': text, 'confidence': 0.8}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LayoutLMv3 extraction error: {e}\")\n",
    "            return {'text': '', 'confidence': 0.0}\n",
    "    \n",
    "    def extract_text_from_image(self, image: Image.Image) -> str:\n",
    "        \"\"\"Extract text from image using LayoutLMv3 with form understanding\"\"\"\n",
    "        try:\n",
    "            # Load model if not already loaded\n",
    "            if self.model is None:\n",
    "                self.load_model()\n",
    "            \n",
    "            # Use LayoutLMv3 for structured extraction\n",
    "            result = self.extract_with_layoutlm(image)\n",
    "            \n",
    "            # Detect checkboxes\n",
    "            if CONFIG[\"use_checkbox_detection\"]:\n",
    "                checkboxes = self.detect_checkboxes(image)\n",
    "                \n",
    "                # Add checkbox information to text\n",
    "                checkbox_text = \"\\n=== CHECKBOX DETECTION ===\\n\"\n",
    "                for checkbox_name, is_checked in checkboxes.items():\n",
    "                    if is_checked:\n",
    "                        checkbox_text += f\"{checkbox_name}: CHECKED\\n\"\n",
    "                \n",
    "                result['text'] += checkbox_text\n",
    "            \n",
    "            return result['text']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LayoutLMv3 text extraction failed: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def parse_contract_fields(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Parse contract fields using enhanced patterns with section awareness\"\"\"\n",
    "        \n",
    "        fields = {\n",
    "            'eds_number': '',                    # 1. EDS Number\n",
    "            'date_prepared': '',                 # 2. Date prepared\n",
    "            'contracts_leases': '',              # 3. Contracts & Leases\n",
    "            'account_number': '',                # 4. Account Number\n",
    "            'account_name': '',                  # 5. Account Name\n",
    "            'total_amount_this_action': '',      # 6. Total amount this action\n",
    "            'new_contract_total': '',            # 7. New contract total\n",
    "            'revenue_generated_this_action': '', # 8. Revenue generated this action\n",
    "            'revenue_generated_total_contract': '', # 9. Revenue generated total contract\n",
    "            'from_date': '',                     # 11. From (month, day, year)\n",
    "            'to_date': '',                       # 12. To (month, day, year)\n",
    "            'method_source_selection': '',       # 13. Method of source selection\n",
    "            'email_address': '',                 # 19. E-mail address\n",
    "            'vendor_id': '',                     # 23. Vendor ID #\n",
    "            'vendor_name': '',                   # 24. Name\n",
    "            'primary_vendor_mwbe': '',           # 29. Primary Vendor: M/WBE\n",
    "            'sub_vendor_mwbe': '',               # 31. Sub Vendor:M/WBE\n",
    "            'renewal_language': '',              # 33. Is there Renewal Language in the document?\n",
    "            'termination_convenience_clause': '',# 34. Is there a \"Termination for Convenience\" clause\n",
    "            'description_work_justification': '' # 37. Description of work and justification for spending money\n",
    "        }\n",
    "        \n",
    "        # Extract sections for better context\n",
    "        sections = self.extract_sections(text)\n",
    "        \n",
    "        # Enhanced regex patterns with section awareness\n",
    "        patterns = {\n",
    "            'eds_number': [\n",
    "                r'1\\.\\s*EDS Number[:\\s]*([^\\n\\r]+)',\n",
    "                r'EDS Number[:\\s]*([A-Z0-9\\-]+)',\n",
    "                r'([A-Z]\\d{2}-\\d+-\\d{4})',\n",
    "            ],\n",
    "            'date_prepared': [\n",
    "                r'2\\.\\s*Date prepared[:\\s]*([^\\n\\r]+)',\n",
    "                r'Date prepared[:\\s]*([^\\n\\r]+)',\n",
    "                r'(\\d{1,2}\\/\\d{1,2}\\/\\d{4})',\n",
    "            ],\n",
    "            'contracts_leases': [\n",
    "                r'3\\.\\s*CONTRACTS & LEASES[:\\s]*([^\\n\\r]+)',\n",
    "                r'Professional/Personal Services.*?CHECKED',\n",
    "                r'Grant.*?CHECKED',\n",
    "                r'Lease.*?CHECKED',\n",
    "                # Will be enhanced by checkbox detection\n",
    "            ],\n",
    "            'account_number': [\n",
    "                r'4\\.\\s*Account Number[:\\s]*([^\\n\\r]+)',\n",
    "                r'Account Number[:\\s]*([^\\n\\r]+)',\n",
    "                r'(\\d{4}-?\\d+)',\n",
    "            ],\n",
    "            'account_name': [\n",
    "                r'5\\.\\s*Account Name[:\\s]*([^\\n\\r]+)',\n",
    "                r'Account Name[:\\s]*([^\\n\\r]+)',\n",
    "            ],\n",
    "            'total_amount_this_action': [\n",
    "                r'6\\.\\s*Total amount this action[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "                r'Total amount this action[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "                r'6\\.\\s*Total amount[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "            ],\n",
    "            'new_contract_total': [\n",
    "                r'7\\.\\s*New contract total[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "                r'New contract total[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "            ],\n",
    "            'revenue_generated_this_action': [\n",
    "                r'8\\.\\s*Revenue generated this action[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "                r'Revenue generated this action[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "            ],\n",
    "            'revenue_generated_total_contract': [\n",
    "                r'9\\.\\s*Revenue generated total contract[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "                r'Revenue generated total contract[:\\s]*\\$?([\\d,]+\\.?\\d*)',\n",
    "            ],\n",
    "            'from_date': [\n",
    "                r'11\\.\\s*From \\(month, day, year\\)[:\\s]*([^\\n\\r]+)',\n",
    "                r'From \\(month, day, year\\)[:\\s]*([^\\n\\r]+)',\n",
    "                r'11\\.\\s*From.*?(\\d{1,2}\\/\\d{1,2}\\/\\d{4})',\n",
    "                r'(\\d{1,2}\\/\\d{1,2}\\/\\d{4})\\s+\\d{1,2}\\/\\d{1,2}\\/\\d{4}',  # First date in pair\n",
    "            ],\n",
    "            'to_date': [\n",
    "                r'12\\.\\s*To \\(month, day, year\\)[:\\s]*([^\\n\\r]+)',\n",
    "                r'To \\(month, day, year\\)[:\\s]*([^\\n\\r]+)',\n",
    "                r'12\\.\\s*To.*?(\\d{1,2}\\/\\d{1,2}\\/\\d{4})',\n",
    "                r'\\d{1,2}\\/\\d{1,2}\\/\\d{4}\\s+(\\d{1,2}\\/\\d{1,2}\\/\\d{4})',  # Second date in pair\n",
    "            ],\n",
    "            'method_source_selection': [\n",
    "                r'13\\.\\s*Method of source selection[:\\s]*([^\\n\\r]+)',\n",
    "                r'Method of source selection[:\\s]*([^\\n\\r]+)',\n",
    "                r'(Bid/Quotation|RFP|Emergency|Negotiated|Special Procurement)',\n",
    "            ],\n",
    "            'email_address': [\n",
    "                r'19\\.\\s*E-mail address[:\\s]*([^\\n\\r]+)',\n",
    "                r'E-mail address[:\\s]*([^\\n\\r]+)',\n",
    "                r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})',\n",
    "            ],\n",
    "            'vendor_id': [\n",
    "                r'23\\.\\s*Vendor ID #[:\\s]*([^\\n\\r]+)',\n",
    "                r'Vendor ID #[:\\s]*([^\\n\\r]+)',\n",
    "                r'VENDOR INFORMATION.*?(\\d{10})',\n",
    "                r'(\\d{10})',\n",
    "            ],\n",
    "            'vendor_name': [\n",
    "                r'24\\.\\s*Name[:\\s]*([^\\n\\r]+)',\n",
    "                r'VENDOR INFORMATION.*?24\\.\\s*Name[:\\s]*([^\\n\\r]+)',\n",
    "                r'VENDOR INFORMATION.*?Name[:\\s]*([^\\n\\r]+)',\n",
    "            ],\n",
    "            'primary_vendor_mwbe': [\n",
    "                r'29\\.\\s*Primary Vendor: M/WBE[:\\s]*([^\\n\\r]+)',\n",
    "                r'Primary Vendor: M/WBE[:\\s]*([^\\n\\r]+)',\n",
    "                r'Primary.*?M/WBE.*?(Yes|No)',\n",
    "            ],\n",
    "            'sub_vendor_mwbe': [\n",
    "                r'31\\.\\s*Sub Vendor:M/WBE[:\\s]*([^\\n\\r]+)',\n",
    "                r'Sub Vendor:M/WBE[:\\s]*([^\\n\\r]+)',\n",
    "                r'Sub.*?M/WBE.*?(Yes|No)',\n",
    "            ],\n",
    "            'renewal_language': [\n",
    "                r'33\\.\\s*Is there Renewal Language in the document\\?[:\\s]*([^\\n\\r]+)',\n",
    "                r'Is there Renewal Language in the document\\?[:\\s]*([^\\n\\r]+)',\n",
    "                r'Renewal Language.*?(Yes|No)',\n",
    "            ],\n",
    "            'termination_convenience_clause': [\n",
    "                r'34\\.\\s*Is there a \"Termination for Convenience\" clause in the document\\?[:\\s]*([^\\n\\r]+)',\n",
    "                r'Is there a \"Termination for Convenience\" clause.*?(Yes|No)',\n",
    "                r'Termination.*?Convenience.*?(Yes|No)',\n",
    "            ],\n",
    "            'description_work_justification': [\n",
    "                r'37\\.\\s*Description of work and justification for spending money[:\\s]*([^\\n\\r]{1,500})',\n",
    "                r'Description of work and justification for spending money[:\\s]*([^\\n\\r]{1,500})',\n",
    "                r'Description.*?work.*?justification[:\\s]*([^\\n\\r]{1,500})',\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Extract fields using patterns with section context\n",
    "        for field_name, field_patterns in patterns.items():\n",
    "            for pattern in field_patterns:\n",
    "                # Try pattern on full text first\n",
    "                match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "                if match:\n",
    "                    extracted_value = match.group(1).strip() if match.lastindex >= 1 else match.group(0).strip()\n",
    "                    if extracted_value and len(extracted_value) < 500:  # Sanity check\n",
    "                        fields[field_name] = extracted_value\n",
    "                        break\n",
    "                \n",
    "                # If not found, try in relevant section\n",
    "                relevant_section = self.get_relevant_section(field_name, sections)\n",
    "                if relevant_section:\n",
    "                    match = re.search(pattern, relevant_section, re.IGNORECASE | re.MULTILINE)\n",
    "                    if match:\n",
    "                        extracted_value = match.group(1).strip() if match.lastindex >= 1 else match.group(0).strip()\n",
    "                        if extracted_value and len(extracted_value) < 500:\n",
    "                            fields[field_name] = extracted_value\n",
    "                            break\n",
    "        \n",
    "        # Post-process checkbox information\n",
    "        fields = self.process_checkbox_info(text, fields)\n",
    "        \n",
    "        return fields\n",
    "    \n",
    "    def extract_sections(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract different sections of the form for better context\"\"\"\n",
    "        sections = {}\n",
    "        \n",
    "        section_patterns = {\n",
    "            'agency_info': (r'AGENCY INFORMATION', r'COURIER INFORMATION'),\n",
    "            'courier_info': (r'COURIER INFORMATION', r'VENDOR INFORMATION'),\n",
    "            'vendor_info': (r'VENDOR INFORMATION', r'FISCAL INFORMATION'),\n",
    "            'fiscal_info': (r'FISCAL INFORMATION', r'TIME PERIOD'),\n",
    "            'time_period': (r'TIME PERIOD COVERED', r'Method of source selection'),\n",
    "            'contracts_leases': (r'CONTRACTS & LEASES', r'FISCAL INFORMATION'),\n",
    "        }\n",
    "        \n",
    "        for section_name, (start_pattern, end_pattern) in section_patterns.items():\n",
    "            start_match = re.search(start_pattern, text, re.IGNORECASE)\n",
    "            end_match = re.search(end_pattern, text, re.IGNORECASE)\n",
    "            \n",
    "            if start_match:\n",
    "                start_pos = start_match.end()\n",
    "                end_pos = end_match.start() if end_match else len(text)\n",
    "                sections[section_name] = text[start_pos:end_pos]\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def get_relevant_section(self, field_name: str, sections: Dict[str, str]) -> str:\n",
    "        \"\"\"Get the most relevant section for a field\"\"\"\n",
    "        field_section_map = {\n",
    "            'vendor_id': 'vendor_info',\n",
    "            'vendor_name': 'vendor_info',\n",
    "            'account_number': 'fiscal_info',\n",
    "            'account_name': 'fiscal_info',\n",
    "            'total_amount_this_action': 'fiscal_info',\n",
    "            'new_contract_total': 'fiscal_info',\n",
    "            'from_date': 'time_period',\n",
    "            'to_date': 'time_period',\n",
    "            'contracts_leases': 'contracts_leases',\n",
    "        }\n",
    "        \n",
    "        section_name = field_section_map.get(field_name)\n",
    "        return sections.get(section_name, '') if section_name else ''\n",
    "    \n",
    "    def process_checkbox_info(self, text: str, fields: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"Process checkbox detection results\"\"\"\n",
    "        \n",
    "        # Look for checkbox indicators in text\n",
    "        checkbox_indicators = [\n",
    "            'professional_services_checked',\n",
    "            'grant_checked', \n",
    "            'lease_checked',\n",
    "            'other_checked'\n",
    "        ]\n",
    "        \n",
    "        contract_types = []\n",
    "        \n",
    "        # Check for each type\n",
    "        if re.search(r'Professional/Personal Services.*?CHECKED', text, re.IGNORECASE):\n",
    "            contract_types.append('Professional/Personal Services')\n",
    "        elif re.search(r'X.*?Professional/Personal Services|Professional/Personal Services.*?X', text, re.IGNORECASE):\n",
    "            contract_types.append('Professional/Personal Services')\n",
    "            \n",
    "        if re.search(r'Grant.*?CHECKED', text, re.IGNORECASE):\n",
    "            contract_types.append('Grant')\n",
    "        elif re.search(r'X.*?Grant|Grant.*?X', text, re.IGNORECASE):\n",
    "            contract_types.append('Grant')\n",
    "            \n",
    "        if re.search(r'Other.*?CHECKED', text, re.IGNORECASE):\n",
    "            # Look for the \"Other\" specification\n",
    "            other_match = re.search(r'Other[:\\s]*([^\\n\\r]+)', text, re.IGNORECASE)\n",
    "            if other_match:\n",
    "                contract_types.append(f\"Other: {other_match.group(1).strip()}\")\n",
    "        \n",
    "        # Update contracts_leases field\n",
    "        if contract_types:\n",
    "            fields['contracts_leases'] = ', '.join(contract_types)\n",
    "        \n",
    "        return fields\n",
    "        \n",
    "        # Extract fields using patterns\n",
    "        for field_name, field_patterns in patterns.items():\n",
    "            for pattern in field_patterns:\n",
    "                match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "                if match:\n",
    "                    extracted_value = match.group(1).strip()\n",
    "                    if extracted_value and len(extracted_value) < 200:  # Sanity check\n",
    "                        fields[field_name] = extracted_value\n",
    "                        break\n",
    "        \n",
    "        return fields\n",
    "    \n",
    "    def process_document(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single contract document\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'file_path': file_path,\n",
    "            'filename': os.path.basename(file_path),\n",
    "            'status': 'processing',\n",
    "            'processing_time': 0,\n",
    "            'error': None,\n",
    "            'pages_processed': 0,\n",
    "            'extracted_fields': {},\n",
    "            'raw_text': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Load model if not already loaded\n",
    "            if self.model is None:\n",
    "                self.load_model()\n",
    "            \n",
    "            # Handle different file types\n",
    "            if file_path.lower().endswith('.pdf'):\n",
    "                images = self.pdf_to_images(file_path)\n",
    "            else:\n",
    "                # Assume image file\n",
    "                images = [Image.open(file_path)]\n",
    "            \n",
    "            if not images:\n",
    "                raise ValueError(\"No images extracted from document\")\n",
    "            \n",
    "            # Process all pages\n",
    "            all_text_parts = []\n",
    "            for i, image in enumerate(images[:CONFIG[\"max_pages_per_doc\"]]):\n",
    "                page_text = self.extract_text_from_image(image)\n",
    "                if page_text:\n",
    "                    all_text_parts.append(f\"=== Page {i+1} ===\\n{page_text}\")\n",
    "            \n",
    "            # Combine all text\n",
    "            combined_text = \"\\n\\n\".join(all_text_parts)\n",
    "            \n",
    "            # Parse structured fields\n",
    "            extracted_fields = self.parse_contract_fields(combined_text)\n",
    "            \n",
    "            # Update result\n",
    "            result.update({\n",
    "                'status': 'success',\n",
    "                'extracted_fields': extracted_fields,\n",
    "                'pages_processed': len(images),\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'raw_text': combined_text if CONFIG[\"save_raw_text\"] else ''\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            result.update({\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'processing_time': time.time() - start_time\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_batch(self, file_paths: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a batch of documents with progress bar\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        with tqdm(total=len(file_paths), desc=\"Processing contracts\") as pbar:\n",
    "            for file_path in file_paths:\n",
    "                result = self.process_document(file_path)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update progress bar\n",
    "                status_icon = \"âœ“\" if result['status'] == 'success' else \"âŒ\"\n",
    "                pbar.set_postfix({\n",
    "                    'file': os.path.basename(file_path)[:20],\n",
    "                    'status': status_icon\n",
    "                })\n",
    "                pbar.update(1)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# %%\n",
    "# Initialize the processor\n",
    "processor = ContractProcessor()\n",
    "print(\"âœ“ ContractProcessor initialized\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Test on Sample Documents\n",
    "\n",
    "# %%\n",
    "# Test the processor on a single document\n",
    "def test_single_document(file_path: str):\n",
    "    \"\"\"Test processing on a single document\"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        print(\"\\nðŸ’¡ To test the processor:\")\n",
    "        print(\"1. Place a sample contract PDF in the './sample_data/' folder\")\n",
    "        print(\"2. Update the file_path variable below\")\n",
    "        print(\"3. Run this cell again\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ”„ Testing on: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    result = processor.process_document(file_path)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    print(f\"Processing time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"Pages processed: {result['pages_processed']}\")\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        print(\"\\nðŸ“‹ Extracted Fields:\")\n",
    "        for field, value in result['extracted_fields'].items():\n",
    "            if value:  # Only show non-empty fields\n",
    "                print(f\"  {field}: {value}\")\n",
    "        \n",
    "        if not any(result['extracted_fields'].values()):\n",
    "            print(\"  âš ï¸ No fields extracted. Raw text preview:\")\n",
    "            print(f\"  {result['raw_text'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Error: {result['error']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with a sample file (update path as needed)\n",
    "sample_file = \"./sample_data/sample_contract.pdf\"\n",
    "\n",
    "# Uncomment to test with your own file:\n",
    "# test_result = test_single_document(sample_file)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Batch Processing Function\n",
    "\n",
    "# %%\n",
    "def process_document_directory(input_dir: str, file_extensions: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Process all documents in a directory\"\"\"\n",
    "    \n",
    "    if file_extensions is None:\n",
    "        file_extensions = ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.tif']\n",
    "    \n",
    "    # Find all contract files\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in file_extensions):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"âŒ No files found in {input_dir} with extensions {file_extensions}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"ðŸ“ Found {len(file_paths)} files to process\")\n",
    "    print(f\"ðŸ“Š Processing in batches of {CONFIG['batch_size']}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(0, len(file_paths), CONFIG['batch_size']):\n",
    "        batch_files = file_paths[i:i + CONFIG['batch_size']]\n",
    "        batch_num = i // CONFIG['batch_size'] + 1\n",
    "        total_batches = (len(file_paths) + CONFIG['batch_size'] - 1) // CONFIG['batch_size']\n",
    "        \n",
    "        print(f\"\\nðŸ”„ Processing batch {batch_num}/{total_batches}\")\n",
    "        \n",
    "        batch_results = processor.process_batch(batch_files)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Show batch summary\n",
    "        successful = sum(1 for r in batch_results if r['status'] == 'success')\n",
    "        print(f\"   âœ“ {successful}/{len(batch_results)} successful\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = create_results_dataframe(all_results)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_successful = (df['status'] == 'success').sum()\n",
    "    success_rate = (total_successful / len(df)) * 100\n",
    "    avg_time = df[df['status'] == 'success']['processing_time'].mean()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š PROCESSING COMPLETE\")\n",
    "    print(f\"   Total files: {len(df)}\")\n",
    "    print(f\"   Successful: {total_successful}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   Average time: {avg_time:.2f}s per document\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_results_dataframe(results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"Convert results list to structured DataFrame\"\"\"\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for result in results:\n",
    "        # Base record\n",
    "        record = {\n",
    "            'filename': result['filename'],\n",
    "            'file_path': result['file_path'],\n",
    "            'status': result['status'],\n",
    "            'processing_time': result['processing_time'],\n",
    "            'pages_processed': result['pages_processed'],\n",
    "            'error': result.get('error', '')\n",
    "        }\n",
    "        \n",
    "        # Add extracted fields\n",
    "        if result['status'] == 'success':\n",
    "            record.update(result['extracted_fields'])\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Process Your Documents\n",
    "\n",
    "# %%\n",
    "# MAIN PROCESSING SECTION\n",
    "# Update this path to point to your contract documents\n",
    "INPUT_DIRECTORY = \"./sample_data\"\n",
    "\n",
    "# Process documents (uncomment when ready)\n",
    "print(\"ðŸš€ Ready to process documents!\")\n",
    "print(f\"Input directory: {INPUT_DIRECTORY}\")\n",
    "print(f\"Configuration: {CONFIG}\")\n",
    "\n",
    "# Uncomment the following lines to start processing:\n",
    "# df_results = process_document_directory(INPUT_DIRECTORY)\n",
    "\n",
    "# For now, let's create some sample results for demonstration\n",
    "print(\"\\nðŸ’¡ To process your documents:\")\n",
    "print(\"1. Place your contract files in a directory\")\n",
    "print(\"2. Update INPUT_DIRECTORY above\")\n",
    "print(\"3. Uncomment the processing line\")\n",
    "print(\"4. Run this cell\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Results Analysis and Visualization\n",
    "\n",
    "# %%\n",
    "# Create sample data for demonstration (replace with actual results)\n",
    "def create_sample_results():\n",
    "    \"\"\"Create sample results for demonstration purposes\"\"\"\n",
    "    \n",
    "    sample_data = [\n",
    "        {\n",
    "            'filename': 'contract_001.pdf',\n",
    "            'status': 'success',\n",
    "            'processing_time': 25.3,\n",
    "            'pages_processed': 2,\n",
    "            'eds_number': 'C44P-5-121',\n",
    "            'total_amount_this_action': '150000.00',\n",
    "            'new_contract_total': '150000.00',\n",
    "            'vendor_name': 'ABC Corporation',\n",
    "            'from_date': '01/01/2023',\n",
    "            'to_date': '12/31/2023',\n",
    "            'account_number': '6000-154600',\n",
    "            'vendor_id': '0000054625'\n",
    "        },\n",
    "        {\n",
    "            'filename': 'contract_002.pdf', \n",
    "            'status': 'success',\n",
    "            'processing_time': 18.7,\n",
    "            'pages_processed': 1,\n",
    "            'eds_number': 'C45A-6-789',\n",
    "            'total_amount_this_action': '75000.00',\n",
    "            'vendor_name': 'XYZ Services',\n",
    "            'from_date': '03/15/2023',\n",
    "            'to_date': '03/14/2024',\n",
    "            'email_address': 'contract@xyz.com'\n",
    "        },\n",
    "        {\n",
    "            'filename': 'contract_003.pdf',\n",
    "            'status': 'failed',\n",
    "            'processing_time': 45.1,\n",
    "            'pages_processed': 0,\n",
    "            'error': 'PDF conversion failed'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(sample_data)\n",
    "\n",
    "# Use sample data for now (replace with df_results from actual processing)\n",
    "df_results = create_sample_results()\n",
    "print(\"ðŸ“Š Sample results loaded for demonstration\")\n",
    "\n",
    "# %%\n",
    "def analyze_results(df: pd.DataFrame):\n",
    "    \"\"\"Analyze and visualize processing results\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"ðŸ“ˆ RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_docs = len(df)\n",
    "    successful = (df['status'] == 'success').sum()\n",
    "    failed = (df['status'] == 'failed').sum()\n",
    "    success_rate = (successful / total_docs) * 100\n",
    "    \n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if successful > 0:\n",
    "        successful_df = df[df['status'] == 'success']\n",
    "        avg_time = successful_df['processing_time'].mean()\n",
    "        avg_pages = successful_df['pages_processed'].mean()\n",
    "        \n",
    "        print(f\"Average processing time: {avg_time:.2f}s\")\n",
    "        print(f\"Average pages per document: {avg_pages:.1f}\")\n",
    "    \n",
    "    # Field extraction rates\n",
    "    print(f\"\\nðŸ“‹ Field Extraction Rates:\")\n",
    "    field_columns = [\n",
    "        'eds_number', 'date_prepared', 'account_number', 'account_name',\n",
    "        'total_amount_this_action', 'new_contract_total', 'from_date', 'to_date',\n",
    "        'vendor_id', 'vendor_name', 'email_address'\n",
    "    ]\n",
    "    \n",
    "    for field in field_columns:\n",
    "        if field in df.columns:\n",
    "            non_empty = df[field].notna() & (df[field] != '')\n",
    "            rate = (non_empty.sum() / successful) * 100 if successful > 0 else 0\n",
    "            print(f\"  {field}: {rate:.1f}%\")\n",
    "    \n",
    "    # Visualizations\n",
    "    create_visualizations(df)\n",
    "\n",
    "def create_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations of the results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Contract Processing Results Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Success/Failure pie chart\n",
    "    status_counts = df['status'].value_counts()\n",
    "    axes[0, 0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Processing Status Distribution')\n",
    "    \n",
    "    # 2. Processing time histogram\n",
    "    successful_df = df[df['status'] == 'success']\n",
    "    if not successful_df.empty:\n",
    "        axes[0, 1].hist(successful_df['processing_time'], bins=10, alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Processing Time (seconds)')\n",
    "        axes[0, 1].set_ylabel('Number of Documents')\n",
    "        axes[0, 1].set_title('Processing Time Distribution')\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No successful\\nprocessing times', \n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Processing Time Distribution')\n",
    "    \n",
    "    # 3. Field extraction success rates\n",
    "    field_columns = [\n",
    "        'eds_number', 'date_prepared', 'account_number', 'total_amount_this_action',\n",
    "        'vendor_name', 'from_date', 'to_date', 'email_address'\n",
    "    ]\n",
    "    \n",
    "    field_rates = []\n",
    "    field_names = []\n",
    "    \n",
    "    for field in field_columns:\n",
    "        if field in df.columns:\n",
    "            non_empty = df[field].notna() & (df[field] != '')\n",
    "            rate = (non_empty.sum() / len(successful_df)) * 100 if len(successful_df) > 0 else 0\n",
    "            field_rates.append(rate)\n",
    "            field_names.append(field.replace('_', ' ').title())\n",
    "    \n",
    "    if field_rates:\n",
    "        bars = axes[1, 0].bar(field_names, field_rates)\n",
    "        axes[1, 0].set_ylabel('Extraction Rate (%)')\n",
    "        axes[1, 0].set_title('Field Extraction Success Rates')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Color bars based on success rate\n",
    "        for bar, rate in zip(bars, field_rates):\n",
    "            if rate >= 80:\n",
    "                bar.set_color('green')\n",
    "            elif rate >= 60:\n",
    "                bar.set_color('orange')\n",
    "            else:\n",
    "                bar.set_color('red')\n",
    "    \n",
    "    # 4. Pages processed distribution\n",
    "    if not successful_df.empty and 'pages_processed' in successful_df.columns:\n",
    "        pages_counts = successful_df['pages_processed'].value_counts().sort_index()\n",
    "        axes[1, 1].bar(pages_counts.index, pages_counts.values)\n",
    "        axes[1, 1].set_xlabel('Number of Pages')\n",
    "        axes[1, 1].set_ylabel('Number of Documents')\n",
    "        axes[1, 1].set_title('Pages Processed Distribution')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No page count\\ndata available', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Pages Processed Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze the results\n",
    "analyze_results(df_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Export Results\n",
    "\n",
    "# %%\n",
    "def export_results(df: pd.DataFrame, output_dir: str = \"./results\"):\n",
    "    \"\"\"Export results to various formats\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export to CSV\n",
    "    csv_path = os.path.join(output_dir, \"contract_extraction_results.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ“ Results exported to CSV: {csv_path}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_path = os.path.join(output_dir, \"contract_extraction_results.json\")\n",
    "    df.to_json(json_path, orient='records', indent=2)\n",
    "    print(f\"âœ“ Results exported to JSON: {json_path}\")\n",
    "    \n",
    "    # Export to Excel with multiple sheets\n",
    "    excel_path = os.path.join(output_dir, \"contract_extraction_results.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path) as writer:\n",
    "        # All results\n",
    "        df.to_excel(writer, sheet_name='All_Results', index=False)\n",
    "        \n",
    "        # Successful extractions only\n",
    "        successful_df = df[df['status'] == 'success']\n",
    "        if not successful_df.empty:\n",
    "            successful_df.to_excel(writer, sheet_name='Successful_Extractions', index=False)\n",
    "        \n",
    "        # Failed extractions\n",
    "        failed_df = df[df['status'] == 'failed']\n",
    "        if not failed_df.empty:\n",
    "            failed_df.to_excel(writer, sheet_name='Failed_Extractions', index=False)\n",
    "        \n",
    "        # Summary statistics\n",
    "        summary_stats = create_summary_stats(df)\n",
    "        summary_stats.to_excel(writer, sheet_name='Summary', index=True)\n",
    "    \n",
    "    print(f\"âœ“ Results exported to Excel: {excel_path}\")\n",
    "    \n",
    "    # Create processing report\n",
    "    report_path = os.path.join(output_dir, \"processing_report.txt\")\n",
    "    create_processing_report(df, report_path)\n",
    "    print(f\"âœ“ Processing report: {report_path}\")\n",
    "\n",
    "def create_summary_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create summary statistics DataFrame\"\"\"\n",
    "    \n",
    "    stats = {\n",
    "        'Total Documents': len(df),\n",
    "        'Successful Extractions': (df['status'] == 'success').sum(),\n",
    "        'Failed Extractions': (df['status'] == 'failed').sum(),\n",
    "        'Success Rate (%)': ((df['status'] == 'success').sum() / len(df)) * 100,\n",
    "    }\n",
    "    \n",
    "    if (df['status'] == 'success').any():\n",
    "        successful_df = df[df['status'] == 'success']\n",
    "        stats.update({\n",
    "            'Average Processing Time (s)': successful_df['processing_time'].mean(),\n",
    "            'Total Processing Time (s)': successful_df['processing_time'].sum(),\n",
    "            'Average Pages per Document': successful_df['pages_processed'].mean(),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(list(stats.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "def create_processing_report(df: pd.DataFrame, output_path: str):\n",
    "    \"\"\"Create a detailed processing report\"\"\"\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"GOVERNMENT CONTRACT PROCESSING REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        f.write(\"PROCESSING SUMMARY\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        f.write(f\"Total documents processed: {len(df)}\\n\")\n",
    "        f.write(f\"Successful extractions: {(df['status'] == 'success').sum()}\\n\")\n",
    "        f.write(f\"Failed extractions: {(df['status'] == 'failed').sum()}\\n\")\n",
    "        f.write(f\"Success rate: {((df['status'] == 'success').sum() / len(df)) * 100:.1f}%\\n\\n\")\n",
    "        \n",
    "        # Field extraction rates\n",
    "        f.write(\"FIELD EXTRACTION RATES\\n\")\n",
    "        f.write(\"-\" * 25 + \"\\n\")\n",
    "        \n",
    "        field_columns = [\n",
    "            'eds_number', 'date_prepared', 'account_number', 'account_name',\n",
    "            'total_amount_this_action', 'new_contract_total', 'from_date', 'to_date',\n",
    "            'vendor_id', 'vendor_name', 'email_address', 'method_source_selection'\n",
    "        ]\n",
    "        \n",
    "        successful_count = (df['status'] == 'success').sum()\n",
    "        \n",
    "        for field in field_columns:\n",
    "            if field in df.columns:\n",
    "                non_empty = df[field].notna() & (df[field] != '')\n",
    "                rate = (non_empty.sum() / successful_count) * 100 if successful_count > 0 else 0\n",
    "                f.write(f\"{field.replace('_', ' ').title()}: {rate:.1f}%\\n\")\n",
    "        \n",
    "        # Failed files\n",
    "        failed_df = df[df['status'] == 'failed']\n",
    "        if not failed_df.empty:\n",
    "            f.write(f\"\\nFAILED EXTRACTIONS ({len(failed_df)} files)\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for _, row in failed_df.iterrows():\n",
    "                f.write(f\"File: {row['filename']}\\n\")\n",
    "                f.write(f\"Error: {row.get('error', 'Unknown error')}\\n\\n\")\n",
    "\n",
    "# Export results\n",
    "export_results(df_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Advanced Processing Options\n",
    "\n",
    "# %%\n",
    "def process_large_dataset(input_dir: str, checkpoint_interval: int = 100):\n",
    "    \"\"\"Process large datasets with checkpointing for recovery\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ LARGE DATASET PROCESSING MODE\")\n",
    "    print(\"Features: Checkpointing, Progress saving, Error recovery\")\n",
    "    \n",
    "    # Find all files\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.pdf', '.png', '.jpg', '.jpeg')):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"ðŸ“ Found {len(file_paths)} files to process\")\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    checkpoint_path = \"./results/processing_checkpoint.json\"\n",
    "    processed_files = set()\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "            processed_files = set(checkpoint_data.get('processed_files', []))\n",
    "        print(f\"ðŸ“‹ Resuming from checkpoint: {len(processed_files)} files already processed\")\n",
    "    \n",
    "    # Filter out already processed files\n",
    "    remaining_files = [f for f in file_paths if f not in processed_files]\n",
    "    print(f\"ðŸ”„ {len(remaining_files)} files remaining to process\")\n",
    "    \n",
    "    if not remaining_files:\n",
    "        print(\"âœ… All files already processed!\")\n",
    "        return load_existing_results()\n",
    "    \n",
    "    # Process remaining files\n",
    "    all_results = load_existing_results() if processed_files else []\n",
    "    \n",
    "    for i, file_path in enumerate(remaining_files):\n",
    "        print(f\"Processing {i+1}/{len(remaining_files)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        result = processor.process_document(file_path)\n",
    "        all_results.append(result)\n",
    "        processed_files.add(file_path)\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (i + 1) % checkpoint_interval == 0:\n",
    "            save_checkpoint(processed_files, all_results, checkpoint_path)\n",
    "            print(f\"ðŸ“‹ Checkpoint saved at {i+1} files\")\n",
    "    \n",
    "    # Final save\n",
    "    save_checkpoint(processed_files, all_results, checkpoint_path)\n",
    "    \n",
    "    # Convert to DataFrame and return\n",
    "    df_results = create_results_dataframe(all_results)\n",
    "    return df_results\n",
    "\n",
    "def save_checkpoint(processed_files: set, results: list, checkpoint_path: str):\n",
    "    \"\"\"Save processing checkpoint\"\"\"\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        'processed_files': list(processed_files),\n",
    "        'total_processed': len(processed_files),\n",
    "        'last_updated': time.time()\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    with open(checkpoint_path, 'w') as f:\n",
    "        json.dump(checkpoint_data, f, indent=2)\n",
    "    \n",
    "    # Save results\n",
    "    results_path = \"./results/interim_results.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "def load_existing_results() -> list:\n",
    "    \"\"\"Load existing results from checkpoint\"\"\"\n",
    "    \n",
    "    results_path = \"./results/interim_results.json\"\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# df_large = process_large_dataset(\"./your_large_dataset\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Custom Field Patterns\n",
    "\n",
    "# %%\n",
    "def add_custom_extraction_patterns():\n",
    "    \"\"\"Add custom extraction patterns for specific contract types\"\"\"\n",
    "    \n",
    "    # Example: Department of Defense specific patterns\n",
    "    dod_patterns = {\n",
    "        'contract_number': [\n",
    "            r'Contract(?:\\s+No\\.?|\\s+Number)[:\\s]*([A-Z0-9\\-]+)',\n",
    "            r'([A-Z]{2}\\d{2}-\\d{2}-[A-Z]-\\d{4})',  # Standard DoD format\n",
    "        ],\n",
    "        'po_number': [\n",
    "            r'P\\.?O\\.?\\s*(?:Number|No\\.?)[:\\s]*([A-Z0-9\\-]+)',\n",
    "            r'Purchase Order[:\\s]*([A-Z0-9\\-]+)',\n",
    "        ],\n",
    "        'naics_code': [\n",
    "            r'NAICS[:\\s]*(\\d{6})',\n",
    "            r'Industry Code[:\\s]*(\\d{6})',\n",
    "        ],\n",
    "        'small_business': [\n",
    "            r'Small Business[:\\s]*(Yes|No|Y|N)',\n",
    "            r'SB Status[:\\s]*(Yes|No|Y|N)',\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Healthcare/FDA specific patterns\n",
    "    fda_patterns = {\n",
    "        'drug_code': [\n",
    "            r'NDC[:\\s]*(\\d{5}-\\d{4}-\\d{2})',\n",
    "            r'Drug Code[:\\s]*([A-Z0-9\\-]+)',\n",
    "        ],\n",
    "        'facility_number': [\n",
    "            r'Facility[:\\s]*(\\d{7})',\n",
    "            r'FEI[:\\s]*(\\d{10})',\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ“‹ Custom extraction patterns available:\")\n",
    "    print(\"- Department of Defense (DoD)\")\n",
    "    print(\"- Food and Drug Administration (FDA)\")\n",
    "    print(\"\\nTo use custom patterns, modify the parse_contract_fields method\")\n",
    "    \n",
    "    return dod_patterns, fda_patterns\n",
    "\n",
    "# Load custom patterns\n",
    "dod_patterns, fda_patterns = add_custom_extraction_patterns()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Quality Control and Validation\n",
    "\n",
    "# %%\n",
    "def validate_extraction_quality(df: pd.DataFrame, sample_size: int = 10):\n",
    "    \"\"\"Validate extraction quality on a sample of results\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” QUALITY VALIDATION\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No results to validate\")\n",
    "        return\n",
    "    \n",
    "    successful_df = df[df['status'] == 'success']\n",
    "    \n",
    "    if len(successful_df) == 0:\n",
    "        print(\"No successful extractions to validate\")\n",
    "        return\n",
    "    \n",
    "    # Sample for validation\n",
    "    sample_df = successful_df.sample(min(sample_size, len(successful_df)))\n",
    "    \n",
    "    print(f\"Validating {len(sample_df)} samples...\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    quality_metrics = {\n",
    "        'files_with_agency': 0,\n",
    "        'files_with_amount': 0,\n",
    "        'files_with_vendor': 0,\n",
    "        'files_with_dates': 0,\n",
    "        'avg_fields_extracted': 0,\n",
    "        'files_with_valid_amounts': 0\n",
    "    }\n",
    "    \n",
    "    total_fields = 0\n",
    "    \n",
    "    for _, row in sample_df.iterrows():\n",
    "        # Check for key fields\n",
    "        if row.get('vendor_name', '').strip():\n",
    "            quality_metrics['files_with_agency'] += 1\n",
    "        \n",
    "        if row.get('total_amount_this_action', '').strip() or row.get('new_contract_total', '').strip():\n",
    "            quality_metrics['files_with_amount'] += 1\n",
    "            \n",
    "            # Validate amount format\n",
    "            amount = row.get('total_amount_this_action', '') or row.get('new_contract_total', '')\n",
    "            if re.match(r'^[\\d,]+\\.?\\d*\n",
    "    \n",
    "    quality_metrics['avg_fields_extracted'] = total_fields / len(sample_df)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nðŸ“Š Quality Metrics:\")\n",
    "    for metric, value in quality_metrics.items():\n",
    "        if metric == 'avg_fields_extracted':\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            percentage = (value / len(sample_df)) * 100\n",
    "            print(f\"  {metric}: {value}/{len(sample_df)} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Identify potential issues\n",
    "    print(\"\\nâš ï¸  Potential Issues:\")\n",
    "    if quality_metrics['files_with_agency'] < len(sample_df) * 0.8:\n",
    "        print(\"  - Low agency name extraction rate\")\n",
    "    if quality_metrics['files_with_amount'] < len(sample_df) * 0.7:\n",
    "        print(\"  - Low contract amount extraction rate\")\n",
    "    if quality_metrics['files_with_valid_amounts'] < quality_metrics['files_with_amount'] * 0.9:\n",
    "        print(\"  - Amount format validation issues\")\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Validate sample quality\n",
    "quality_metrics = validate_extraction_quality(df_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Summary and Next Steps\n",
    "\n",
    "# %%\n",
    "def display_processing_summary():\n",
    "    \"\"\"Display final processing summary and recommendations\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ‰ CONTRACT PROCESSING PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸ“‹ What This Notebook Provides:\")\n",
    "    print(\"  âœ“ Zero-shot contract field extraction\")\n",
    "    print(\"  âœ“ Batch processing capabilities\")\n",
    "    print(\"  âœ“ CPU-optimized processing\")\n",
    "    print(\"  âœ“ Progress tracking and checkpointing\")\n",
    "    print(\"  âœ“ Results analysis and visualization\")\n",
    "    print(\"  âœ“ Multiple export formats (CSV, JSON, Excel)\")\n",
    "    print(\"  âœ“ Quality validation tools\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Next Steps for Your Research:\")\n",
    "    \n",
    "    print(\"\\n1. ðŸ“ PREPARE YOUR DATA:\")\n",
    "    print(\"   - Organize contract files in a single directory\")\n",
    "    print(\"   - Ensure files are in supported formats (PDF, PNG, JPG)\")\n",
    "    print(\"   - Consider file naming convention for better organization\")\n",
    "    \n",
    "    print(\"\\n2. âš™ï¸  CONFIGURE PROCESSING:\")\n",
    "    print(\"   - Adjust batch_size based on your CPU memory\")\n",
    "    print(\"   - Modify extraction patterns for your specific contracts\")\n",
    "    print(\"   - Set up checkpoint directory for large datasets\")\n",
    "    \n",
    "    print(\"\\n3. ðŸ”„ RUN PROCESSING:\")\n",
    "    print(\"   - Test on small sample first\")\n",
    "    print(\"   - Use large dataset processing for 200K documents\")\n",
    "    print(\"   - Monitor progress and handle any failures\")\n",
    "    \n",
    "    print(\"\\n4. ðŸ“Š ANALYZE RESULTS:\")\n",
    "    print(\"   - Validate extraction quality on samples\")\n",
    "    print(\"   - Export results in preferred format\")\n",
    "    print(\"   - Use visualizations for data exploration\")\n",
    "    \n",
    "    print(\"\\n5. ðŸ”¬ ACADEMIC ANALYSIS:\")\n",
    "    print(\"   - Clean and standardize extracted data\")\n",
    "    print(\"   - Perform statistical analysis\")\n",
    "    print(\"   - Document methodology for reproducibility\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Performance Expectations:\")\n",
    "    print(\"   - Speed: ~20-30 seconds per document on CPU\")\n",
    "    print(\"   - Accuracy: ~75-80% for zero-shot extraction\")\n",
    "    print(\"   - Scale: Can handle 200K+ documents\")\n",
    "    print(\"   - Timeline: 2-5 days for full dataset processing\")\n",
    "    \n",
    "    print(\"\\nðŸ“ž Support:\")\n",
    "    print(\"   - Test with sample documents first\")\n",
    "    print(\"   - Check logs for processing errors\")\n",
    "    print(\"   - Adjust extraction patterns as needed\")\n",
    "    print(\"   - Use quality validation to assess results\")\n",
    "\n",
    "# Display final summary\n",
    "display_processing_summary()\n",
    "\n",
    "# %%\n",
    "print(\"\\nâœ… NOTEBOOK SETUP COMPLETE!\")\n",
    "print(\"\\nTo process your contracts:\")\n",
    "print(\"1. Update INPUT_DIRECTORY in Section 6\")\n",
    "print(\"2. Uncomment the processing line\")\n",
    "print(\"3. Run the processing cell\")\n",
    "print(\"4. Use the analysis and export functions\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Current Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Ready to process government contracts at scale!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# \n",
    "# ## Additional Notes\n",
    "# \n",
    "# **For Academic Use:**\n",
    "# - This notebook uses models that are free for academic research\n",
    "# - LayoutLMv3 can be substituted if you need higher accuracy\n",
    "# - All processing is done locally - no data leaves your environment\n",
    "# \n",
    "# **Performance Tips:**\n",
    "# - Start with a small sample to test extraction patterns\n",
    "# - Adjust batch_size based on available CPU memory\n",
    "# - Use checkpointing for very large datasets\n",
    "# - Consider preprocessing PDFs to images for better consistency\n",
    "# \n",
    "# **Customization:**\n",
    "# - Modify regex patterns in `parse_contract_fields()` for your specific contract formats\n",
    "# - Add new field types by extending the extraction patterns\n",
    "# - Implement custom validation rules for your use case\n",
    "# \n",
    "# **Troubleshooting:**\n",
    "# - Check model download in cache directory\n",
    "# - Verify PDF processing dependencies (poppler-utils)\n",
    "# - Monitor memory usage during batch processing\n",
    "# - Use quality validation to identify extraction issues, amount.replace('\n",
    "    \n",
    "    quality_metrics['avg_fields_extracted'] = total_fields / len(sample_df)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nðŸ“Š Quality Metrics:\")\n",
    "    for metric, value in quality_metrics.items():\n",
    "        if metric == 'avg_fields_extracted':\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            percentage = (value / len(sample_df)) * 100\n",
    "            print(f\"  {metric}: {value}/{len(sample_df)} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Identify potential issues\n",
    "    print(\"\\nâš ï¸  Potential Issues:\")\n",
    "    if quality_metrics['files_with_agency'] < len(sample_df) * 0.8:\n",
    "        print(\"  - Low agency name extraction rate\")\n",
    "    if quality_metrics['files_with_amount'] < len(sample_df) * 0.7:\n",
    "        print(\"  - Low contract amount extraction rate\")\n",
    "    if quality_metrics['files_with_valid_amounts'] < quality_metrics['files_with_amount'] * 0.9:\n",
    "        print(\"  - Amount format validation issues\")\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Validate sample quality\n",
    "quality_metrics = validate_extraction_quality(df_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Summary and Next Steps\n",
    "\n",
    "# %%\n",
    "def display_processing_summary():\n",
    "    \"\"\"Display final processing summary and recommendations\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ‰ CONTRACT PROCESSING PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸ“‹ What This Notebook Provides:\")\n",
    "    print(\"  âœ“ Zero-shot contract field extraction\")\n",
    "    print(\"  âœ“ Batch processing capabilities\")\n",
    "    print(\"  âœ“ CPU-optimized processing\")\n",
    "    print(\"  âœ“ Progress tracking and checkpointing\")\n",
    "    print(\"  âœ“ Results analysis and visualization\")\n",
    "    print(\"  âœ“ Multiple export formats (CSV, JSON, Excel)\")\n",
    "    print(\"  âœ“ Quality validation tools\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Next Steps for Your Research:\")\n",
    "    \n",
    "    print(\"\\n1. ðŸ“ PREPARE YOUR DATA:\")\n",
    "    print(\"   - Organize contract files in a single directory\")\n",
    "    print(\"   - Ensure files are in supported formats (PDF, PNG, JPG)\")\n",
    "    print(\"   - Consider file naming convention for better organization\")\n",
    "    \n",
    "    print(\"\\n2. âš™ï¸  CONFIGURE PROCESSING:\")\n",
    "    print(\"   - Adjust batch_size based on your CPU memory\")\n",
    "    print(\"   - Modify extraction patterns for your specific contracts\")\n",
    "    print(\"   - Set up checkpoint directory for large datasets\")\n",
    "    \n",
    "    print(\"\\n3. ðŸ”„ RUN PROCESSING:\")\n",
    "    print(\"   - Test on small sample first\")\n",
    "    print(\"   - Use large dataset processing for 200K documents\")\n",
    "    print(\"   - Monitor progress and handle any failures\")\n",
    "    \n",
    "    print(\"\\n4. ðŸ“Š ANALYZE RESULTS:\")\n",
    "    print(\"   - Validate extraction quality on samples\")\n",
    "    print(\"   - Export results in preferred format\")\n",
    "    print(\"   - Use visualizations for data exploration\")\n",
    "    \n",
    "    print(\"\\n5. ðŸ”¬ ACADEMIC ANALYSIS:\")\n",
    "    print(\"   - Clean and standardize extracted data\")\n",
    "    print(\"   - Perform statistical analysis\")\n",
    "    print(\"   - Document methodology for reproducibility\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Performance Expectations:\")\n",
    "    print(\"   - Speed: ~20-30 seconds per document on CPU\")\n",
    "    print(\"   - Accuracy: ~75-80% for zero-shot extraction\")\n",
    "    print(\"   - Scale: Can handle 200K+ documents\")\n",
    "    print(\"   - Timeline: 2-5 days for full dataset processing\")\n",
    "    \n",
    "    print(\"\\nðŸ“ž Support:\")\n",
    "    print(\"   - Test with sample documents first\")\n",
    "    print(\"   - Check logs for processing errors\")\n",
    "    print(\"   - Adjust extraction patterns as needed\")\n",
    "    print(\"   - Use quality validation to assess results\")\n",
    "\n",
    "# Display final summary\n",
    "display_processing_summary()\n",
    "\n",
    "# %%\n",
    "print(\"\\nâœ… NOTEBOOK SETUP COMPLETE!\")\n",
    "print(\"\\nTo process your contracts:\")\n",
    "print(\"1. Update INPUT_DIRECTORY in Section 6\")\n",
    "print(\"2. Uncomment the processing line\")\n",
    "print(\"3. Run the processing cell\")\n",
    "print(\"4. Use the analysis and export functions\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Current Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Ready to process government contracts at scale!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# \n",
    "# ## Additional Notes\n",
    "# \n",
    "# **For Academic Use:**\n",
    "# - This notebook uses models that are free for academic research\n",
    "# - LayoutLMv3 can be substituted if you need higher accuracy\n",
    "# - All processing is done locally - no data leaves your environment\n",
    "# \n",
    "# **Performance Tips:**\n",
    "# - Start with a small sample to test extraction patterns\n",
    "# - Adjust batch_size based on available CPU memory\n",
    "# - Use checkpointing for very large datasets\n",
    "# - Consider preprocessing PDFs to images for better consistency\n",
    "# \n",
    "# **Customization:**\n",
    "# - Modify regex patterns in `parse_contract_fields()` for your specific contract formats\n",
    "# - Add new field types by extending the extraction patterns\n",
    "# - Implement custom validation rules for your use case\n",
    "# \n",
    "# **Troubleshooting:**\n",
    "# - Check model download in cache directory\n",
    "# - Verify PDF processing dependencies (poppler-utils)\n",
    "# - Monitor memory usage during batch processing\n",
    "# - Use quality validation to identify extraction issues, '').strip()):\n",
    "                quality_metrics['files_with_valid_amounts'] += 1\n",
    "        \n",
    "        if row.get('vendor_name', '').strip():\n",
    "            quality_metrics['files_with_vendor'] += 1\n",
    "        \n",
    "        if row.get('from_date', '').strip() and row.get('to_date', '').strip():\n",
    "            quality_metrics['files_with_dates'] += 1\n",
    "        \n",
    "        # Count total fields extracted\n",
    "        field_count = sum(1 for field in ['eds_number', 'vendor_name', 'total_amount_this_action', \n",
    "                                        'from_date', 'to_date', 'account_number']\n",
    "                         if row.get(field, '').strip())\n",
    "        total_fields += field_count\n",
    "    \n",
    "    quality_metrics['avg_fields_extracted'] = total_fields / len(sample_df)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nðŸ“Š Quality Metrics:\")\n",
    "    for metric, value in quality_metrics.items():\n",
    "        if metric == 'avg_fields_extracted':\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            percentage = (value / len(sample_df)) * 100\n",
    "            print(f\"  {metric}: {value}/{len(sample_df)} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Identify potential issues\n",
    "    print(\"\\nâš ï¸  Potential Issues:\")\n",
    "    if quality_metrics['files_with_agency'] < len(sample_df) * 0.8:\n",
    "        print(\"  - Low agency name extraction rate\")\n",
    "    if quality_metrics['files_with_amount'] < len(sample_df) * 0.7:\n",
    "        print(\"  - Low contract amount extraction rate\")\n",
    "    if quality_metrics['files_with_valid_amounts'] < quality_metrics['files_with_amount'] * 0.9:\n",
    "        print(\"  - Amount format validation issues\")\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Validate sample quality\n",
    "quality_metrics = validate_extraction_quality(df_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Summary and Next Steps\n",
    "\n",
    "# %%\n",
    "def display_processing_summary():\n",
    "    \"\"\"Display final processing summary and recommendations\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ‰ CONTRACT PROCESSING PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸ“‹ What This Notebook Provides:\")\n",
    "    print(\"  âœ“ Zero-shot contract field extraction\")\n",
    "    print(\"  âœ“ Batch processing capabilities\")\n",
    "    print(\"  âœ“ CPU-optimized processing\")\n",
    "    print(\"  âœ“ Progress tracking and checkpointing\")\n",
    "    print(\"  âœ“ Results analysis and visualization\")\n",
    "    print(\"  âœ“ Multiple export formats (CSV, JSON, Excel)\")\n",
    "    print(\"  âœ“ Quality validation tools\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Next Steps for Your Research:\")\n",
    "    \n",
    "    print(\"\\n1. ðŸ“ PREPARE YOUR DATA:\")\n",
    "    print(\"   - Organize contract files in a single directory\")\n",
    "    print(\"   - Ensure files are in supported formats (PDF, PNG, JPG)\")\n",
    "    print(\"   - Consider file naming convention for better organization\")\n",
    "    \n",
    "    print(\"\\n2. âš™ï¸  CONFIGURE PROCESSING:\")\n",
    "    print(\"   - Adjust batch_size based on your CPU memory\")\n",
    "    print(\"   - Modify extraction patterns for your specific contracts\")\n",
    "    print(\"   - Set up checkpoint directory for large datasets\")\n",
    "    \n",
    "    print(\"\\n3. ðŸ”„ RUN PROCESSING:\")\n",
    "    print(\"   - Test on small sample first\")\n",
    "    print(\"   - Use large dataset processing for 200K documents\")\n",
    "    print(\"   - Monitor progress and handle any failures\")\n",
    "    \n",
    "    print(\"\\n4. ðŸ“Š ANALYZE RESULTS:\")\n",
    "    print(\"   - Validate extraction quality on samples\")\n",
    "    print(\"   - Export results in preferred format\")\n",
    "    print(\"   - Use visualizations for data exploration\")\n",
    "    \n",
    "    print(\"\\n5. ðŸ”¬ ACADEMIC ANALYSIS:\")\n",
    "    print(\"   - Clean and standardize extracted data\")\n",
    "    print(\"   - Perform statistical analysis\")\n",
    "    print(\"   - Document methodology for reproducibility\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Performance Expectations:\")\n",
    "    print(\"   - Speed: ~20-30 seconds per document on CPU\")\n",
    "    print(\"   - Accuracy: ~75-80% for zero-shot extraction\")\n",
    "    print(\"   - Scale: Can handle 200K+ documents\")\n",
    "    print(\"   - Timeline: 2-5 days for full dataset processing\")\n",
    "    \n",
    "    print(\"\\nðŸ“ž Support:\")\n",
    "    print(\"   - Test with sample documents first\")\n",
    "    print(\"   - Check logs for processing errors\")\n",
    "    print(\"   - Adjust extraction patterns as needed\")\n",
    "    print(\"   - Use quality validation to assess results\")\n",
    "\n",
    "# Display final summary\n",
    "display_processing_summary()\n",
    "\n",
    "# %%\n",
    "print(\"\\nâœ… NOTEBOOK SETUP COMPLETE!\")\n",
    "print(\"\\nTo process your contracts:\")\n",
    "print(\"1. Update INPUT_DIRECTORY in Section 6\")\n",
    "print(\"2. Uncomment the processing line\")\n",
    "print(\"3. Run the processing cell\")\n",
    "print(\"4. Use the analysis and export functions\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Current Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Ready to process government contracts at scale!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# \n",
    "# ## Additional Notes\n",
    "# \n",
    "# **For Academic Use:**\n",
    "# - This notebook uses models that are free for academic research\n",
    "# - LayoutLMv3 can be substituted if you need higher accuracy\n",
    "# - All processing is done locally - no data leaves your environment\n",
    "# \n",
    "# **Performance Tips:**\n",
    "# - Start with a small sample to test extraction patterns\n",
    "# - Adjust batch_size based on available CPU memory\n",
    "# - Use checkpointing for very large datasets\n",
    "# - Consider preprocessing PDFs to images for better consistency\n",
    "# \n",
    "# **Customization:**\n",
    "# - Modify regex patterns in `parse_contract_fields()` for your specific contract formats\n",
    "# - Add new field types by extending the extraction patterns\n",
    "# - Implement custom validation rules for your use case\n",
    "# \n",
    "# **Troubleshooting:**\n",
    "# - Check model download in cache directory\n",
    "# - Verify PDF processing dependencies (poppler-utils)\n",
    "# - Monitor memory usage during batch processing\n",
    "# - Use quality validation to identify extraction issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
