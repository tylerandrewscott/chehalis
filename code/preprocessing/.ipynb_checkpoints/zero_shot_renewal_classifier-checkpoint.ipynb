{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Renewal Page Classifier\n",
    "\n",
    "This notebook implements a zero-shot classifier that identifies renewal pages by comparing page embeddings to renewal examples.\n",
    "\n",
    "## Key Features:\n",
    "- **No training required** - Works immediately with renewal examples\n",
    "- **Fast inference** - Process large document corpus efficiently  \n",
    "- **Parallel processing** - Utilizes multiple CPU cores\n",
    "- **Easy to update** - Add new renewal examples without retraining\n",
    "- **Interpretable** - Simple similarity threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set tokenizers parallelism to false to avoid fork warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF - replaces pdf2image/poppler\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# For different model options\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoImageProcessor, \n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "### base path for jupyter lab remote\n",
    "BASE_PATH = Path('../..')\n",
    "RENEWAL_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_renewalexamples'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "PROF_SERVICES_JSON_PATH = BASE_PATH / 'data' / 'raw' / 'indiana_prof_services_contracts.json'\n",
    "\n",
    "# Model selection\n",
    "MODEL_TYPE = \"clip\"  # Options: \"donut\", \"clip\", \"dinov2\"\n",
    "\n",
    "# Processing parameters\n",
    "IMAGE_DPI = 150  # Resolution for PDF conversion\n",
    "BATCH_SIZE = 8   # Batch size for embedding extraction\n",
    "SIMILARITY_THRESHOLD = 0.85  # Initial threshold (will be tuned)\n",
    "\n",
    "# File filtering parameters\n",
    "PROF_SERVICES_ONLY = True  # True = only process files listed in prof_services JSON, False = process all contracts\n",
    "\n",
    "# Parallel processing parameters\n",
    "USE_MULTIPROCESSING = True  # Enable multiprocessing\n",
    "N_WORKERS = min(200, os.cpu_count() - 1)  # Leave one core free\n",
    "\n",
    "# Output paths\n",
    "EMBEDDINGS_CACHE_PATH = BASE_PATH / 'code' / 'preprocessing' / 'cached_embeddings'\n",
    "EMBEDDINGS_CACHE_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type: str):\n",
    "    \"\"\"Load the specified model for embedding extraction\"\"\"\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        print(\"Loading Donut model\")\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        return model.encoder.to(device), processor\n",
    "    \n",
    "    elif model_type == \"clip\":\n",
    "        print(\"Loading CLIP model\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        return model.vision_model.to(device), processor\n",
    "    \n",
    "    elif model_type == \"dinov2\":\n",
    "        print(\"Loading DINOv2 model\")\n",
    "        processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        return model.to(device), processor\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Load the selected model\n",
    "model, processor = load_model(MODEL_TYPE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: Path, dpi: int = IMAGE_DPI) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF to list of PIL Images using PyMuPDF (no poppler needed)\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "        \n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc[page_num]\n",
    "            # Convert to pixmap with specified DPI\n",
    "            mat = fitz.Matrix(dpi/72, dpi/72)  # 72 is default DPI\n",
    "            pix = page.get_pixmap(matrix=mat)\n",
    "            # Convert to PIL Image\n",
    "            img_data = pix.tobytes(\"ppm\")\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            images.append(img)\n",
    "        \n",
    "        doc.close()\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image(image: Image.Image, model_type: str) -> Image.Image:\n",
    "    \"\"\"Preprocess image based on model requirements\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        # Donut expects larger images\n",
    "        image.thumbnail((1280, 960), Image.Resampling.LANCZOS)\n",
    "    else:\n",
    "        # CLIP and DINOv2 work with smaller images\n",
    "        image.thumbnail((224, 224), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_prof_services_contracts(json_path: Path) -> List[str]:\n",
    "    \"\"\"Load the list of PDF filenames from the prof_services JSON file\"\"\"\n",
    "    try:\n",
    "        if not json_path.exists():\n",
    "            print(f\"Warning: Prof services JSON file not found at {json_path}\")\n",
    "            return []\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract PDF filenames from pdfUrl fields\n",
    "        pdf_filenames = []\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            # If it's a list of objects\n",
    "            for item in data:\n",
    "                if isinstance(item, dict) and 'pdfUrl' in item:\n",
    "                    pdf_url = item['pdfUrl']\n",
    "                    # Extract filename from URL (last part after /)\n",
    "                    if pdf_url:\n",
    "                        filename = pdf_url.split('/')[-1]\n",
    "                        if filename.endswith('.pdf'):\n",
    "                            pdf_filenames.append(filename)\n",
    "        elif isinstance(data, dict):\n",
    "            # If it's a single dict with pdfUrl\n",
    "            if 'pdfUrl' in data:\n",
    "                pdf_url = data['pdfUrl']\n",
    "                if pdf_url:\n",
    "                    filename = pdf_url.split('/')[-1]\n",
    "                    if filename.endswith('.pdf'):\n",
    "                        pdf_filenames.append(filename)\n",
    "        \n",
    "        print(f\"Loaded {len(pdf_filenames)} PDF filenames from prof_services JSON\")\n",
    "        return pdf_filenames\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prof_services JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_pdf_files(folder_path: Path, prof_services_only: bool = False, \n",
    "                    prof_services_filenames: List[str] = None) -> List[Path]:\n",
    "    \"\"\"Filter PDF files based on prof_services_only setting\"\"\"\n",
    "    all_pdf_files = list(folder_path.glob('*.pdf'))\n",
    "    \n",
    "    if not prof_services_only or not prof_services_filenames:\n",
    "        return all_pdf_files\n",
    "    \n",
    "    # Filter to only include files listed in the prof_services JSON\n",
    "    prof_services_set = set(prof_services_filenames)\n",
    "    filtered_files = [f for f in all_pdf_files if f.name in prof_services_set]\n",
    "    \n",
    "    print(f\"Filtered {len(all_pdf_files)} total PDFs to {len(filtered_files)} prof_services PDFs\")\n",
    "    if len(filtered_files) < len(prof_services_filenames):\n",
    "        missing_count = len(prof_services_filenames) - len(filtered_files)\n",
    "        print(f\"Warning: {missing_count} prof_services PDFs not found in {folder_path}\")\n",
    "    \n",
    "    return filtered_files\n",
    "\n",
    "print(\"âœ“ PDF processing functions defined (using PyMuPDF)\")\n",
    "print(\"âœ“ Prof services filtering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings(images: List[Image.Image], model, processor, model_type: str, \n",
    "                      batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings for a list of images\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        \n",
    "        # Preprocess images\n",
    "        batch_images = [preprocess_image(img, model_type) for img in batch_images]\n",
    "        \n",
    "        # Process batch\n",
    "        if model_type in [\"donut\", \"clip\"]:\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        else:  # dinov2\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract embeddings based on model type\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            batch_embeddings = outputs.pooler_output\n",
    "        elif hasattr(outputs, 'last_hidden_state'):\n",
    "            # Mean pool the sequence dimension\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            batch_embeddings = outputs[0].mean(dim=1)  # Fallback\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embedding_single(image: Image.Image, model, processor, model_type: str) -> np.ndarray:\n",
    "    \"\"\"Extract embedding for a single image\"\"\"\n",
    "    image = preprocess_image(image, model_type)\n",
    "    \n",
    "    if model_type in [\"donut\", \"clip\"]:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    else:  # dinov2\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embedding\n",
    "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "        embedding = outputs.pooler_output\n",
    "    elif hasattr(outputs, 'last_hidden_state'):\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    else:\n",
    "        embedding = outputs[0].mean(dim=1)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Reference Embeddings from Renewal Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cached embeddings exist\n",
    "cache_file = EMBEDDINGS_CACHE_PATH / f\"{MODEL_TYPE}_renewal_reference_embeddings.pkl\"\n",
    "\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading cached renewal reference embeddings from {cache_file}\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "        reference_embeddings = cache_data['embeddings']\n",
    "        reference_metadata = cache_data['metadata']\n",
    "else:\n",
    "    print(\"Building reference embeddings from renewal examples...\")\n",
    "    reference_embeddings = []\n",
    "    reference_metadata = []\n",
    "    \n",
    "    pdf_files = list(RENEWAL_EXAMPLES_PATH.glob('*.pdf'))\n",
    "    print(f\"Processing {len(pdf_files)} renewal example PDFs\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing renewal examples\"):\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        \n",
    "        if images:\n",
    "            # Extract embeddings for all pages\n",
    "            embeddings = extract_embeddings(images, model, processor, MODEL_TYPE)\n",
    "            \n",
    "            # Store embeddings and metadata\n",
    "            for i, embedding in enumerate(embeddings):\n",
    "                reference_embeddings.append(embedding)\n",
    "                reference_metadata.append({\n",
    "                    'file_path': str(pdf_path),\n",
    "                    'filename': pdf_path.name,\n",
    "                    'page_num': i + 1,\n",
    "                    'total_pages': len(images)\n",
    "                })\n",
    "    \n",
    "    reference_embeddings = np.array(reference_embeddings)\n",
    "    \n",
    "    # Cache the embeddings\n",
    "    print(f\"Caching renewal reference embeddings to {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': reference_embeddings,\n",
    "            'metadata': reference_metadata,\n",
    "            'model_type': MODEL_TYPE\n",
    "        }, f)\n",
    "\n",
    "# Create reference prototype (average of all reference embeddings)\n",
    "reference_prototype = np.mean(reference_embeddings, axis=0)\n",
    "\n",
    "print(f\"\\nRenewal reference set contains {len(reference_embeddings)} renewal pages\")\n",
    "print(f\"Embedding dimension: {reference_embeddings.shape[1]}\")\n",
    "print(f\"Reference prototype shape: {reference_prototype.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Similarity Functions and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_to_prototype(embedding: np.ndarray, prototype: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity between an embedding and the prototype\"\"\"\n",
    "    return cosine_similarity([embedding], [prototype])[0][0]\n",
    "\n",
    "def compute_similarity_to_references(embedding: np.ndarray, \n",
    "                                   reference_embeddings: np.ndarray, \n",
    "                                   method: str = 'max') -> float:\n",
    "    \"\"\"Compute similarity to reference embeddings using specified method\"\"\"\n",
    "    similarities = cosine_similarity([embedding], reference_embeddings)[0]\n",
    "    if method == 'max':\n",
    "        return np.max(similarities)\n",
    "    elif method == 'mean':\n",
    "        return np.mean(similarities)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "def classify_pdf_renewal(pdf_path: Path, \n",
    "                        model, \n",
    "                        processor, \n",
    "                        model_type: str,\n",
    "                        reference_embeddings: np.ndarray,\n",
    "                        reference_prototype: np.ndarray,\n",
    "                        threshold: float = SIMILARITY_THRESHOLD,\n",
    "                        use_prototype: bool = True) -> Dict:\n",
    "    \"\"\"Classify a single PDF for renewal pages using zero-shot similarity matching\"\"\"\n",
    "    \n",
    "    result = {\n",
    "        'filename': pdf_path.name,\n",
    "        'contains_renewal': False,\n",
    "        'renewal_pages': [],\n",
    "        'page_scores': [],\n",
    "        'max_similarity': 0.0,\n",
    "        'total_pages': 0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        if not images:\n",
    "            result['error'] = \"Failed to convert PDF to images\"\n",
    "            return result\n",
    "        \n",
    "        result['total_pages'] = len(images)\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            try:\n",
    "                # Extract embedding for this page\n",
    "                embedding = extract_embedding_single(image, model, processor, model_type)\n",
    "                \n",
    "                # Compute similarity\n",
    "                if use_prototype:\n",
    "                    similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "                else:\n",
    "                    similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "                \n",
    "                # Store page score\n",
    "                result['page_scores'].append({\n",
    "                    'page': page_num,\n",
    "                    'similarity': float(similarity)\n",
    "                })\n",
    "                \n",
    "                # Check if this page contains a renewal\n",
    "                if similarity >= threshold:\n",
    "                    result['renewal_pages'].append(page_num)\n",
    "                \n",
    "                # Update max similarity\n",
    "                result['max_similarity'] = max(result['max_similarity'], similarity)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_num} of {pdf_path.name}: {e}\")\n",
    "        \n",
    "        # Determine if document contains renewal pages\n",
    "        result['contains_renewal'] = len(result['renewal_pages']) > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ Similarity and classification functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial threshold\n",
    "OPTIMAL_THRESHOLD = SIMILARITY_THRESHOLD  # Start with the configured threshold\n",
    "\n",
    "print(\"Testing on known examples to tune threshold...\")\n",
    "\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "\n",
    "# Test on subset of renewal examples (already processed)\n",
    "print(\"\\nComputing self-similarity for positive examples (renewal pages)...\")\n",
    "for i in range(min(100, len(reference_embeddings))):\n",
    "    # Leave-one-out similarity\n",
    "    embedding = reference_embeddings[i]\n",
    "    other_embeddings = np.delete(reference_embeddings, i, axis=0)\n",
    "    similarity = compute_similarity_to_references(embedding, other_embeddings, method='max')\n",
    "    positive_scores.append(similarity)\n",
    "\n",
    "# Test on non-examples\n",
    "print(\"\\nTesting on non-examples...\")\n",
    "non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))[:20]  # Test subset\n",
    "\n",
    "for pdf_path in tqdm(non_example_files, desc=\"Processing non-examples\"):\n",
    "    result = classify_pdf_renewal(\n",
    "        pdf_path, model, processor, MODEL_TYPE,\n",
    "        reference_embeddings, reference_prototype,\n",
    "        threshold=0.0,  # Set to 0 to get all scores\n",
    "        use_prototype=False\n",
    "    )\n",
    "    \n",
    "    if result['page_scores']:\n",
    "        for page_score in result['page_scores']:\n",
    "            negative_scores.append(page_score['similarity'])\n",
    "\n",
    "# Analyze scores\n",
    "positive_scores = np.array(positive_scores)\n",
    "negative_scores = np.array(negative_scores)\n",
    "\n",
    "print(f\"\\nPositive scores (renewal pages) - Mean: {positive_scores.mean():.3f}, Std: {positive_scores.std():.3f}\")\n",
    "print(f\"Positive scores - Min: {positive_scores.min():.3f}, Max: {positive_scores.max():.3f}\")\n",
    "print(f\"\\nNegative scores (non-renewal pages) - Mean: {negative_scores.mean():.3f}, Std: {negative_scores.std():.3f}\")\n",
    "print(f\"Negative scores - Min: {negative_scores.min():.3f}, Max: {negative_scores.max():.3f}\")\n",
    "\n",
    "# Plot score distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(positive_scores, bins=30, alpha=0.7, label='Renewal pages', color='green')\n",
    "plt.hist(negative_scores, bins=30, alpha=0.7, label='Non-renewal pages', color='red')\n",
    "plt.axvline(x=SIMILARITY_THRESHOLD, color='black', linestyle='--', \n",
    "            label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
    "plt.xlabel('Cosine Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores - Renewal vs Non-Renewal Pages')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold\n",
    "gap_threshold = (positive_scores.min() + negative_scores.max()) / 2\n",
    "percentile_threshold = np.percentile(negative_scores, 99)  # 99th percentile of negatives\n",
    "\n",
    "print(f\"\\nSuggested thresholds:\")\n",
    "print(f\"Gap-based threshold: {gap_threshold:.3f}\")\n",
    "print(f\"99th percentile threshold: {percentile_threshold:.3f}\")\n",
    "\n",
    "# Update threshold\n",
    "OPTIMAL_THRESHOLD = max(gap_threshold, percentile_threshold)\n",
    "print(f\"\\nSelected optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Parallel Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread-safe function for processing PDFs\n",
    "def process_pdf_threaded(pdf_path: Path, model, processor, model_type: str) -> Dict:\n",
    "    \"\"\"Process a single PDF (thread-safe version)\"\"\"\n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'path': str(pdf_path),\n",
    "        'embeddings': [],\n",
    "        'page_numbers': [],\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to images using PyMuPDF\n",
    "        images = pdf_to_images(pdf_path, dpi=IMAGE_DPI)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        # Extract embeddings for each page\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            try:\n",
    "                embedding = extract_embedding_single(image, model, processor, model_type)\n",
    "                results['embeddings'].append(embedding)\n",
    "                results['page_numbers'].append(page_num)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_num} of {pdf_path.name}: {e}\")\n",
    "        \n",
    "        results['num_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_pdfs_parallel(pdf_files: List[Path], model_type: str, n_workers: int = None) -> List[Dict]:\n",
    "    \"\"\"Process multiple PDFs in parallel using ThreadPoolExecutor\"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = N_WORKERS\n",
    "    \n",
    "    print(f\"\\nProcessing {len(pdf_files)} PDFs using {n_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_pdf = {\n",
    "            executor.submit(process_pdf_threaded, pdf_path, model, processor, model_type): pdf_path \n",
    "            for pdf_path in pdf_files\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        for future in tqdm(as_completed(future_to_pdf), total=len(pdf_files), desc=\"Processing PDFs\"):\n",
    "            pdf_path = future_to_pdf[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdf_path}: {e}\")\n",
    "                results.append({\n",
    "                    'filename': pdf_path.name,\n",
    "                    'path': str(pdf_path),\n",
    "                    'embeddings': [],\n",
    "                    'page_numbers': [],\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Processed {len(pdf_files)} PDFs in {elapsed:.1f} seconds\")\n",
    "    print(f\"Average: {elapsed/len(pdf_files):.2f} seconds per PDF\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def classify_pdf_batch_parallel(pdf_paths: List[Path],\n",
    "                               reference_embeddings: np.ndarray,\n",
    "                               reference_prototype: np.ndarray,\n",
    "                               model_type: str,\n",
    "                               threshold: float = SIMILARITY_THRESHOLD,\n",
    "                               use_prototype: bool = True,\n",
    "                               n_workers: int = None) -> List[Dict]:\n",
    "    \"\"\"Classify a batch of PDFs for renewal pages in parallel\"\"\"\n",
    "    \n",
    "    # First, extract embeddings in parallel\n",
    "    embedding_results = process_pdfs_parallel(pdf_paths, model_type, n_workers)\n",
    "    \n",
    "    # Then classify based on embeddings\n",
    "    classification_results = []\n",
    "    \n",
    "    for result in embedding_results:\n",
    "        if result['error']:\n",
    "            classification_results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_renewal': False,\n",
    "                'renewal_pages': [],\n",
    "                'page_scores': [],\n",
    "                'max_similarity': 0.0,\n",
    "                'error': result['error']\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Classify each page\n",
    "        renewal_pages = []\n",
    "        page_scores = []\n",
    "        max_similarity = 0.0\n",
    "        \n",
    "        for embedding, page_num in zip(result['embeddings'], result['page_numbers']):\n",
    "            # Compute similarity\n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            page_scores.append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                renewal_pages.append(page_num)\n",
    "            \n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        classification_results.append({\n",
    "            'filename': result['filename'],\n",
    "            'contains_renewal': len(renewal_pages) > 0,\n",
    "            'renewal_pages': renewal_pages,\n",
    "            'page_scores': page_scores,\n",
    "            'max_similarity': max_similarity,\n",
    "            'total_pages': result.get('num_pages', len(result['embeddings'])),\n",
    "            'error': None\n",
    "        })\n",
    "    \n",
    "    return classification_results\n",
    "\n",
    "print(\"âœ“ Parallel processing functions defined for renewal detection (PyMuPDF backend)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Document Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_folder_renewal(folder_path: Path,\n",
    "                          model,\n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float,\n",
    "                          output_file: str = 'zero_shot_renewal_results.csv',\n",
    "                          max_files: Optional[int] = None,\n",
    "                          use_prototype: bool = True,\n",
    "                          use_parallel: bool = None,\n",
    "                          clobber: bool = True,\n",
    "                          prof_services_only: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Process all PDFs in a folder using zero-shot renewal classification\n",
    "    \n",
    "    Args:\n",
    "        clobber: If False, skip files already in output_file. If True, reprocess all files.\n",
    "        prof_services_only: If True, only process files listed in prof_services JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_parallel is None:\n",
    "        use_parallel = USE_MULTIPROCESSING\n",
    "    \n",
    "    # Load prof_services filenames if needed\n",
    "    prof_services_filenames = []\n",
    "    if prof_services_only:\n",
    "        prof_services_filenames = load_prof_services_contracts(PROF_SERVICES_JSON_PATH)\n",
    "        if not prof_services_filenames:\n",
    "            print(\"Warning: No prof_services filenames loaded, processing all files instead\")\n",
    "            prof_services_only = False\n",
    "    \n",
    "    # Load existing results if clobber=False\n",
    "    already_processed = set()\n",
    "    existing_results = []\n",
    "    \n",
    "    if not clobber and Path(output_file).exists():\n",
    "        print(f\"Loading existing results from {output_file}...\")\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "        already_processed = set(existing_df['filename'].unique())\n",
    "        existing_results = existing_df.to_dict('records')\n",
    "        print(f\"Found {len(already_processed)} already processed files\")\n",
    "    \n",
    "    results = existing_results.copy()\n",
    "    \n",
    "    # Get PDF files with optional filtering\n",
    "    pdf_files = filter_pdf_files(folder_path, prof_services_only, prof_services_filenames)\n",
    "    \n",
    "    # Filter out already processed files\n",
    "    if not clobber:\n",
    "        pdf_files = [f for f in pdf_files if f.name not in already_processed]\n",
    "        print(f\"Skipping {len(already_processed)} already processed files\")\n",
    "    \n",
    "    if max_files:\n",
    "        pdf_files = pdf_files[:max_files]\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files for renewal detection...\")\n",
    "    if prof_services_only:\n",
    "        print(f\"  (filtered to prof_services contracts only)\")\n",
    "    \n",
    "    if len(pdf_files) == 0:\n",
    "        print(\"No new files to process.\")\n",
    "        return pd.DataFrame(existing_results)\n",
    "    \n",
    "    if use_parallel and len(pdf_files) > 10:\n",
    "        # Use parallel processing\n",
    "        print(f\"Using parallel processing with {N_WORKERS} workers...\")\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = min(50 * N_WORKERS, len(pdf_files))\n",
    "        \n",
    "        for i in range(0, len(pdf_files), batch_size):\n",
    "            batch_files = pdf_files[i:i+batch_size]\n",
    "            print(f\"\\nProcessing batch {i//batch_size + 1}/{(len(pdf_files) + batch_size - 1)//batch_size}\")\n",
    "            \n",
    "            batch_results = classify_pdf_batch_parallel(\n",
    "                batch_files,\n",
    "                reference_embeddings,\n",
    "                reference_prototype,\n",
    "                model_type,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype,\n",
    "                n_workers=N_WORKERS\n",
    "            )\n",
    "            \n",
    "            # Flatten results for CSV\n",
    "            for result in batch_results:\n",
    "                results.append({\n",
    "                    'filename': result['filename'],\n",
    "                    'contains_renewal': result['contains_renewal'],\n",
    "                    'renewal_pages': ','.join(map(str, result['renewal_pages'])),\n",
    "                    'num_renewal_pages': len(result['renewal_pages']),\n",
    "                    'total_pages': result.get('total_pages', 0),\n",
    "                    'max_similarity': result['max_similarity'],\n",
    "                    'error': result['error']\n",
    "                })\n",
    "    else:\n",
    "        # Use sequential processing\n",
    "        print(\"Using sequential processing...\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files):\n",
    "            result = classify_pdf_renewal(\n",
    "                pdf_path, model, processor, model_type,\n",
    "                reference_embeddings, reference_prototype,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype\n",
    "            )\n",
    "            \n",
    "            # Flatten results for CSV\n",
    "            results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_renewal': result['contains_renewal'],\n",
    "                'renewal_pages': ','.join(map(str, result['renewal_pages'])),\n",
    "                'num_renewal_pages': len(result['renewal_pages']),\n",
    "                'total_pages': result.get('total_pages', 0),\n",
    "                'max_similarity': result['max_similarity'],\n",
    "                'error': result['error']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    if not clobber and len(existing_results) > 0:\n",
    "        print(f\"  - Previously processed: {len(existing_results)}\")\n",
    "        print(f\"  - Newly processed: {len(results) - len(existing_results)}\")\n",
    "    print(f\"Documents with renewal pages: {df_results['contains_renewal'].sum()}\")\n",
    "    print(f\"Documents without renewal pages: {(~df_results['contains_renewal']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['error'].notna().sum()}\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "print(\"âœ“ Document processing function defined for renewal detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample first\n",
    "print(\"Testing renewal detection on sample documents...\")\n",
    "\n",
    "test_results = process_document_folder_renewal(\n",
    "    NON_EXAMPLES_PATH,\n",
    "    model,\n",
    "    processor,\n",
    "    MODEL_TYPE,\n",
    "    reference_embeddings,\n",
    "    reference_prototype,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='zero_shot_renewal_test_results.csv',\n",
    "    max_files=50,\n",
    "    use_prototype=False  # Use full reference set for better accuracy\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample renewal detection results:\")\n",
    "print(test_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Process Full Corpus for Renewal Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROF_SERVICES_ONLY = False\n",
    "\n",
    "# Process the full corpus for renewal detection\n",
    "CORPUS_PATH = BASE_PATH / 'data' / 'raw' / '_contracts'  # Adjust path as needed\n",
    "OUTPUT_FILE = 'zero_shot_renewal_results_full_corpus.csv'\n",
    "CLOBBER = False  # Set to False to skip already processed files\n",
    "\n",
    "print(f\"ðŸ“Š Current configuration: {MODEL_TYPE} model, threshold={OPTIMAL_THRESHOLD:.3f}\")\n",
    "print(f\"ðŸ“ Prof services only: {PROF_SERVICES_ONLY}\")\n",
    "print(f\"ðŸ”„ Renewal detection mode: ON\")\n",
    "\n",
    "# Check if output file exists and show status\n",
    "if Path(OUTPUT_FILE).exists() and not CLOBBER:\n",
    "    existing_df = pd.read_csv(OUTPUT_FILE)\n",
    "    print(f\"Found existing results file: {OUTPUT_FILE}\")\n",
    "    print(f\"Already processed: {len(existing_df)} documents\")\n",
    "    print(f\"Documents with renewal pages: {existing_df['contains_renewal'].sum()}\")\n",
    "    \n",
    "    # Show which files will be skipped\n",
    "    if CORPUS_PATH.exists():\n",
    "        if PROF_SERVICES_ONLY:\n",
    "            # Load prof_services files for counting\n",
    "            prof_services_filenames = load_prof_services_contracts(PROF_SERVICES_JSON_PATH)\n",
    "            all_pdfs = filter_pdf_files(CORPUS_PATH, PROF_SERVICES_ONLY, prof_services_filenames)\n",
    "            print(f\"\\nTotal prof_services PDFs: {len(all_pdfs)}\")\n",
    "        else:\n",
    "            all_pdfs = list(CORPUS_PATH.glob('*.pdf'))\n",
    "            print(f\"\\nTotal PDFs in corpus: {len(all_pdfs)}\")\n",
    "        \n",
    "        already_done = set(existing_df['filename'].unique())\n",
    "        remaining = [f for f in all_pdfs if f.name not in already_done]\n",
    "        print(f\"Remaining to process: {len(remaining)}\")\n",
    "        \n",
    "        if len(remaining) == 0:\n",
    "            print(\"\\nAll files have been processed! Set CLOBBER=True to reprocess.\")\n",
    "        else:\n",
    "            print(f\"\\nWill process {len(remaining)} new files...\")\n",
    "\n",
    "# Process the corpus\n",
    "if CORPUS_PATH.exists():\n",
    "    print(f\"\\nProcessing corpus for renewal detection at: {CORPUS_PATH}\")\n",
    "    \n",
    "    full_results = process_document_folder_renewal(\n",
    "        CORPUS_PATH,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        max_files=8000,  # Limit for testing, remove for full corpus\n",
    "        use_prototype=False,  # Use full reference set for best accuracy\n",
    "        use_parallel=True,    # Enable parallel processing\n",
    "        clobber=CLOBBER,      # Use CLOBBER setting\n",
    "        prof_services_only=PROF_SERVICES_ONLY  # Use prof_services filtering\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRenewal detection processing complete!\")\n",
    "    print(f\"Total documents with renewal pages: {full_results['contains_renewal'].sum()}\")\n",
    "    print(f\"Total renewal pages found: {full_results['num_renewal_pages'].sum()}\")\n",
    "    \n",
    "    # Save a backup with timestamp if doing a full reprocess\n",
    "    if CLOBBER:\n",
    "        from datetime import datetime\n",
    "        backup_file = f\"{OUTPUT_FILE.replace('.csv', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        full_results.to_csv(backup_file, index=False)\n",
    "        print(f\"\\nBackup saved to: {backup_file}\")\n",
    "else:\n",
    "    print(f\"âŒ Corpus path not found: {CORPUS_PATH}\")\n",
    "    print(\"Please update the CORPUS_PATH variable to point to your document collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Summary\n",
    "print(\"\\nRenewal Detection Performance Summary:\")\n",
    "print(f\"Model type: {MODEL_TYPE}\")\n",
    "print(f\"Renewal reference examples: {len(reference_embeddings)}\")\n",
    "print(f\"Optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")\n",
    "\n",
    "if 'full_results' in locals():\n",
    "    print(f\"\\nResults Analysis:\")\n",
    "    print(f\"Total documents processed: {len(full_results)}\")\n",
    "    print(f\"Documents with renewal pages: {full_results['contains_renewal'].sum()}\")\n",
    "    print(f\"Renewal detection rate: {full_results['contains_renewal'].mean()*100:.1f}%\")\n",
    "    \n",
    "    # Show distribution of renewal pages per document\n",
    "    renewal_docs = full_results[full_results['contains_renewal'] == True]\n",
    "    if len(renewal_docs) > 0:\n",
    "        print(f\"\\nRenewal pages per document (for documents with renewals):\")\n",
    "        print(f\"Mean: {renewal_docs['num_renewal_pages'].mean():.1f}\")\n",
    "        print(f\"Median: {renewal_docs['num_renewal_pages'].median():.1f}\")\n",
    "        print(f\"Max: {renewal_docs['num_renewal_pages'].max()}\")\n",
    "\n",
    "print(\"\\nâœ… Renewal detection classifier ready for production use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "### Usage Notes:\n",
    "\n",
    "1. **Renewal Examples**: The classifier uses examples from `data/raw/_renewalexamples/` to identify similar renewal pages\n",
    "\n",
    "2. **Output Format**: Results are saved with columns:\n",
    "   - `contains_renewal`: Boolean indicating if document has renewal pages\n",
    "   - `renewal_pages`: Comma-separated list of page numbers with renewals\n",
    "   - `num_renewal_pages`: Count of renewal pages found\n",
    "   - `max_similarity`: Highest similarity score found\n",
    "\n",
    "3. **Performance**: \n",
    "   - Uses same parallel processing as form detection\n",
    "   - Supports incremental processing with CLOBBER=False\n",
    "   - Can filter to prof_services contracts only\n",
    "\n",
    "4. **Accuracy Tuning**:\n",
    "   - Add more renewal examples to `_renewalexamples/` to improve detection\n",
    "   - Adjust threshold based on precision/recall requirements\n",
    "   - Monitor false positives/negatives and refine examples\n",
    "\n",
    "5. **Integration**:\n",
    "   - Results can be combined with form detection results\n",
    "   - Use for downstream analysis of renewal patterns\n",
    "   - Filter contract corpus by renewal presence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}