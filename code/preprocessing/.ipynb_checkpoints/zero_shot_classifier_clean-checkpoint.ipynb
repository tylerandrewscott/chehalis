{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Administrative Form Classifier\n",
    "\n",
    "This notebook implements a zero-shot classifier that identifies administrative forms by comparing page embeddings to reference examples.\n",
    "\n",
    "## Key Features:\n",
    "- **No training required** - Works immediately with example forms\n",
    "- **Fast inference** - Process 190k documents efficiently  \n",
    "- **Parallel processing** - Utilizes multiple CPU cores\n",
    "- **Easy to update** - Add new examples without retraining\n",
    "- **Interpretable** - Simple similarity threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set tokenizers parallelism to false to avoid fork warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# For different model options\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoImageProcessor, \n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Model selection\n",
    "MODEL_TYPE = \"clip\"  # Options: \"donut\", \"clip\", \"dinov2\"\n",
    "\n",
    "# Processing parameters\n",
    "IMAGE_DPI = 150  # Resolution for PDF conversion\n",
    "BATCH_SIZE = 8   # Batch size for embedding extraction\n",
    "SIMILARITY_THRESHOLD = 0.85  # Initial threshold (will be tuned)\n",
    "\n",
    "# Parallel processing parameters\n",
    "USE_MULTIPROCESSING = True  # Enable multiprocessing\n",
    "N_WORKERS = min(8, os.cpu_count() - 1)  # Leave one core free\n",
    "\n",
    "# Output paths\n",
    "EMBEDDINGS_CACHE_PATH = BASE_PATH / 'code' / 'preprocessing' / 'cached_embeddings'\n",
    "EMBEDDINGS_CACHE_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type: str):\n",
    "    \"\"\"Load the specified model for embedding extraction\"\"\"\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        print(\"Loading Donut model\")\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        return model.encoder.to(device), processor\n",
    "    \n",
    "    elif model_type == \"clip\":\n",
    "        print(\"Loading CLIP model\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        return model.vision_model.to(device), processor\n",
    "    \n",
    "    elif model_type == \"dinov2\":\n",
    "        print(\"Loading DINOv2 model\")\n",
    "        processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        return model.to(device), processor\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Load the selected model\n",
    "model, processor = load_model(MODEL_TYPE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: Path, dpi: int = IMAGE_DPI) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF to list of PIL Images\"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image(image: Image.Image, model_type: str) -> Image.Image:\n",
    "    \"\"\"Preprocess image based on model requirements\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        # Donut expects larger images\n",
    "        image.thumbnail((1280, 960), Image.Resampling.LANCZOS)\n",
    "    else:\n",
    "        # CLIP and DINOv2 work with smaller images\n",
    "        image.thumbnail((224, 224), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings(images: List[Image.Image], model, processor, model_type: str, \n",
    "                      batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings for a list of images\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        \n",
    "        # Preprocess images\n",
    "        batch_images = [preprocess_image(img, model_type) for img in batch_images]\n",
    "        \n",
    "        # Process batch\n",
    "        if model_type in [\"donut\", \"clip\"]:\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        else:  # dinov2\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract embeddings based on model type\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            batch_embeddings = outputs.pooler_output\n",
    "        elif hasattr(outputs, 'last_hidden_state'):\n",
    "            # Mean pool the sequence dimension\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            batch_embeddings = outputs[0].mean(dim=1)  # Fallback\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embedding_single(image: Image.Image, model, processor, model_type: str) -> np.ndarray:\n",
    "    \"\"\"Extract embedding for a single image\"\"\"\n",
    "    image = preprocess_image(image, model_type)\n",
    "    \n",
    "    if model_type in [\"donut\", \"clip\"]:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    else:  # dinov2\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embedding\n",
    "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "        embedding = outputs.pooler_output\n",
    "    elif hasattr(outputs, 'last_hidden_state'):\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    else:\n",
    "        embedding = outputs[0].mean(dim=1)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cached embeddings exist\n",
    "cache_file = EMBEDDINGS_CACHE_PATH / f\"{MODEL_TYPE}_reference_embeddings.pkl\"\n",
    "\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading cached reference embeddings from {cache_file}\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "        reference_embeddings = cache_data['embeddings']\n",
    "        reference_metadata = cache_data['metadata']\n",
    "else:\n",
    "    print(\"Building reference embeddings from example forms...\")\n",
    "    reference_embeddings = []\n",
    "    reference_metadata = []\n",
    "    \n",
    "    pdf_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "    print(f\"Processing {len(pdf_files)} example form PDFs\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing example forms\"):\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        \n",
    "        if images:\n",
    "            # Extract embeddings for all pages\n",
    "            embeddings = extract_embeddings(images, model, processor, MODEL_TYPE)\n",
    "            \n",
    "            # Store embeddings and metadata\n",
    "            for i, embedding in enumerate(embeddings):\n",
    "                reference_embeddings.append(embedding)\n",
    "                reference_metadata.append({\n",
    "                    'file_path': str(pdf_path),\n",
    "                    'filename': pdf_path.name,\n",
    "                    'page_num': i + 1,\n",
    "                    'total_pages': len(images)\n",
    "                })\n",
    "    \n",
    "    reference_embeddings = np.array(reference_embeddings)\n",
    "    \n",
    "    # Cache the embeddings\n",
    "    print(f\"Caching reference embeddings to {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': reference_embeddings,\n",
    "            'metadata': reference_metadata,\n",
    "            'model_type': MODEL_TYPE\n",
    "        }, f)\n",
    "\n",
    "print(f\"\\nReference set contains {len(reference_embeddings)} form pages\")\n",
    "print(f\"Embedding dimension: {reference_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Similarity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reference representation\n",
    "print(\"Computing reference representation...\")\n",
    "\n",
    "# Option 1: Mean embedding (prototype)\n",
    "reference_prototype = reference_embeddings.mean(axis=0)\n",
    "print(f\"Reference prototype shape: {reference_prototype.shape}\")\n",
    "\n",
    "def compute_similarity_to_prototype(embedding: np.ndarray, prototype: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity to prototype\"\"\"\n",
    "    return cosine_similarity(embedding.reshape(1, -1), prototype.reshape(1, -1))[0, 0]\n",
    "\n",
    "def compute_similarity_to_references(embedding: np.ndarray, references: np.ndarray, \n",
    "                                   method: str = 'max') -> float:\n",
    "    \"\"\"Compute similarity to reference set\n",
    "    \n",
    "    Args:\n",
    "        embedding: Single embedding to compare\n",
    "        references: Array of reference embeddings\n",
    "        method: 'max', 'mean', or 'top_k'\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity(embedding.reshape(1, -1), references)[0]\n",
    "    \n",
    "    if method == 'max':\n",
    "        return similarities.max()\n",
    "    elif method == 'mean':\n",
    "        return similarities.mean()\n",
    "    elif method == 'top_k':\n",
    "        # Average of top 5 most similar\n",
    "        k = min(5, len(similarities))\n",
    "        return np.sort(similarities)[-k:].mean()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zero-Shot Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pdf_zero_shot(pdf_path: Path, \n",
    "                          model, \n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float = SIMILARITY_THRESHOLD,\n",
    "                          use_prototype: bool = True) -> Dict:\n",
    "    \"\"\"Classify pages in a PDF using zero-shot similarity matching\"\"\"\n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'contains_form': False,\n",
    "        'form_pages': [],\n",
    "        'page_scores': [],\n",
    "        'max_similarity': 0.0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            # Extract embedding\n",
    "            embedding = extract_embedding_single(image, model, processor, model_type)\n",
    "            \n",
    "            # Compute similarity\n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            results['page_scores'].append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            # Check if page is a form\n",
    "            if similarity >= threshold:\n",
    "                results['form_pages'].append(page_num)\n",
    "                results['contains_form'] = True\n",
    "            \n",
    "            results['max_similarity'] = max(results['max_similarity'], similarity)\n",
    "        \n",
    "        results['total_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Parallel Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread-safe function for processing PDFs\n",
    "def process_pdf_threaded(pdf_path: Path, model, processor, model_type: str) -> Dict:\n",
    "    \"\"\"Process a single PDF (thread-safe version)\"\"\"\n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'path': str(pdf_path),\n",
    "        'embeddings': [],\n",
    "        'page_numbers': [],\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        images = pdf_to_images(pdf_path, dpi=IMAGE_DPI)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        # Extract embeddings for each page\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            try:\n",
    "                embedding = extract_embedding_single(image, model, processor, model_type)\n",
    "                results['embeddings'].append(embedding)\n",
    "                results['page_numbers'].append(page_num)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_num} of {pdf_path.name}: {e}\")\n",
    "        \n",
    "        results['num_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_pdfs_parallel(pdf_files: List[Path], model_type: str, n_workers: int = None) -> List[Dict]:\n",
    "    \"\"\"Process multiple PDFs in parallel using ThreadPoolExecutor\"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = N_WORKERS\n",
    "    \n",
    "    print(f\"\\nProcessing {len(pdf_files)} PDFs using {n_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_pdf = {\n",
    "            executor.submit(process_pdf_threaded, pdf_path, model, processor, model_type): pdf_path \n",
    "            for pdf_path in pdf_files\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        for future in tqdm(as_completed(future_to_pdf), total=len(pdf_files), desc=\"Processing PDFs\"):\n",
    "            pdf_path = future_to_pdf[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdf_path}: {e}\")\n",
    "                results.append({\n",
    "                    'filename': pdf_path.name,\n",
    "                    'path': str(pdf_path),\n",
    "                    'embeddings': [],\n",
    "                    'page_numbers': [],\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Processed {len(pdf_files)} PDFs in {elapsed:.1f} seconds\")\n",
    "    print(f\"Average: {elapsed/len(pdf_files):.2f} seconds per PDF\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def classify_pdf_batch_parallel(pdf_paths: List[Path],\n",
    "                               reference_embeddings: np.ndarray,\n",
    "                               reference_prototype: np.ndarray,\n",
    "                               model_type: str,\n",
    "                               threshold: float = SIMILARITY_THRESHOLD,\n",
    "                               use_prototype: bool = True,\n",
    "                               n_workers: int = None) -> List[Dict]:\n",
    "    \"\"\"Classify a batch of PDFs in parallel\"\"\"\n",
    "    \n",
    "    # First, extract embeddings in parallel\n",
    "    embedding_results = process_pdfs_parallel(pdf_paths, model_type, n_workers)\n",
    "    \n",
    "    # Then classify based on embeddings\n",
    "    classification_results = []\n",
    "    \n",
    "    for result in embedding_results:\n",
    "        if result['error']:\n",
    "            classification_results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_form': False,\n",
    "                'form_pages': [],\n",
    "                'page_scores': [],\n",
    "                'max_similarity': 0.0,\n",
    "                'error': result['error']\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Classify each page\n",
    "        form_pages = []\n",
    "        page_scores = []\n",
    "        max_similarity = 0.0\n",
    "        \n",
    "        for embedding, page_num in zip(result['embeddings'], result['page_numbers']):\n",
    "            # Compute similarity\n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            page_scores.append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                form_pages.append(page_num)\n",
    "            \n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        classification_results.append({\n",
    "            'filename': result['filename'],\n",
    "            'contains_form': len(form_pages) > 0,\n",
    "            'form_pages': form_pages,\n",
    "            'page_scores': page_scores,\n",
    "            'max_similarity': max_similarity,\n",
    "            'total_pages': result.get('num_pages', len(result['embeddings'])),\n",
    "            'error': None\n",
    "        })\n",
    "    \n",
    "    return classification_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on known examples and non-examples to find optimal threshold\n",
    "print(\"Testing on known examples to tune threshold...\")\n",
    "\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "\n",
    "# Test on subset of examples (already processed)\n",
    "print(\"\\nComputing self-similarity for positive examples...\")\n",
    "for i in range(min(50, len(reference_embeddings))):\n",
    "    # Leave-one-out similarity\n",
    "    embedding = reference_embeddings[i]\n",
    "    other_embeddings = np.delete(reference_embeddings, i, axis=0)\n",
    "    similarity = compute_similarity_to_references(embedding, other_embeddings, method='max')\n",
    "    positive_scores.append(similarity)\n",
    "\n",
    "# Test on non-examples\n",
    "print(\"\\nTesting on non-examples...\")\n",
    "non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))[:20]  # Test subset\n",
    "\n",
    "for pdf_path in tqdm(non_example_files, desc=\"Processing non-examples\"):\n",
    "    result = classify_pdf_zero_shot(\n",
    "        pdf_path, model, processor, MODEL_TYPE,\n",
    "        reference_embeddings, reference_prototype,\n",
    "        threshold=0.0,  # Set to 0 to get all scores\n",
    "        use_prototype=False\n",
    "    )\n",
    "    \n",
    "    if result['page_scores']:\n",
    "        for page_score in result['page_scores']:\n",
    "            negative_scores.append(page_score['similarity'])\n",
    "\n",
    "# Analyze scores\n",
    "positive_scores = np.array(positive_scores)\n",
    "negative_scores = np.array(negative_scores)\n",
    "\n",
    "print(f\"\\nPositive scores - Mean: {positive_scores.mean():.3f}, Std: {positive_scores.std():.3f}\")\n",
    "print(f\"Positive scores - Min: {positive_scores.min():.3f}, Max: {positive_scores.max():.3f}\")\n",
    "print(f\"\\nNegative scores - Mean: {negative_scores.mean():.3f}, Std: {negative_scores.std():.3f}\")\n",
    "print(f\"Negative scores - Min: {negative_scores.min():.3f}, Max: {negative_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(positive_scores, bins=30, alpha=0.7, label='Form pages', color='green')\n",
    "plt.hist(negative_scores, bins=30, alpha=0.7, label='Non-form pages', color='red')\n",
    "plt.axvline(x=SIMILARITY_THRESHOLD, color='black', linestyle='--', \n",
    "            label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
    "plt.xlabel('Cosine Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold\n",
    "gap_threshold = (positive_scores.min() + negative_scores.max()) / 2\n",
    "percentile_threshold = np.percentile(negative_scores, 99)  # 99th percentile of negatives\n",
    "\n",
    "print(f\"\\nSuggested thresholds:\")\n",
    "print(f\"Gap-based threshold: {gap_threshold:.3f}\")\n",
    "print(f\"99th percentile threshold: {percentile_threshold:.3f}\")\n",
    "\n",
    "# Update threshold\n",
    "OPTIMAL_THRESHOLD = max(gap_threshold, percentile_threshold)\n",
    "print(f\"\\nSelected optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_folder(folder_path: Path,\n",
    "                          model,\n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float,\n",
    "                          output_file: str = 'zero_shot_results.csv',\n",
    "                          max_files: Optional[int] = None,\n",
    "                          use_prototype: bool = True,\n",
    "                          use_parallel: bool = None) -> pd.DataFrame:\n",
    "    \"\"\"Process all PDFs in a folder using zero-shot classification\"\"\"\n",
    "    \n",
    "    if use_parallel is None:\n",
    "        use_parallel = USE_MULTIPROCESSING\n",
    "    \n",
    "    results = []\n",
    "    pdf_files = list(folder_path.glob('*.pdf'))\n",
    "    \n",
    "    if max_files:\n",
    "        pdf_files = pdf_files[:max_files]\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    \n",
    "    if use_parallel and len(pdf_files) > 10:\n",
    "        # Use parallel processing\n",
    "        print(f\"Using parallel processing with {N_WORKERS} workers...\")\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = min(100 * N_WORKERS, len(pdf_files))\n",
    "        \n",
    "        for i in range(0, len(pdf_files), batch_size):\n",
    "            batch_files = pdf_files[i:i+batch_size]\n",
    "            print(f\"\\nProcessing batch {i//batch_size + 1}/{(len(pdf_files) + batch_size - 1)//batch_size}\")\n",
    "            \n",
    "            batch_results = classify_pdf_batch_parallel(\n",
    "                batch_files,\n",
    "                reference_embeddings,\n",
    "                reference_prototype,\n",
    "                model_type,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype,\n",
    "                n_workers=N_WORKERS\n",
    "            )\n",
    "            \n",
    "            # Flatten results for CSV\n",
    "            for result in batch_results:\n",
    "                results.append({\n",
    "                    'filename': result['filename'],\n",
    "                    'contains_form': result['contains_form'],\n",
    "                    'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "                    'num_form_pages': len(result['form_pages']),\n",
    "                    'total_pages': result.get('total_pages', 0),\n",
    "                    'max_similarity': result['max_similarity'],\n",
    "                    'error': result['error']\n",
    "                })\n",
    "    else:\n",
    "        # Use sequential processing\n",
    "        print(\"Using sequential processing...\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files):\n",
    "            result = classify_pdf_zero_shot(\n",
    "                pdf_path, model, processor, model_type,\n",
    "                reference_embeddings, reference_prototype,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype\n",
    "            )\n",
    "            \n",
    "            # Flatten results for CSV\n",
    "            results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_form': result['contains_form'],\n",
    "                'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "                'num_form_pages': len(result['form_pages']),\n",
    "                'total_pages': result.get('total_pages', 0),\n",
    "                'max_similarity': result['max_similarity'],\n",
    "                'error': result['error']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    print(f\"Documents with forms: {df_results['contains_form'].sum()}\")\n",
    "    print(f\"Documents without forms: {(~df_results['contains_form']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['error'].notna().sum()}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample first\n",
    "print(\"Testing on sample documents...\")\n",
    "\n",
    "test_results = process_document_folder(\n",
    "    NON_EXAMPLES_PATH,\n",
    "    model,\n",
    "    processor,\n",
    "    MODEL_TYPE,\n",
    "    reference_embeddings,\n",
    "    reference_prototype,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='zero_shot_test_results.csv',\n",
    "    max_files=50,\n",
    "    use_prototype=False  # Use full reference set for better accuracy\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "print(test_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Comparison: Sequential vs Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance: Sequential vs Parallel processing\n",
    "print(\"Performance Comparison: Sequential vs Parallel Processing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test on a small subset\n",
    "test_path = NON_EXAMPLES_PATH\n",
    "test_files = list(test_path.glob('*.pdf'))[:20]  # Test with 20 files\n",
    "\n",
    "if len(test_files) >= 10:\n",
    "    # Sequential processing\n",
    "    print(\"\\n1. Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    seq_results = process_document_folder(\n",
    "        test_path,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file='sequential_test.csv',\n",
    "        max_files=20,\n",
    "        use_prototype=False,\n",
    "        use_parallel=False\n",
    "    )\n",
    "    seq_time = time.time() - start_time\n",
    "    print(f\"Sequential time: {seq_time:.1f} seconds\")\n",
    "    print(f\"Speed: {len(test_files)/seq_time:.1f} PDFs/second\")\n",
    "    \n",
    "    # Parallel processing\n",
    "    print(\"\\n2. Parallel Processing:\")\n",
    "    start_time = time.time()\n",
    "    par_results = process_document_folder(\n",
    "        test_path,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file='parallel_test.csv',\n",
    "        max_files=20,\n",
    "        use_prototype=False,\n",
    "        use_parallel=True\n",
    "    )\n",
    "    par_time = time.time() - start_time\n",
    "    print(f\"Parallel time: {par_time:.1f} seconds\")\n",
    "    print(f\"Speed: {len(test_files)/par_time:.1f} PDFs/second\")\n",
    "    \n",
    "    # Speedup\n",
    "    speedup = seq_time / par_time\n",
    "    print(f\"\\nSpeedup: {speedup:.1f}x faster with {N_WORKERS} workers\")\n",
    "    \n",
    "    # Efficiency\n",
    "    efficiency = speedup / N_WORKERS * 100\n",
    "    print(f\"Parallel efficiency: {efficiency:.0f}%\")\n",
    "    \n",
    "    # Estimate for full corpus\n",
    "    print(f\"\\nEstimated time for 190k documents:\")\n",
    "    print(f\"Sequential: {190000 * seq_time / len(test_files) / 3600:.1f} hours\")\n",
    "    print(f\"Parallel: {190000 * par_time / len(test_files) / 3600:.1f} hours\")\n",
    "else:\n",
    "    print(\"Not enough test files for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Process Full Corpus\n",
    "\n",
    "Example code for processing the full 190k corpus. Uncomment and modify the path as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for processing the full 190k corpus\n",
    "# Uncomment and modify the path as needed\n",
    "\n",
    "\"\"\"\n",
    "CORPUS_PATH = BASE_PATH / 'data' / 'raw' / '_contracts'  # Adjust path\n",
    "\n",
    "# Process the full corpus with parallel processing\n",
    "full_results = process_document_folder(\n",
    "    CORPUS_PATH,\n",
    "    model,\n",
    "    processor,\n",
    "    MODEL_TYPE,\n",
    "    reference_embeddings,\n",
    "    reference_prototype,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='zero_shot_results_full_corpus.csv',\n",
    "    use_prototype=False,\n",
    "    use_parallel=True\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Total documents with forms: {full_results['contains_form'].sum()}\")\n",
    "print(f\"Total form pages found: {full_results['num_form_pages'].sum()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps\n",
    "\n",
    "### Performance Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"Model type: {MODEL_TYPE}\")\n",
    "print(f\"Reference examples: {len(reference_embeddings)}\")\n",
    "print(f\"Optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")\n",
    "print(f\"\\nProcessing speed: ~{len(test_results) / 60:.1f} documents per minute (sequential)\")\n",
    "if 'speedup' in locals():\n",
    "    print(f\"Parallel speedup: {speedup:.1f}x with {N_WORKERS} workers\")\n",
    "\n",
    "# Estimate time for full corpus\n",
    "docs_per_minute = len(test_results) / 60  # Rough estimate\n",
    "total_minutes = 190000 / docs_per_minute\n",
    "print(f\"\\nEstimated time for 190k documents (sequential): {total_minutes/60:.1f} hours\")\n",
    "if 'speedup' in locals():\n",
    "    print(f\"Estimated time for 190k documents (parallel): {total_minutes/60/speedup:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. **Production Deployment**:\n",
    "   - Use GPU for faster processing (10-50x speedup)\n",
    "   - Increase number of workers for CPU processing\n",
    "   - Process in larger batches\n",
    "\n",
    "2. **Optimization**:\n",
    "   - Try different models (DINOv2 might be faster)\n",
    "   - Reduce image resolution if accuracy permits\n",
    "   - Implement page-level caching\n",
    "\n",
    "3. **Accuracy Improvements**:\n",
    "   - Add more reference examples\n",
    "   - Fine-tune threshold using validation set\n",
    "   - Try ensemble of multiple models\n",
    "\n",
    "4. **When to Consider Training**:\n",
    "   - If zero-shot accuracy < 90%\n",
    "   - If you need to detect subtle variations\n",
    "   - If inference speed is critical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}