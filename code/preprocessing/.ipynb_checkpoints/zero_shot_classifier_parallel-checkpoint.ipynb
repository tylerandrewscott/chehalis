{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Administrative Form Classifier with Parallel Processing\n",
    "\n",
    "This notebook implements a zero-shot classifier that identifies administrative forms using embedding similarity, with multiprocessing support for efficient large-scale processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and imports\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Multiprocessing imports\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "# Model imports\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoImageProcessor, \n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# Check device and cores\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Available CPU cores: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Model selection\n",
    "MODEL_TYPE = \"clip\"  # Options: \"donut\", \"clip\", \"dinov2\"\n",
    "\n",
    "# Processing parameters\n",
    "IMAGE_DPI = 150\n",
    "BATCH_SIZE = 8\n",
    "SIMILARITY_THRESHOLD = 0.85\n",
    "\n",
    "# Parallel processing parameters\n",
    "USE_MULTIPROCESSING = True\n",
    "N_WORKERS = cpu_count() - 1\n",
    "MAX_PDFS_PER_WORKER = 100\n",
    "\n",
    "# Output paths\n",
    "EMBEDDINGS_CACHE_PATH = BASE_PATH / 'code' / 'preprocessing' / 'cached_embeddings'\n",
    "EMBEDDINGS_CACHE_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load model\n",
    "def load_model(model_type: str):\n",
    "    \"\"\"Load the specified model for embedding extraction\"\"\"\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        MODEL_DIR = \"./form_classifier_model\"\n",
    "        if os.path.exists(MODEL_DIR):\n",
    "            print(f\"Loading fine-tuned Donut model from {MODEL_DIR}\")\n",
    "            model = VisionEncoderDecoderModel.from_pretrained(MODEL_DIR)\n",
    "            processor = DonutProcessor.from_pretrained(MODEL_DIR)\n",
    "        else:\n",
    "            print(\"Loading base Donut model\")\n",
    "            model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "            processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        return model.encoder.to(device), processor\n",
    "    \n",
    "    elif model_type == \"clip\":\n",
    "        print(\"Loading CLIP model\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        return model.vision_model.to(device), processor\n",
    "    \n",
    "    elif model_type == \"dinov2\":\n",
    "        print(\"Loading DINOv2 model\")\n",
    "        processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        return model.to(device), processor\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Load the selected model\n",
    "model, processor = load_model(MODEL_TYPE)\n",
    "model.eval()\n",
    "print(f\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Basic helper functions\n",
    "def pdf_to_images(pdf_path: Path, dpi: int = IMAGE_DPI) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF to list of PIL Images\"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image(image: Image.Image, model_type: str) -> Image.Image:\n",
    "    \"\"\"Preprocess image based on model requirements\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        image.thumbnail((1280, 960), Image.Resampling.LANCZOS)\n",
    "    else:\n",
    "        image.thumbnail((224, 224), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Embedding extraction functions\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(images: List[Image.Image], model, processor, model_type: str, \n",
    "                      batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings for a list of images\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        batch_images = [preprocess_image(img, model_type) for img in batch_images]\n",
    "        \n",
    "        if model_type in [\"donut\", \"clip\"]:\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        else:  # dinov2\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            batch_embeddings = outputs.pooler_output\n",
    "        elif hasattr(outputs, 'last_hidden_state'):\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            batch_embeddings = outputs[0].mean(dim=1)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embedding_single(image: Image.Image, model, processor, model_type: str) -> np.ndarray:\n",
    "    \"\"\"Extract embedding for a single image\"\"\"\n",
    "    image = preprocess_image(image, model_type)\n",
    "    \n",
    "    if model_type in [\"donut\", \"clip\"]:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    else:  # dinov2\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "        embedding = outputs.pooler_output\n",
    "    elif hasattr(outputs, 'last_hidden_state'):\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    else:\n",
    "        embedding = outputs[0].mean(dim=1)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parallel processing functions\n",
    "def init_worker(model_type_arg):\n",
    "    \"\"\"Initialize model in each worker process\"\"\"\n",
    "    global worker_model, worker_processor, worker_model_type\n",
    "    worker_model_type = model_type_arg\n",
    "    \n",
    "    if model_type_arg == \"clip\":\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        worker_model = model.vision_model\n",
    "    elif model_type_arg == \"donut\":\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        worker_model = model.encoder\n",
    "    elif model_type_arg == \"dinov2\":\n",
    "        processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        worker_model = model\n",
    "    \n",
    "    worker_model.eval()\n",
    "    worker_processor = processor\n",
    "\n",
    "def process_pdf_parallel(pdf_path: Path) -> Dict:\n",
    "    \"\"\"Process a single PDF in a worker process\"\"\"\n",
    "    global worker_model, worker_processor, worker_model_type\n",
    "    \n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'path': str(pdf_path),\n",
    "        'embeddings': [],\n",
    "        'page_numbers': [],\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        images = pdf_to_images(pdf_path, dpi=IMAGE_DPI)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            try:\n",
    "                image = preprocess_image(image, worker_model_type)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if worker_model_type in [\"donut\", \"clip\"]:\n",
    "                        inputs = worker_processor(images=image, return_tensors=\"pt\")\n",
    "                        outputs = worker_model(**inputs)\n",
    "                    else:  # dinov2\n",
    "                        inputs = worker_processor(images=image, return_tensors=\"pt\")\n",
    "                        outputs = worker_model(**inputs)\n",
    "                    \n",
    "                    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                        embedding = outputs.pooler_output\n",
    "                    elif hasattr(outputs, 'last_hidden_state'):\n",
    "                        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "                    else:\n",
    "                        embedding = outputs[0].mean(dim=1) if isinstance(outputs, tuple) else outputs.mean(dim=1)\n",
    "                    \n",
    "                    embedding = embedding.numpy()[0]\n",
    "                \n",
    "                results['embeddings'].append(embedding)\n",
    "                results['page_numbers'].append(page_num)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_num} of {pdf_path.name}: {e}\")\n",
    "        \n",
    "        results['num_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_pdfs_parallel(pdf_files: List[Path], model_type: str, n_workers: int = None) -> List[Dict]:\n",
    "    \"\"\"Process multiple PDFs in parallel\"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = N_WORKERS\n",
    "    \n",
    "    print(f\"\\nProcessing {len(pdf_files)} PDFs using {n_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with Pool(n_workers, initializer=init_worker, initargs=(model_type,)) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_pdf_parallel, pdf_files),\n",
    "            total=len(pdf_files),\n",
    "            desc=\"Processing PDFs\"\n",
    "        ))\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Processed {len(pdf_files)} PDFs in {elapsed:.1f} seconds\")\n",
    "    print(f\"Average: {elapsed/len(pdf_files):.2f} seconds per PDF\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Build reference embeddings\n",
    "cache_file = EMBEDDINGS_CACHE_PATH / f\"{MODEL_TYPE}_reference_embeddings.pkl\"\n",
    "\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading cached reference embeddings from {cache_file}\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "        reference_embeddings = cache_data['embeddings']\n",
    "        reference_metadata = cache_data['metadata']\n",
    "else:\n",
    "    print(\"Building reference embeddings from example forms...\")\n",
    "    pdf_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "    print(f\"Processing {len(pdf_files)} example form PDFs\")\n",
    "    \n",
    "    if USE_MULTIPROCESSING and len(pdf_files) > 10:\n",
    "        # Use parallel processing\n",
    "        print(\"Using parallel processing for reference embeddings...\")\n",
    "        results = process_pdfs_parallel(pdf_files, MODEL_TYPE, N_WORKERS)\n",
    "        \n",
    "        reference_embeddings = []\n",
    "        reference_metadata = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result['error']:\n",
    "                print(f\"Skipping {result['filename']} due to error: {result['error']}\")\n",
    "                continue\n",
    "            \n",
    "            for embedding, page_num in zip(result['embeddings'], result['page_numbers']):\n",
    "                reference_embeddings.append(embedding)\n",
    "                reference_metadata.append({\n",
    "                    'file_path': result['path'],\n",
    "                    'filename': result['filename'],\n",
    "                    'page_num': page_num,\n",
    "                    'total_pages': result['num_pages']\n",
    "                })\n",
    "    else:\n",
    "        # Use sequential processing\n",
    "        reference_embeddings = []\n",
    "        reference_metadata = []\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing example forms\"):\n",
    "            images = pdf_to_images(pdf_path)\n",
    "            if images:\n",
    "                embeddings = extract_embeddings(images, model, processor, MODEL_TYPE)\n",
    "                for i, embedding in enumerate(embeddings):\n",
    "                    reference_embeddings.append(embedding)\n",
    "                    reference_metadata.append({\n",
    "                        'file_path': str(pdf_path),\n",
    "                        'filename': pdf_path.name,\n",
    "                        'page_num': i + 1,\n",
    "                        'total_pages': len(images)\n",
    "                    })\n",
    "    \n",
    "    reference_embeddings = np.array(reference_embeddings)\n",
    "    \n",
    "    # Cache the embeddings\n",
    "    print(f\"Caching reference embeddings to {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': reference_embeddings,\n",
    "            'metadata': reference_metadata,\n",
    "            'model_type': MODEL_TYPE\n",
    "        }, f)\n",
    "\n",
    "print(f\"\\nReference set contains {len(reference_embeddings)} form pages\")\n",
    "print(f\"Embedding dimension: {reference_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Similarity computation functions\n",
    "reference_prototype = reference_embeddings.mean(axis=0)\n",
    "print(f\"Reference prototype shape: {reference_prototype.shape}\")\n",
    "\n",
    "def compute_similarity_to_prototype(embedding: np.ndarray, prototype: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity to prototype\"\"\"\n",
    "    return cosine_similarity(embedding.reshape(1, -1), prototype.reshape(1, -1))[0, 0]\n",
    "\n",
    "def compute_similarity_to_references(embedding: np.ndarray, references: np.ndarray, \n",
    "                                   method: str = 'max') -> float:\n",
    "    \"\"\"Compute similarity to reference set\"\"\"\n",
    "    similarities = cosine_similarity(embedding.reshape(1, -1), references)[0]\n",
    "    \n",
    "    if method == 'max':\n",
    "        return similarities.max()\n",
    "    elif method == 'mean':\n",
    "        return similarities.mean()\n",
    "    elif method == 'top_k':\n",
    "        k = min(5, len(similarities))\n",
    "        return np.sort(similarities)[-k:].mean()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Classification functions\n",
    "def classify_pdf_zero_shot(pdf_path: Path, \n",
    "                          model, \n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float = SIMILARITY_THRESHOLD,\n",
    "                          use_prototype: bool = True) -> Dict:\n",
    "    \"\"\"Classify pages in a PDF using zero-shot similarity matching\"\"\"\n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'contains_form': False,\n",
    "        'form_pages': [],\n",
    "        'page_scores': [],\n",
    "        'max_similarity': 0.0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            embedding = extract_embedding_single(image, model, processor, model_type)\n",
    "            \n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            results['page_scores'].append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                results['form_pages'].append(page_num)\n",
    "                results['contains_form'] = True\n",
    "            \n",
    "            results['max_similarity'] = max(results['max_similarity'], similarity)\n",
    "        \n",
    "        results['total_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def classify_pdf_batch_parallel(pdf_paths: List[Path],\n",
    "                               reference_embeddings: np.ndarray,\n",
    "                               reference_prototype: np.ndarray,\n",
    "                               model_type: str,\n",
    "                               threshold: float = SIMILARITY_THRESHOLD,\n",
    "                               use_prototype: bool = True,\n",
    "                               n_workers: int = None) -> List[Dict]:\n",
    "    \"\"\"Classify a batch of PDFs in parallel\"\"\"\n",
    "    \n",
    "    # Extract embeddings in parallel\n",
    "    embedding_results = process_pdfs_parallel(pdf_paths, model_type, n_workers)\n",
    "    \n",
    "    # Classify based on embeddings\n",
    "    classification_results = []\n",
    "    \n",
    "    for result in embedding_results:\n",
    "        if result['error']:\n",
    "            classification_results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_form': False,\n",
    "                'form_pages': [],\n",
    "                'page_scores': [],\n",
    "                'max_similarity': 0.0,\n",
    "                'error': result['error']\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        form_pages = []\n",
    "        page_scores = []\n",
    "        max_similarity = 0.0\n",
    "        \n",
    "        for embedding, page_num in zip(result['embeddings'], result['page_numbers']):\n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            page_scores.append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                form_pages.append(page_num)\n",
    "            \n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        classification_results.append({\n",
    "            'filename': result['filename'],\n",
    "            'contains_form': len(form_pages) > 0,\n",
    "            'form_pages': form_pages,\n",
    "            'page_scores': page_scores,\n",
    "            'max_similarity': max_similarity,\n",
    "            'total_pages': result.get('num_pages', len(result['embeddings'])),\n",
    "            'error': None\n",
    "        })\n",
    "    \n",
    "    return classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Threshold tuning\n",
    "print(\"Testing on known examples to tune threshold...\")\n",
    "\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "\n",
    "# Test on subset of examples\n",
    "print(\"\\nComputing self-similarity for positive examples...\")\n",
    "for i in range(min(50, len(reference_embeddings))):\n",
    "    embedding = reference_embeddings[i]\n",
    "    other_embeddings = np.delete(reference_embeddings, i, axis=0)\n",
    "    similarity = compute_similarity_to_references(embedding, other_embeddings, method='max')\n",
    "    positive_scores.append(similarity)\n",
    "\n",
    "# Test on non-examples\n",
    "print(\"\\nTesting on non-examples...\")\n",
    "non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))[:20]\n",
    "\n",
    "for pdf_path in tqdm(non_example_files, desc=\"Processing non-examples\"):\n",
    "    result = classify_pdf_zero_shot(\n",
    "        pdf_path, model, processor, MODEL_TYPE,\n",
    "        reference_embeddings, reference_prototype,\n",
    "        threshold=0.0,\n",
    "        use_prototype=False\n",
    "    )\n",
    "    \n",
    "    if result['page_scores']:\n",
    "        for page_score in result['page_scores']:\n",
    "            negative_scores.append(page_score['similarity'])\n",
    "\n",
    "# Analyze scores\n",
    "positive_scores = np.array(positive_scores)\n",
    "negative_scores = np.array(negative_scores)\n",
    "\n",
    "print(f\"\\nPositive scores - Mean: {positive_scores.mean():.3f}, Std: {positive_scores.std():.3f}\")\n",
    "print(f\"Negative scores - Mean: {negative_scores.mean():.3f}, Std: {negative_scores.std():.3f}\")\n",
    "\n",
    "# Find optimal threshold\n",
    "gap_threshold = (positive_scores.min() + negative_scores.max()) / 2\n",
    "percentile_threshold = np.percentile(negative_scores, 99)\n",
    "OPTIMAL_THRESHOLD = max(gap_threshold, percentile_threshold)\n",
    "print(f\"\\nSelected optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Batch processing function\n",
    "def process_document_folder(folder_path: Path,\n",
    "                          model,\n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float,\n",
    "                          output_file: str = 'zero_shot_results.csv',\n",
    "                          max_files: Optional[int] = None,\n",
    "                          use_prototype: bool = True,\n",
    "                          use_parallel: bool = None) -> pd.DataFrame:\n",
    "    \"\"\"Process all PDFs in a folder using zero-shot classification\"\"\"\n",
    "    \n",
    "    if use_parallel is None:\n",
    "        use_parallel = USE_MULTIPROCESSING\n",
    "    \n",
    "    results = []\n",
    "    pdf_files = list(folder_path.glob('*.pdf'))\n",
    "    \n",
    "    if max_files:\n",
    "        pdf_files = pdf_files[:max_files]\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    \n",
    "    if use_parallel and len(pdf_files) > 10:\n",
    "        # Use parallel processing\n",
    "        print(f\"Using parallel processing with {N_WORKERS} workers...\")\n",
    "        batch_size = min(MAX_PDFS_PER_WORKER * N_WORKERS, len(pdf_files))\n",
    "        \n",
    "        for i in range(0, len(pdf_files), batch_size):\n",
    "            batch_files = pdf_files[i:i+batch_size]\n",
    "            print(f\"\\nProcessing batch {i//batch_size + 1}/{(len(pdf_files) + batch_size - 1)//batch_size}\")\n",
    "            \n",
    "            batch_results = classify_pdf_batch_parallel(\n",
    "                batch_files,\n",
    "                reference_embeddings,\n",
    "                reference_prototype,\n",
    "                model_type,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype,\n",
    "                n_workers=N_WORKERS\n",
    "            )\n",
    "            \n",
    "            for result in batch_results:\n",
    "                results.append({\n",
    "                    'filename': result['filename'],\n",
    "                    'contains_form': result['contains_form'],\n",
    "                    'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "                    'num_form_pages': len(result['form_pages']),\n",
    "                    'total_pages': result.get('total_pages', 0),\n",
    "                    'max_similarity': result['max_similarity'],\n",
    "                    'error': result['error']\n",
    "                })\n",
    "    else:\n",
    "        # Use sequential processing\n",
    "        print(\"Using sequential processing...\")\n",
    "        for pdf_path in tqdm(pdf_files):\n",
    "            result = classify_pdf_zero_shot(\n",
    "                pdf_path, model, processor, model_type,\n",
    "                reference_embeddings, reference_prototype,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_form': result['contains_form'],\n",
    "                'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "                'num_form_pages': len(result['form_pages']),\n",
    "                'total_pages': result.get('total_pages', 0),\n",
    "                'max_similarity': result['max_similarity'],\n",
    "                'error': result['error']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    print(f\"Documents with forms: {df_results['contains_form'].sum()}\")\n",
    "    print(f\"Documents without forms: {(~df_results['contains_form']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['error'].notna().sum()}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Test classification\n",
    "print(\"Testing on sample documents...\")\n",
    "\n",
    "test_results = process_document_folder(\n",
    "    NON_EXAMPLES_PATH,\n",
    "    model,\n",
    "    processor,\n",
    "    MODEL_TYPE,\n",
    "    reference_embeddings,\n",
    "    reference_prototype,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='zero_shot_test_results.csv',\n",
    "    max_files=50,\n",
    "    use_prototype=False\n",
    ")\n",
    "\n",
    "print(\"\\nSample results:\")\n",
    "print(test_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Performance comparison (Sequential vs Parallel)\n",
    "print(\"\\nPerformance Comparison: Sequential vs Parallel Processing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))[:20]\n",
    "\n",
    "if len(test_files) >= 10:\n",
    "    # Sequential processing\n",
    "    print(\"\\n1. Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    seq_results = process_document_folder(\n",
    "        NON_EXAMPLES_PATH,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file='sequential_test.csv',\n",
    "        max_files=20,\n",
    "        use_prototype=False,\n",
    "        use_parallel=False\n",
    "    )\n",
    "    seq_time = time.time() - start_time\n",
    "    print(f\"Sequential time: {seq_time:.1f} seconds\")\n",
    "    \n",
    "    # Parallel processing\n",
    "    print(\"\\n2. Parallel Processing:\")\n",
    "    start_time = time.time()\n",
    "    par_results = process_document_folder(\n",
    "        NON_EXAMPLES_PATH,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file='parallel_test.csv',\n",
    "        max_files=20,\n",
    "        use_prototype=False,\n",
    "        use_parallel=True\n",
    "    )\n",
    "    par_time = time.time() - start_time\n",
    "    print(f\"Parallel time: {par_time:.1f} seconds\")\n",
    "    \n",
    "    # Results\n",
    "    speedup = seq_time / par_time\n",
    "    print(f\"\\nSpeedup: {speedup:.1f}x faster with {N_WORKERS} workers\")\n",
    "    print(f\"\\nEstimated time for 190k documents:\")\n",
    "    print(f\"Sequential: {190000 * seq_time / len(test_files) / 3600:.1f} hours\")\n",
    "    print(f\"Parallel: {190000 * par_time / len(test_files) / 3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Process full corpus (example code)\n",
    "print(\"\\nExample code for processing full corpus:\")\n",
    "print(\"\"\"\n",
    "# To process your full 190k corpus:\n",
    "CORPUS_PATH = BASE_PATH / 'data' / 'raw' / 'all_contracts'\n",
    "\n",
    "results_df = process_document_folder(\n",
    "    CORPUS_PATH,\n",
    "    model,\n",
    "    processor,\n",
    "    MODEL_TYPE,\n",
    "    reference_embeddings,\n",
    "    reference_prototype,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='full_corpus_results.csv',\n",
    "    use_prototype=False,  # Use full reference set for better accuracy\n",
    "    use_parallel=True     # Use parallel processing\n",
    ")\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}