{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Administrative Form Classifier\n",
    "\n",
    "This notebook implements a zero-shot classifier that identifies administrative forms by comparing page embeddings to reference examples.\n",
    "\n",
    "## Advantages:\n",
    "- **No training required** - Works immediately with example forms\n",
    "- **Fast inference** - Process 190k documents efficiently\n",
    "- **Easy to update** - Add new examples without retraining\n",
    "- **Interpretable** - Simple similarity threshold\n",
    "\n",
    "## Approach:\n",
    "1. Extract embeddings from all example administrative forms\n",
    "2. Create a reference representation (mean embedding or keep all)\n",
    "3. For each page in a document, compute similarity to reference\n",
    "4. Classify based on similarity threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set tokenizers parallelism to false to avoid fork warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For different model options\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoImageProcessor, \n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Model selection\n",
    "MODEL_TYPE = \"donut\"  # Options: \"donut\", \"clip\", \"dinov2\"\n",
    "\n",
    "# Processing parameters\n",
    "IMAGE_DPI = 150  # Resolution for PDF conversion\n",
    "BATCH_SIZE = 8   # Batch size for embedding extraction\n",
    "SIMILARITY_THRESHOLD = 0.85  # Initial threshold (will be tuned)\n",
    "\n",
    "# Output paths\n",
    "EMBEDDINGS_CACHE_PATH = BASE_PATH / 'code' / 'preprocessing' / 'cached_embeddings'\n",
    "EMBEDDINGS_CACHE_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type: str):\n",
    "    \"\"\"Load the specified model for embedding extraction\"\"\"\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        # Check if fine-tuned model exists\n",
    "        MODEL_DIR = \"./form_classifier_model\"\n",
    "        if os.path.exists(MODEL_DIR):\n",
    "            print(f\"Loading fine-tuned Donut model from {MODEL_DIR}\")\n",
    "            model = VisionEncoderDecoderModel.from_pretrained(MODEL_DIR)\n",
    "            processor = DonutProcessor.from_pretrained(MODEL_DIR)\n",
    "        else:\n",
    "            print(\"Loading base Donut model\")\n",
    "            model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "            processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        return model.encoder.to(device), processor\n",
    "    \n",
    "    elif model_type == \"clip\":\n",
    "        print(\"Loading CLIP model\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        return model.vision_model.to(device), processor\n",
    "    \n",
    "    elif model_type == \"dinov2\":\n",
    "        print(\"Loading DINOv2 model\")\n",
    "        processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        return model.to(device), processor\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Load the selected model\n",
    "model, processor = load_model(MODEL_TYPE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: Path, dpi: int = IMAGE_DPI) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF to list of PIL Images\"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image(image: Image.Image, model_type: str) -> Image.Image:\n",
    "    \"\"\"Preprocess image based on model requirements\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        # Donut expects larger images\n",
    "        image.thumbnail((1280, 960), Image.Resampling.LANCZOS)\n",
    "    else:\n",
    "        # CLIP and DINOv2 work with smaller images\n",
    "        image.thumbnail((224, 224), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings(images: List[Image.Image], model, processor, model_type: str, \n",
    "                      batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings for a list of images\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        \n",
    "        # Preprocess images\n",
    "        batch_images = [preprocess_image(img, model_type) for img in batch_images]\n",
    "        \n",
    "        # Process batch\n",
    "        if model_type in [\"donut\", \"clip\"]:\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        else:  # dinov2\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract embeddings based on model type\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            batch_embeddings = outputs.pooler_output\n",
    "        elif hasattr(outputs, 'last_hidden_state'):\n",
    "            # Mean pool the sequence dimension\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            batch_embeddings = outputs[0].mean(dim=1)  # Fallback\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embedding_single(image: Image.Image, model, processor, model_type: str) -> np.ndarray:\n",
    "    \"\"\"Extract embedding for a single image\"\"\"\n",
    "    image = preprocess_image(image, model_type)\n",
    "    \n",
    "    if model_type in [\"donut\", \"clip\"]:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    else:  # dinov2\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embedding\n",
    "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "        embedding = outputs.pooler_output\n",
    "    elif hasattr(outputs, 'last_hidden_state'):\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    else:\n",
    "        embedding = outputs[0].mean(dim=1)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cached embeddings exist\n",
    "cache_file = EMBEDDINGS_CACHE_PATH / f\"{MODEL_TYPE}_reference_embeddings.pkl\"\n",
    "\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading cached reference embeddings from {cache_file}\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "        reference_embeddings = cache_data['embeddings']\n",
    "        reference_metadata = cache_data['metadata']\n",
    "else:\n",
    "    print(\"Building reference embeddings from example forms...\")\n",
    "    reference_embeddings = []\n",
    "    reference_metadata = []\n",
    "    \n",
    "    pdf_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "    print(f\"Processing {len(pdf_files)} example form PDFs\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing example forms\"):\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        \n",
    "        if images:\n",
    "            # Extract embeddings for all pages\n",
    "            embeddings = extract_embeddings(images, model, processor, MODEL_TYPE)\n",
    "            \n",
    "            # Store embeddings and metadata\n",
    "            for i, embedding in enumerate(embeddings):\n",
    "                reference_embeddings.append(embedding)\n",
    "                reference_metadata.append({\n",
    "                    'file_path': str(pdf_path),\n",
    "                    'filename': pdf_path.name,\n",
    "                    'page_num': i + 1,\n",
    "                    'total_pages': len(images)\n",
    "                })\n",
    "    \n",
    "    reference_embeddings = np.array(reference_embeddings)\n",
    "    \n",
    "    # Cache the embeddings\n",
    "    print(f\"Caching reference embeddings to {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': reference_embeddings,\n",
    "            'metadata': reference_metadata,\n",
    "            'model_type': MODEL_TYPE\n",
    "        }, f)\n",
    "\n",
    "print(f\"\\nReference set contains {len(reference_embeddings)} form pages\")\n",
    "print(f\"Embedding dimension: {reference_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Similarity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reference representation\n",
    "print(\"Computing reference representation...\")\n",
    "\n",
    "# Option 1: Mean embedding (prototype)\n",
    "reference_prototype = reference_embeddings.mean(axis=0)\n",
    "print(f\"Reference prototype shape: {reference_prototype.shape}\")\n",
    "\n",
    "# Option 2: Keep all embeddings for k-NN style matching\n",
    "# This is more flexible but slower\n",
    "\n",
    "def compute_similarity_to_prototype(embedding: np.ndarray, prototype: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity to prototype\"\"\"\n",
    "    return cosine_similarity(embedding.reshape(1, -1), prototype.reshape(1, -1))[0, 0]\n",
    "\n",
    "def compute_similarity_to_references(embedding: np.ndarray, references: np.ndarray, \n",
    "                                   method: str = 'max') -> float:\n",
    "    \"\"\"Compute similarity to reference set\n",
    "    \n",
    "    Args:\n",
    "        embedding: Single embedding to compare\n",
    "        references: Array of reference embeddings\n",
    "        method: 'max', 'mean', or 'top_k'\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity(embedding.reshape(1, -1), references)[0]\n",
    "    \n",
    "    if method == 'max':\n",
    "        return similarities.max()\n",
    "    elif method == 'mean':\n",
    "        return similarities.mean()\n",
    "    elif method == 'top_k':\n",
    "        # Average of top 5 most similar\n",
    "        k = min(5, len(similarities))\n",
    "        return np.sort(similarities)[-k:].mean()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zero-Shot Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pdf_zero_shot(pdf_path: Path, \n",
    "                          model, \n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float = SIMILARITY_THRESHOLD,\n",
    "                          use_prototype: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Classify pages in a PDF using zero-shot similarity matching\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'contains_form': False,\n",
    "        'form_pages': [],\n",
    "        'page_scores': [],\n",
    "        'max_similarity': 0.0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            # Extract embedding\n",
    "            embedding = extract_embedding_single(image, model, processor, model_type)\n",
    "            \n",
    "            # Compute similarity\n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            results['page_scores'].append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            # Check if page is a form\n",
    "            if similarity >= threshold:\n",
    "                results['form_pages'].append(page_num)\n",
    "                results['contains_form'] = True\n",
    "            \n",
    "            results['max_similarity'] = max(results['max_similarity'], similarity)\n",
    "        \n",
    "        results['total_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on known examples and non-examples to find optimal threshold\n",
    "print(\"Testing on known examples to tune threshold...\")\n",
    "\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "\n",
    "# Test on subset of examples (already processed)\n",
    "print(\"\\nComputing self-similarity for positive examples...\")\n",
    "for i in range(min(50, len(reference_embeddings))):\n",
    "    # Leave-one-out similarity\n",
    "    embedding = reference_embeddings[i]\n",
    "    other_embeddings = np.delete(reference_embeddings, i, axis=0)\n",
    "    similarity = compute_similarity_to_references(embedding, other_embeddings, method='max')\n",
    "    positive_scores.append(similarity)\n",
    "\n",
    "# Test on non-examples\n",
    "print(\"\\nTesting on non-examples...\")\n",
    "non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))[:20]  # Test subset\n",
    "\n",
    "for pdf_path in tqdm(non_example_files, desc=\"Processing non-examples\"):\n",
    "    result = classify_pdf_zero_shot(\n",
    "        pdf_path, model, processor, MODEL_TYPE,\n",
    "        reference_embeddings, reference_prototype,\n",
    "        threshold=0.0,  # Set to 0 to get all scores\n",
    "        use_prototype=False\n",
    "    )\n",
    "    \n",
    "    if result['page_scores']:\n",
    "        for page_score in result['page_scores']:\n",
    "            negative_scores.append(page_score['similarity'])\n",
    "\n",
    "# Analyze scores\n",
    "positive_scores = np.array(positive_scores)\n",
    "negative_scores = np.array(negative_scores)\n",
    "\n",
    "print(f\"\\nPositive scores - Mean: {positive_scores.mean():.3f}, Std: {positive_scores.std():.3f}\")\n",
    "print(f\"Positive scores - Min: {positive_scores.min():.3f}, Max: {positive_scores.max():.3f}\")\n",
    "print(f\"\\nNegative scores - Mean: {negative_scores.mean():.3f}, Std: {negative_scores.std():.3f}\")\n",
    "print(f\"Negative scores - Min: {negative_scores.min():.3f}, Max: {negative_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(positive_scores, bins=30, alpha=0.7, label='Form pages', color='green')\n",
    "plt.hist(negative_scores, bins=30, alpha=0.7, label='Non-form pages', color='red')\n",
    "plt.axvline(x=SIMILARITY_THRESHOLD, color='black', linestyle='--', \n",
    "            label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
    "plt.xlabel('Cosine Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold\n",
    "# Simple approach: use value that separates distributions\n",
    "gap_threshold = (positive_scores.min() + negative_scores.max()) / 2\n",
    "percentile_threshold = np.percentile(negative_scores, 99)  # 99th percentile of negatives\n",
    "\n",
    "print(f\"\\nSuggested thresholds:\")\n",
    "print(f\"Gap-based threshold: {gap_threshold:.3f}\")\n",
    "print(f\"99th percentile threshold: {percentile_threshold:.3f}\")\n",
    "\n",
    "# Update threshold\n",
    "OPTIMAL_THRESHOLD = max(gap_threshold, percentile_threshold)\n",
    "print(f\"\\nSelected optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_folder(folder_path: Path,\n",
    "                          model,\n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float,\n",
    "                          output_file: str = 'zero_shot_results.csv',\n",
    "                          max_files: Optional[int] = None,\n",
    "                          use_prototype: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all PDFs in a folder using zero-shot classification\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    pdf_files = list(folder_path.glob('*.pdf'))\n",
    "    \n",
    "    if max_files:\n",
    "        pdf_files = pdf_files[:max_files]\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files):\n",
    "        result = classify_pdf_zero_shot(\n",
    "            pdf_path, model, processor, model_type,\n",
    "            reference_embeddings, reference_prototype,\n",
    "            threshold=threshold,\n",
    "            use_prototype=use_prototype\n",
    "        )\n",
    "        \n",
    "        # Flatten results for CSV\n",
    "        results.append({\n",
    "            'filename': result['filename'],\n",
    "            'contains_form': result['contains_form'],\n",
    "            'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "            'num_form_pages': len(result['form_pages']),\n",
    "            'total_pages': result.get('total_pages', 0),\n",
    "            'max_similarity': result['max_similarity'],\n",
    "            'error': result['error']\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    print(f\"Documents with forms: {df_results['contains_form'].sum()}\")\n",
    "    print(f\"Documents without forms: {(~df_results['contains_form']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['error'].notna().sum()}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample first\n",
    "print(\"Testing on sample documents...\")\n",
    "\n",
    "# Create a test folder with mixed examples\n",
    "test_results = process_document_folder(\n",
    "    NON_EXAMPLES_PATH,\n",
    "    model,\n",
    "    processor,\n",
    "    MODEL_TYPE,\n",
    "    reference_embeddings,\n",
    "    reference_prototype,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='zero_shot_test_results.csv',\n",
    "    max_files=50,\n",
    "    use_prototype=False  # Use full reference set for better accuracy\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "print(test_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Process Full Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for processing the full 190k corpus\n",
    "# Uncomment and modify the path as needed\n",
    "\n",
    "\"\"\"\n",
    "CORPUS_PATH = BASE_PATH / 'data' / 'raw' / 'all_contracts'  # Adjust path\n",
    "\n",
    "# Process in batches to save memory and allow interruption\n",
    "batch_size = 1000\n",
    "all_pdfs = list(CORPUS_PATH.glob('*.pdf'))\n",
    "total_batches = (len(all_pdfs) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Processing {len(all_pdfs)} documents in {total_batches} batches...\")\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(all_pdfs))\n",
    "    \n",
    "    batch_pdfs = all_pdfs[start_idx:end_idx]\n",
    "    batch_folder = Path('/tmp/batch_pdfs')  # Temporary folder\n",
    "    batch_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create symlinks for batch\n",
    "    for pdf in batch_pdfs:\n",
    "        (batch_folder / pdf.name).symlink_to(pdf)\n",
    "    \n",
    "    # Process batch\n",
    "    batch_results = process_document_folder(\n",
    "        batch_folder,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file=f'zero_shot_results_batch_{batch_idx}.csv',\n",
    "        use_prototype=False\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    for link in batch_folder.glob('*.pdf'):\n",
    "        link.unlink()\n",
    "    \n",
    "    print(f\"Completed batch {batch_idx + 1}/{total_batches}\")\n",
    "\n",
    "# Combine all batch results\n",
    "all_results = []\n",
    "for batch_idx in range(total_batches):\n",
    "    batch_df = pd.read_csv(f'zero_shot_results_batch_{batch_idx}.csv')\n",
    "    all_results.append(batch_df)\n",
    "\n",
    "final_results = pd.concat(all_results, ignore_index=True)\n",
    "final_results.to_csv('zero_shot_results_complete.csv', index=False)\n",
    "print(f\"\\nComplete results saved to zero_shot_results_complete.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with trained model performance (if available)\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"Model type: {MODEL_TYPE}\")\n",
    "print(f\"Reference examples: {len(reference_embeddings)}\")\n",
    "print(f\"Optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")\n",
    "print(f\"\\nProcessing speed: ~{len(test_results) / 60:.1f} documents per minute\")\n",
    "\n",
    "# Estimate time for full corpus\n",
    "docs_per_minute = len(test_results) / 60  # Rough estimate\n",
    "total_minutes = 190000 / docs_per_minute\n",
    "print(f\"\\nEstimated time for 190k documents: {total_minutes/60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### For production use:\n",
    "\n",
    "1. **Model Selection**: Try different models (CLIP, DINOv2) to see which works best\n",
    "2. **Threshold Tuning**: Use more examples/non-examples to fine-tune threshold\n",
    "3. **Optimization**: \n",
    "   - Use GPU for faster processing\n",
    "   - Process in parallel with multiprocessing\n",
    "   - Cache embeddings for documents you process repeatedly\n",
    "\n",
    "### Advantages over training:\n",
    "- **Immediate deployment** - No training time\n",
    "- **Easy updates** - Just add new reference examples\n",
    "- **Interpretable** - Can inspect which references match\n",
    "- **Flexible threshold** - Adjust precision/recall trade-off easily\n",
    "\n",
    "### When to use training instead:\n",
    "- If zero-shot accuracy is insufficient\n",
    "- If you need to learn subtle patterns\n",
    "- If you have many labeled examples\n",
    "- If inference speed is critical (trained models can be faster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}