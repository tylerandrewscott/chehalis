{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administrative Form Detection using Donut\n",
    "\n",
    "This notebook implements a classifier to detect standardized administrative forms in PDF documents using the Donut transformer model.\n",
    "\n",
    "## Objectives:\n",
    "1. Detect if a PDF document contains a specific standardized administrative form\n",
    "2. Identify which page number the form appears on\n",
    "\n",
    "## Approach:\n",
    "- Use Donut (Document Understanding Transformer) for OCR-free document classification\n",
    "- Process PDFs page by page to identify the administrative form\n",
    "\n",
    "## Future Options:\n",
    "- Consider LayoutLMv3 for combining visual and text features if higher accuracy is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install transformers torch torchvision pdf2image pillow numpy pandas tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set tokenizers parallelism to false to avoid fork warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "from transformers import (\n",
    "    DonutProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example forms path exists: True\n",
      "Non-examples path exists: True\n",
      "\n",
      "Found 106 example form files\n",
      "  - 25581-000.pdf\n",
      "  - 99171-000.pdf\n",
      "  - 13924-002.pdf\n",
      "  - 1197-000.pdf\n",
      "  - 67419-000.pdf\n",
      "\n",
      "Found 133 non-example files\n",
      "  - 25581-000.pdf\n",
      "  - 0000000000000000000062223-001.pdf\n",
      "  - 1197-000.pdf\n",
      "  - 67419-000.pdf\n",
      "  - 104473-000.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Check if paths exist\n",
    "print(f\"Example forms path exists: {EXAMPLE_FORMS_PATH.exists()}\")\n",
    "print(f\"Non-examples path exists: {NON_EXAMPLES_PATH.exists()}\")\n",
    "\n",
    "# List example files\n",
    "if EXAMPLE_FORMS_PATH.exists():\n",
    "    example_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "    print(f\"\\nFound {len(example_files)} example form files\")\n",
    "    for f in example_files[:5]:  # Show first 5\n",
    "        print(f\"  - {f.name}\")\n",
    "\n",
    "# List non-example files\n",
    "if NON_EXAMPLES_PATH.exists():\n",
    "    non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))\n",
    "    print(f\"\\nFound {len(non_example_files)} non-example files\")\n",
    "    for f in non_example_files[:5]:  # Show first 5\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: Path, dpi: int = 200) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Convert PDF to list of PIL Images (one per page)\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        dpi: Resolution for conversion\n",
    "    \n",
    "    Returns:\n",
    "        List of PIL Images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image_for_donut(image: Image.Image, size: Tuple[int, int] = (1280, 960)) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Preprocess image for Donut model\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        size: Target size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed PIL Image\n",
    "    \"\"\"\n",
    "    # Convert to RGB if necessary\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Resize while maintaining aspect ratio\n",
    "    image.thumbnail(size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for administrative form classification with lazy loading support\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list: List[Dict], processor, lazy_load: bool = True):\n",
    "        self.data = data_list\n",
    "        self.processor = processor\n",
    "        self.lazy_load = lazy_load\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load image if needed (lazy loading)\n",
    "        if item.get('needs_loading', False) and self.lazy_load:\n",
    "            # Load specific page from PDF\n",
    "            pdf_path = Path(item['image_path'])\n",
    "            page_num = item['page_num'] - 1  # 0-indexed for pdf2image\n",
    "            \n",
    "            try:\n",
    "                # Load only the specific page\n",
    "                images = convert_from_path(pdf_path, dpi=150, \n",
    "                                         first_page=page_num+1, \n",
    "                                         last_page=page_num+1)\n",
    "                if images:\n",
    "                    image = preprocess_image_for_donut(images[0])\n",
    "                else:\n",
    "                    # Return a blank image if loading fails\n",
    "                    image = Image.new('RGB', (1280, 960), color='white')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading page {page_num+1} from {pdf_path.name}: {e}\")\n",
    "                # Return a blank image\n",
    "                image = Image.new('RGB', (1280, 960), color='white')\n",
    "        else:\n",
    "            image = item['image']\n",
    "        \n",
    "        # Process image\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values.squeeze(),\n",
    "            'labels': item['label'],\n",
    "            'metadata': {\n",
    "                'file_path': item.get('file_path', ''),\n",
    "                'page_num': item.get('page_num', -1),\n",
    "                'source_folder': item.get('source_folder', ''),\n",
    "                'original_filename': item.get('original_filename', '')\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_pdfs(example_path: Path, non_example_path: Optional[Path] = None, \n",
    "                           max_samples_per_class: int = None, batch_process: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create dataset from PDF files with class balancing\n",
    "    \n",
    "    IMPORTANT: Handles the case where:\n",
    "    - Example form PDFs: ALL pages are the administrative form (some PDFs may be multi-page)\n",
    "    - Non-example PDFs: NO pages are the administrative form\n",
    "    \n",
    "    Strategy:\n",
    "    - Use ALL pages from ALL positive example PDFs\n",
    "    - Randomly sample 2x that number of negative pages\n",
    "    \n",
    "    Args:\n",
    "        example_path: Path to example forms\n",
    "        non_example_path: Path to non-examples\n",
    "        max_samples_per_class: Limit number of PDFs to process (for testing)\n",
    "        batch_process: If True, process images in batches to save memory\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with 'image', 'label', 'file_path', 'page_num', 'source_folder'\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Process ALL positive examples (forms)\n",
    "    # Every page in every PDF in the examples folder is a positive example\n",
    "    positive_count = 0\n",
    "    if example_path.exists():\n",
    "        pdf_files = list(example_path.glob('*.pdf'))\n",
    "        if max_samples_per_class:\n",
    "            pdf_files = pdf_files[:max_samples_per_class]\n",
    "            \n",
    "        print(f\"Processing {len(pdf_files)} positive example PDFs...\")\n",
    "        print(\"Note: Every page in these PDFs is considered a positive example (the form)\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Positive examples\"):\n",
    "            try:\n",
    "                images = pdf_to_images(pdf_path, dpi=150)  # Lower DPI to save memory\n",
    "                \n",
    "                for page_num, image in enumerate(images):\n",
    "                    # Process and store image path instead of image if batch_process\n",
    "                    if batch_process:\n",
    "                        dataset.append({\n",
    "                            'image_path': str(pdf_path),\n",
    "                            'page_num': page_num + 1,\n",
    "                            'label': 1,  # 1 for administrative form\n",
    "                            'file_path': str(pdf_path),\n",
    "                            'source_folder': 'examples',\n",
    "                            'original_filename': pdf_path.name,\n",
    "                            'needs_loading': True\n",
    "                        })\n",
    "                    else:\n",
    "                        dataset.append({\n",
    "                            'image': preprocess_image_for_donut(image),\n",
    "                            'label': 1,\n",
    "                            'file_path': str(pdf_path),\n",
    "                            'page_num': page_num + 1,\n",
    "                            'source_folder': 'examples',\n",
    "                            'original_filename': pdf_path.name\n",
    "                        })\n",
    "                    positive_count += 1\n",
    "                    \n",
    "                # Clear images from memory\n",
    "                del images\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal positive examples (form pages): {positive_count}\")\n",
    "    \n",
    "    # Process negative examples (non-forms)\n",
    "    # Sample 2x the number of positive examples\n",
    "    negative_metadata = []  # Store metadata first\n",
    "    \n",
    "    if non_example_path and non_example_path.exists():\n",
    "        pdf_files = list(non_example_path.glob('*.pdf'))\n",
    "        print(f\"\\nProcessing {len(pdf_files)} negative example PDFs...\")\n",
    "        print(\"Note: No pages in these PDFs contain the administrative form\")\n",
    "        \n",
    "        # First, collect metadata for all negative pages\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Collecting negative page info\"):\n",
    "            try:\n",
    "                # Use pdfinfo to get page count without loading images\n",
    "                import subprocess\n",
    "                result = subprocess.run(['pdfinfo', str(pdf_path)], \n",
    "                                      capture_output=True, text=True)\n",
    "                if result.returncode == 0:\n",
    "                    for line in result.stdout.split('\\n'):\n",
    "                        if line.startswith('Pages:'):\n",
    "                            num_pages = int(line.split()[1])\n",
    "                            for page_num in range(num_pages):\n",
    "                                negative_metadata.append({\n",
    "                                    'image_path': str(pdf_path),\n",
    "                                    'page_num': page_num + 1,\n",
    "                                    'label': 0,\n",
    "                                    'file_path': str(pdf_path),\n",
    "                                    'source_folder': 'non_examples',\n",
    "                                    'original_filename': pdf_path.name,\n",
    "                                    'needs_loading': True\n",
    "                                })\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting info for {pdf_path.name}: {e}\")\n",
    "        \n",
    "        print(f\"Total negative pages available: {len(negative_metadata)}\")\n",
    "        \n",
    "        # Sample 2x the number of positive examples\n",
    "        target_negative_count = min(len(negative_metadata), positive_count * 2)\n",
    "        \n",
    "        if len(negative_metadata) > target_negative_count:\n",
    "            # Randomly sample to achieve 2:1 ratio\n",
    "            import random\n",
    "            random.seed(42)  # For reproducibility\n",
    "            sampled_negatives = random.sample(negative_metadata, target_negative_count)\n",
    "            dataset.extend(sampled_negatives)\n",
    "            print(f\"Sampled {target_negative_count} negative pages (2x positive examples)\")\n",
    "        else:\n",
    "            # Use all available negatives if we don't have enough\n",
    "            dataset.extend(negative_metadata)\n",
    "            print(f\"Using all {len(negative_metadata)} negative pages\")\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    print(f\"\\nFinal dataset composition:\")\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Positive examples (forms): {sum(1 for d in dataset if d['label'] == 1)}\")\n",
    "    print(f\"Negative examples (non-forms): {sum(1 for d in dataset if d['label'] == 0)}\")\n",
    "    if sum(1 for d in dataset if d['label'] == 0) > 0:\n",
    "        print(f\"Class ratio (pos:neg): 1:{sum(1 for d in dataset if d['label'] == 0) / sum(1 for d in dataset if d['label'] == 1):.2f}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: naver-clova-ix/donut-base\n",
      "Model parameters: 201.9M\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Use pre-trained Donut for document classification\n",
    "# We'll fine-tune it for binary classification (form vs non-form)\n",
    "\n",
    "MODEL_NAME = \"naver-clova-ix/donut-base\"\n",
    "\n",
    "# Load processor and model\n",
    "processor = DonutProcessor.from_pretrained(MODEL_NAME)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Configure model for classification task\n",
    "# Donut uses a decoder, so we'll set up special tokens for our classification task\n",
    "\n",
    "# Add special tokens for our task\n",
    "processor.tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<admin_form>\",\n",
    "        \"<not_admin_form>\",\n",
    "        \"<classification>\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Resize model embeddings to accommodate new tokens\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Set up decoder start token\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([\"<classification>\"])[0]\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels_for_training(batch, processor):\n",
    "    \"\"\"\n",
    "    Prepare decoder labels for training\n",
    "    \"\"\"\n",
    "    labels_list = []\n",
    "    for label in batch['labels']:\n",
    "        if label == 1:\n",
    "            text = \"<classification> <admin_form>\"\n",
    "        else:\n",
    "            text = \"<classification> <not_admin_form>\"\n",
    "        \n",
    "        # Tokenize the label\n",
    "        encoding = processor.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=10,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels_list.append(encoding.input_ids.squeeze())\n",
    "    \n",
    "    return torch.stack(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, processor, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = prepare_labels_for_training(batch, processor).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, processor, device):\n",
    "    \"\"\"\n",
    "    Evaluate model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            true_labels.extend(batch['labels'].numpy())\n",
    "            \n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                pixel_values,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "                max_length=10,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                num_beams=1,\n",
    "                bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "            \n",
    "            # Decode predictions\n",
    "            prediction_text = processor.batch_decode(outputs.sequences)\n",
    "            \n",
    "            # Convert text predictions to binary labels\n",
    "            for pred_text in prediction_text:\n",
    "                if \"<admin_form>\" in pred_text:\n",
    "                    predictions.append(1)\n",
    "                else:\n",
    "                    predictions.append(0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 106 positive example PDFs...\n",
      "Note: Every page in these PDFs is considered a positive example (the form)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positive examples: 100%|██████████████████████| 106/106 [00:14<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total positive examples (form pages): 111\n",
      "\n",
      "Processing 133 negative example PDFs...\n",
      "Note: No pages in these PDFs contain the administrative form\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting negative page info: 100%|██████████| 133/133 [00:02<00:00, 47.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total negative pages available: 2896\n",
      "Sampled 222 negative pages (2x positive examples)\n",
      "\n",
      "Final dataset composition:\n",
      "Total samples: 333\n",
      "Positive examples (forms): 111\n",
      "Negative examples (non-forms): 222\n",
      "Class ratio (pos:neg): 1:2.00\n",
      "\n",
      "Train/Val Split:\n",
      "Training samples: 269 (87 positive, 182 negative)\n",
      "Validation samples: 64 (24 positive, 40 negative)\n",
      "Training documents: 157\n",
      "Validation documents: 40\n",
      "Document overlap: 0 (should be 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataset using ALL positive examples with memory-efficient loading\n",
    "dataset_list = create_dataset_from_pdfs(\n",
    "    EXAMPLE_FORMS_PATH,\n",
    "    NON_EXAMPLES_PATH,\n",
    "    batch_process=True  # Enable lazy loading to save memory\n",
    ")\n",
    "\n",
    "# Document-level train/validation split to prevent data leakage\n",
    "# Group by original filename and source folder to handle duplicates\n",
    "from collections import defaultdict\n",
    "\n",
    "doc_groups = defaultdict(list)\n",
    "for idx, item in enumerate(dataset_list):\n",
    "    # Create unique document ID combining source folder and filename\n",
    "    doc_id = f\"{item['source_folder']}_{item['original_filename']}\"\n",
    "    doc_groups[doc_id].append(idx)\n",
    "\n",
    "# Get unique document IDs and their labels (using first page's label)\n",
    "doc_ids = list(doc_groups.keys())\n",
    "doc_labels = [dataset_list[doc_groups[doc_id][0]]['label'] for doc_id in doc_ids]\n",
    "\n",
    "# Split documents (not pages) into train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_doc_ids, val_doc_ids = train_test_split(\n",
    "    doc_ids, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=doc_labels\n",
    ")\n",
    "\n",
    "# Get page indices for train and validation sets\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for doc_id in train_doc_ids:\n",
    "    train_indices.extend(doc_groups[doc_id])\n",
    "\n",
    "for doc_id in val_doc_ids:\n",
    "    val_indices.extend(doc_groups[doc_id])\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_data = [dataset_list[i] for i in train_indices]\n",
    "val_data = [dataset_list[i] for i in val_indices]\n",
    "\n",
    "print(f\"\\nTrain/Val Split:\")\n",
    "print(f\"Training samples: {len(train_data)} ({sum(1 for d in train_data if d['label'] == 1)} positive, {sum(1 for d in train_data if d['label'] == 0)} negative)\")\n",
    "print(f\"Validation samples: {len(val_data)} ({sum(1 for d in val_data if d['label'] == 1)} positive, {sum(1 for d in val_data if d['label'] == 0)} negative)\")\n",
    "print(f\"Training documents: {len(train_doc_ids)}\")\n",
    "print(f\"Validation documents: {len(val_doc_ids)}\")\n",
    "\n",
    "# Verify no document overlap\n",
    "train_docs = set(f\"{d['source_folder']}_{d['original_filename']}\" for d in train_data)\n",
    "val_docs = set(f\"{d['source_folder']}_{d['original_filename']}\" for d in val_data)\n",
    "print(f\"Document overlap: {len(train_docs.intersection(val_docs))} (should be 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders with lazy loading\n",
    "train_dataset = FormDataset(train_data, processor, lazy_load=True)\n",
    "val_dataset = FormDataset(val_data, processor, lazy_load=True)\n",
    "\n",
    "# Use smaller batch sizes to reduce memory usage\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with full precision...\n",
      "Training on 269 samples, validating on 64 samples\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/135 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Training:  69%|██████████████████▌        | 93/135 [2:46:48<1:15:20, 107.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, processor, device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate every epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, processor, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training configuration with optimizations\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 3  # Reduced from 5 for faster initial training\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Mixed precision training for faster computation (if using GPU)\n",
    "use_amp = device.type == 'cuda'\n",
    "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0\n",
    "training_history = []\n",
    "\n",
    "print(f\"Starting training with {'mixed precision' if use_amp else 'full precision'}...\")\n",
    "print(f\"Training on {len(train_data)} samples, validating on {len(val_data)} samples\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, processor, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate every epoch\n",
    "    val_metrics = evaluate(model, val_loader, processor, device)\n",
    "    print(f\"Validation accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Validation recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"Validation F1: {val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['accuracy'] > best_val_accuracy:\n",
    "        best_val_accuracy = val_metrics['accuracy']\n",
    "        torch.save(model.state_dict(), 'best_form_classifier.pth')\n",
    "        print(\"Saved best model!\")\n",
    "    \n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_accuracy': val_metrics['accuracy'],\n",
    "        'val_precision': val_metrics['precision'],\n",
    "        'val_recall': val_metrics['recall'],\n",
    "        'val_f1': val_metrics['f1']\n",
    "    })\n",
    "    \n",
    "    # Early stopping if perfect accuracy\n",
    "    if val_metrics['accuracy'] >= 0.99:\n",
    "        print(\"Achieved 99% accuracy, stopping early!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation accuracy: {best_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_form_in_pdf(pdf_path: Path, model, processor, device, batch_size: int = 8) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect if a PDF contains the administrative form and on which page(s)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'contains_form': boolean\n",
    "        - 'form_pages': list of page numbers where form is detected\n",
    "        - 'confidence_scores': confidence score for each page\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    if not images:\n",
    "        return {'contains_form': False, 'form_pages': [], 'confidence_scores': []}\n",
    "    \n",
    "    form_pages = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        batch_processed = [preprocess_image_for_donut(img) for img in batch_images]\n",
    "        \n",
    "        # Process images\n",
    "        pixel_values = torch.stack([\n",
    "            processor(img, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "            for img in batch_processed\n",
    "        ]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                pixel_values,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "                max_length=10,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                num_beams=1,\n",
    "                bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "            \n",
    "            # Decode predictions\n",
    "            predictions = processor.batch_decode(outputs.sequences)\n",
    "            \n",
    "            # Process predictions\n",
    "            for j, pred_text in enumerate(predictions):\n",
    "                page_num = i + j + 1\n",
    "                \n",
    "                if \"<admin_form>\" in pred_text:\n",
    "                    form_pages.append(page_num)\n",
    "                    # Simple confidence based on presence of token\n",
    "                    confidence_scores.append((page_num, 1.0))\n",
    "                else:\n",
    "                    confidence_scores.append((page_num, 0.0))\n",
    "    \n",
    "    return {\n",
    "        'contains_form': len(form_pages) > 0,\n",
    "        'form_pages': form_pages,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'total_pages': len(images)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_corpus(pdf_folder: Path, model, processor, device, \n",
    "                          output_file: str = 'form_detection_results.csv') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a corpus of PDF documents and save results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    pdf_files = list(pdf_folder.glob('*.pdf'))\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files):\n",
    "        try:\n",
    "            detection_result = detect_form_in_pdf(pdf_path, model, processor, device)\n",
    "            \n",
    "            results.append({\n",
    "                'filename': pdf_path.name,\n",
    "                'filepath': str(pdf_path),\n",
    "                'contains_form': detection_result['contains_form'],\n",
    "                'form_pages': ','.join(map(str, detection_result['form_pages'])),\n",
    "                'num_form_pages': len(detection_result['form_pages']),\n",
    "                'total_pages': detection_result['total_pages']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "            results.append({\n",
    "                'filename': pdf_path.name,\n",
    "                'filepath': str(pdf_path),\n",
    "                'contains_form': None,\n",
    "                'form_pages': '',\n",
    "                'num_form_pages': 0,\n",
    "                'total_pages': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    print(f\"Documents with forms: {df_results['contains_form'].sum()}\")\n",
    "    print(f\"Documents without forms: {(~df_results['contains_form']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['contains_form'].isna().sum()}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test on Example Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single example file\n",
    "if len(example_files) > 0:\n",
    "    test_pdf = example_files[0]\n",
    "    print(f\"Testing on: {test_pdf.name}\")\n",
    "    \n",
    "    result = detect_form_in_pdf(test_pdf, model, processor, device)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Contains form: {result['contains_form']}\")\n",
    "    print(f\"Form pages: {result['form_pages']}\")\n",
    "    print(f\"Total pages: {result['total_pages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process Full Corpus (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process a folder of documents\n",
    "# Uncomment and modify the path to process your full corpus\n",
    "\n",
    "# CORPUS_PATH = BASE_PATH / 'data' / 'raw' / 'contracts'  # Adjust to your corpus location\n",
    "# results_df = process_document_corpus(CORPUS_PATH, model, processor, device)\n",
    "\n",
    "# # Display some results\n",
    "# print(\"\\nSample results:\")\n",
    "# print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "save_directory = \"./form_classifier_model\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save model and processor\n",
    "model.save_pretrained(save_directory)\n",
    "processor.save_pretrained(save_directory)\n",
    "\n",
    "# Save training configuration\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'task': 'administrative_form_detection',\n",
    "    'num_epochs': num_epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'best_validation_accuracy': best_val_accuracy,\n",
    "    'training_history': training_history\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_directory, 'training_config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### To use the trained model on new documents:\n",
    "\n",
    "```python\n",
    "# Load the saved model\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained('./form_classifier_model')\n",
    "processor = DonutProcessor.from_pretrained('./form_classifier_model')\n",
    "\n",
    "# Detect form in a PDF\n",
    "result = detect_form_in_pdf(pdf_path, model, processor, device)\n",
    "print(f\"Form found on pages: {result['form_pages']}\")\n",
    "```\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "1. **Use LayoutLMv3** if you need to combine visual and text features for higher accuracy\n",
    "2. **Data Augmentation**: Add rotations, noise, etc. to training images\n",
    "3. **Confidence Calibration**: Implement proper probability scores for predictions\n",
    "4. **Multi-GPU Training**: For processing the full 190k document corpus\n",
    "5. **Active Learning**: Use model uncertainty to identify which documents to manually label next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
