{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Administrative Form Classifier\n",
    "\n",
    "This notebook implements a zero-shot classifier that identifies administrative forms by comparing page embeddings to reference examples.\n",
    "\n",
    "## Advantages:\n",
    "- **No training required** - Works immediately with example forms\n",
    "- **Fast inference** - Process 190k documents efficiently\n",
    "- **Easy to update** - Add new examples without retraining\n",
    "- **Interpretable** - Simple similarity threshold\n",
    "\n",
    "## Approach:\n",
    "1. Extract embeddings from all example administrative forms\n",
    "2. Create a reference representation (mean embedding or keep all)\n",
    "3. For each page in a document, compute similarity to reference\n",
    "4. Classify based on similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Available CPU cores: 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set tokenizers parallelism to false to avoid fork warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Multiprocessing imports\n",
    "from multiprocessing import Pool, cpu_count, Queue\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# For different model options\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoImageProcessor, \n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Available CPU cores: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set tokenizers parallelism to false to avoid fork warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For different model options\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoImageProcessor, \n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Model selection\n",
    "MODEL_TYPE = \"clip\"  # Options: \"donut\", \"clip\", \"dinov2\"\n",
    "\n",
    "# Processing parameters\n",
    "IMAGE_DPI = 150  # Resolution for PDF conversion\n",
    "BATCH_SIZE = 8   # Batch size for embedding extraction\n",
    "SIMILARITY_THRESHOLD = 0.85  # Initial threshold (will be tuned)\n",
    "\n",
    "# Parallel processing parameters\n",
    "USE_MULTIPROCESSING = True  # Enable multiprocessing\n",
    "N_WORKERS = cpu_count() - 1  # Leave one core free\n",
    "MAX_PDFS_PER_WORKER = 100  # PDFs per worker batch\n",
    "\n",
    "# Output paths\n",
    "EMBEDDINGS_CACHE_PATH = BASE_PATH / 'code' / 'preprocessing' / 'cached_embeddings'\n",
    "EMBEDDINGS_CACHE_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Model selection\n",
    "MODEL_TYPE = \"clip\"  # Options: \"donut\", \"clip\", \"dinov2\"\n",
    "\n",
    "# Processing parameters\n",
    "IMAGE_DPI = 100  # Resolution for PDF conversion\n",
    "BATCH_SIZE = 8   # Batch size for embedding extraction\n",
    "SIMILARITY_THRESHOLD = 0.85  # Initial threshold (will be tuned)\n",
    "\n",
    "# Output paths\n",
    "EMBEDDINGS_CACHE_PATH = BASE_PATH / 'code' / 'preprocessing' / 'cached_embeddings'\n",
    "EMBEDDINGS_CACHE_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_type: str):\n",
    "    \"\"\"Load the specified model for embedding extraction\"\"\"\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        # Check if fine-tuned model exists\n",
    "        MODEL_DIR = \"./form_classifier_model\"\n",
    "        if os.path.exists(MODEL_DIR):\n",
    "            print(f\"Loading fine-tuned Donut model from {MODEL_DIR}\")\n",
    "            model = VisionEncoderDecoderModel.from_pretrained(MODEL_DIR)\n",
    "            processor = DonutProcessor.from_pretrained(MODEL_DIR)\n",
    "        else:\n",
    "            print(\"Loading base Donut model\")\n",
    "            model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "            processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        return model.encoder.to(device), processor\n",
    "    \n",
    "    elif model_type == \"clip\":\n",
    "        print(\"Loading CLIP model\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        return model.vision_model.to(device), processor\n",
    "    \n",
    "    elif model_type == \"dinov2\":\n",
    "        print(\"Loading DINOv2 model\")\n",
    "        processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        return model.to(device), processor\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Load the selected model\n",
    "model, processor = load_model(MODEL_TYPE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing functions\n",
    "\n",
    "def init_worker(model_type_arg):\n",
    "    \"\"\"Initialize model in each worker process\"\"\"\n",
    "    global worker_model, worker_processor, worker_model_type\n",
    "    worker_model_type = model_type_arg\n",
    "    \n",
    "    # Each worker loads its own model\n",
    "    if model_type_arg == \"clip\":\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        worker_model = model.vision_model\n",
    "    elif model_type_arg == \"donut\":\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        worker_model = model.encoder\n",
    "    elif model_type_arg == \"dinov2\":\n",
    "        processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        worker_model = model\n",
    "    \n",
    "    worker_model.eval()\n",
    "    worker_processor = processor\n",
    "\n",
    "def process_pdf_parallel(pdf_path: Path) -> Dict:\n",
    "    \"\"\"Process a single PDF in a worker process\"\"\"\n",
    "    global worker_model, worker_processor, worker_model_type\n",
    "    \n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'path': str(pdf_path),\n",
    "        'embeddings': [],\n",
    "        'page_numbers': [],\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        images = pdf_to_images(pdf_path, dpi=IMAGE_DPI)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        # Extract embeddings for each page\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            try:\n",
    "                # Preprocess image\n",
    "                image = preprocess_image(image, worker_model_type)\n",
    "                \n",
    "                # Extract embedding (CPU only in worker)\n",
    "                with torch.no_grad():\n",
    "                    if worker_model_type in [\"donut\", \"clip\"]:\n",
    "                        inputs = worker_processor(images=image, return_tensors=\"pt\")\n",
    "                        outputs = worker_model(**inputs)\n",
    "                    else:  # dinov2\n",
    "                        inputs = worker_processor(images=image, return_tensors=\"pt\")\n",
    "                        outputs = worker_model(**inputs)\n",
    "                    \n",
    "                    # Extract embedding\n",
    "                    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                        embedding = outputs.pooler_output\n",
    "                    elif hasattr(outputs, 'last_hidden_state'):\n",
    "                        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "                    else:\n",
    "                        embedding = outputs[0].mean(dim=1) if isinstance(outputs, tuple) else outputs.mean(dim=1)\n",
    "                    \n",
    "                    embedding = embedding.numpy()[0]\n",
    "                \n",
    "                results['embeddings'].append(embedding)\n",
    "                results['page_numbers'].append(page_num)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_num} of {pdf_path.name}: {e}\")\n",
    "        \n",
    "        results['num_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_pdfs_parallel(pdf_files: List[Path], \n",
    "                         model_type: str,\n",
    "                         n_workers: int = None,\n",
    "                         show_progress: bool = True) -> List[Dict]:\n",
    "    \"\"\"Process multiple PDFs in parallel using multiprocessing\"\"\"\n",
    "    \n",
    "    if n_workers is None:\n",
    "        n_workers = N_WORKERS\n",
    "    \n",
    "    print(f\"\\\\nProcessing {len(pdf_files)} PDFs using {n_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create pool with initializer\n",
    "    with Pool(n_workers, initializer=init_worker, initargs=(model_type,)) as pool:\n",
    "        # Process with progress bar\n",
    "        if show_progress:\n",
    "            results = list(tqdm(\n",
    "                pool.imap(process_pdf_parallel, pdf_files),\n",
    "                total=len(pdf_files),\n",
    "                desc=\"Processing PDFs\"\n",
    "            ))\n",
    "        else:\n",
    "            results = pool.map(process_pdf_parallel, pdf_files)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Processed {len(pdf_files)} PDFs in {elapsed:.1f} seconds\")\n",
    "    print(f\"Average: {elapsed/len(pdf_files):.2f} seconds per PDF\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def classify_pdf_batch_parallel(pdf_paths: List[Path],\n",
    "                               reference_embeddings: np.ndarray,\n",
    "                               reference_prototype: np.ndarray,\n",
    "                               model_type: str,\n",
    "                               threshold: float = SIMILARITY_THRESHOLD,\n",
    "                               use_prototype: bool = True,\n",
    "                               n_workers: int = None) -> List[Dict]:\n",
    "    \"\"\"Classify a batch of PDFs in parallel\"\"\"\n",
    "    \n",
    "    # First, extract embeddings in parallel\n",
    "    embedding_results = process_pdfs_parallel(pdf_paths, model_type, n_workers)\n",
    "    \n",
    "    # Then classify based on embeddings\n",
    "    classification_results = []\n",
    "    \n",
    "    for result in embedding_results:\n",
    "        if result['error']:\n",
    "            classification_results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_form': False,\n",
    "                'form_pages': [],\n",
    "                'page_scores': [],\n",
    "                'max_similarity': 0.0,\n",
    "                'error': result['error']\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Classify each page\n",
    "        form_pages = []\n",
    "        page_scores = []\n",
    "        max_similarity = 0.0\n",
    "        \n",
    "        for embedding, page_num in zip(result['embeddings'], result['page_numbers']):\n",
    "            # Compute similarity\n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            page_scores.append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                form_pages.append(page_num)\n",
    "            \n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        classification_results.append({\n",
    "            'filename': result['filename'],\n",
    "            'contains_form': len(form_pages) > 0,\n",
    "            'form_pages': form_pages,\n",
    "            'page_scores': page_scores,\n",
    "            'max_similarity': max_similarity,\n",
    "            'total_pages': result.get('num_pages', len(result['embeddings'])),\n",
    "            'error': None\n",
    "        })\n",
    "    \n",
    "    return classification_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if cached embeddings exist\n",
    "cache_file = EMBEDDINGS_CACHE_PATH / f\"{MODEL_TYPE}_reference_embeddings.pkl\"\n",
    "\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading cached reference embeddings from {cache_file}\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "        reference_embeddings = cache_data['embeddings']\n",
    "        reference_metadata = cache_data['metadata']\n",
    "else:\n",
    "    print(\"Building reference embeddings from example forms...\")\n",
    "    pdf_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "    print(f\"Processing {len(pdf_files)} example form PDFs\")\n",
    "    \n",
    "    if USE_MULTIPROCESSING and len(pdf_files) > 10:\n",
    "        # Use parallel processing for many files\n",
    "        print(\"Using parallel processing for reference embeddings...\")\n",
    "        \n",
    "        # Process PDFs in parallel\n",
    "        results = process_pdfs_parallel(pdf_files, MODEL_TYPE, N_WORKERS)\n",
    "        \n",
    "        # Collect embeddings and metadata\n",
    "        reference_embeddings = []\n",
    "        reference_metadata = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result['error']:\n",
    "                print(f\"Skipping {result['filename']} due to error: {result['error']}\")\n",
    "                continue\n",
    "            \n",
    "            for embedding, page_num in zip(result['embeddings'], result['page_numbers']):\n",
    "                reference_embeddings.append(embedding)\n",
    "                reference_metadata.append({\n",
    "                    'file_path': result['path'],\n",
    "                    'filename': result['filename'],\n",
    "                    'page_num': page_num,\n",
    "                    'total_pages': result['num_pages']\n",
    "                })\n",
    "    else:\n",
    "        # Use sequential processing for small datasets\n",
    "        reference_embeddings = []\n",
    "        reference_metadata = []\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing example forms\"):\n",
    "            images = pdf_to_images(pdf_path)\n",
    "            \n",
    "            if images:\n",
    "                # Extract embeddings for all pages\n",
    "                embeddings = extract_embeddings(images, model, processor, MODEL_TYPE)\n",
    "                \n",
    "                # Store embeddings and metadata\n",
    "                for i, embedding in enumerate(embeddings):\n",
    "                    reference_embeddings.append(embedding)\n",
    "                    reference_metadata.append({\n",
    "                        'file_path': str(pdf_path),\n",
    "                        'filename': pdf_path.name,\n",
    "                        'page_num': i + 1,\n",
    "                        'total_pages': len(images)\n",
    "                    })\n",
    "    \n",
    "    reference_embeddings = np.array(reference_embeddings)\n",
    "    \n",
    "    # Cache the embeddings\n",
    "    print(f\"Caching reference embeddings to {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': reference_embeddings,\n",
    "            'metadata': reference_metadata,\n",
    "            'model_type': MODEL_TYPE\n",
    "        }, f)\n",
    "\n",
    "print(f\"\\nReference set contains {len(reference_embeddings)} form pages\")\n",
    "print(f\"Embedding dimension: {reference_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: Path, dpi: int = IMAGE_DPI) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF to list of PIL Images\"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image(image: Image.Image, model_type: str) -> Image.Image:\n",
    "    \"\"\"Preprocess image based on model requirements\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    if model_type == \"donut\":\n",
    "        # Donut expects larger images\n",
    "        image.thumbnail((1280, 960), Image.Resampling.LANCZOS)\n",
    "    else:\n",
    "        # CLIP and DINOv2 work with smaller images\n",
    "        image.thumbnail((224, 224), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings(images: List[Image.Image], model, processor, model_type: str, \n",
    "                      batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings for a list of images\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        \n",
    "        # Preprocess images\n",
    "        batch_images = [preprocess_image(img, model_type) for img in batch_images]\n",
    "        \n",
    "        # Process batch\n",
    "        if model_type in [\"donut\", \"clip\"]:\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        else:  # dinov2\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract embeddings based on model type\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            batch_embeddings = outputs.pooler_output\n",
    "        elif hasattr(outputs, 'last_hidden_state'):\n",
    "            # Mean pool the sequence dimension\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            batch_embeddings = outputs[0].mean(dim=1)  # Fallback\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embedding_single(image: Image.Image, model, processor, model_type: str) -> np.ndarray:\n",
    "    \"\"\"Extract embedding for a single image\"\"\"\n",
    "    image = preprocess_image(image, model_type)\n",
    "    \n",
    "    if model_type in [\"donut\", \"clip\"]:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    else:  # dinov2\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embedding\n",
    "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "        embedding = outputs.pooler_output\n",
    "    elif hasattr(outputs, 'last_hidden_state'):\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    else:\n",
    "        embedding = outputs[0].mean(dim=1)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def process_document_folder(folder_path: Path,\n",
    "                          model,\n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float,\n",
    "                          output_file: str = 'zero_shot_results.csv',\n",
    "                          max_files: Optional[int] = None,\n",
    "                          use_prototype: bool = True,\n",
    "                          use_parallel: bool = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all PDFs in a folder using zero-shot classification\n",
    "    \n",
    "    Args:\n",
    "        use_parallel: Use parallel processing (defaults to USE_MULTIPROCESSING setting)\n",
    "    \"\"\"\n",
    "    if use_parallel is None:\n",
    "        use_parallel = USE_MULTIPROCESSING\n",
    "    \n",
    "    results = []\n",
    "    pdf_files = list(folder_path.glob('*.pdf'))\n",
    "    \n",
    "    if max_files:\n",
    "        pdf_files = pdf_files[:max_files]\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    \n",
    "    if use_parallel and len(pdf_files) > 10:\n",
    "        # Use parallel processing\n",
    "        print(f\"Using parallel processing with {N_WORKERS} workers...\")\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = min(MAX_PDFS_PER_WORKER * N_WORKERS, len(pdf_files))\n",
    "        \n",
    "        for i in range(0, len(pdf_files), batch_size):\n",
    "            batch_files = pdf_files[i:i+batch_size]\n",
    "            print(f\"\\nProcessing batch {i//batch_size + 1}/{(len(pdf_files) + batch_size - 1)//batch_size}\")\n",
    "            \n",
    "            batch_results = classify_pdf_batch_parallel(\n",
    "                batch_files,\n",
    "                reference_embeddings,\n",
    "                reference_prototype,\n",
    "                model_type,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype,\n",
    "                n_workers=N_WORKERS\n",
    "            )\n",
    "            \n",
    "            # Flatten results for CSV\n",
    "            for result in batch_results:\n",
    "                results.append({\n",
    "                    'filename': result['filename'],\n",
    "                    'contains_form': result['contains_form'],\n",
    "                    'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "                    'num_form_pages': len(result['form_pages']),\n",
    "                    'total_pages': result.get('total_pages', 0),\n",
    "                    'max_similarity': result['max_similarity'],\n",
    "                    'error': result['error']\n",
    "                })\n",
    "    else:\n",
    "        # Use sequential processing\n",
    "        print(\"Using sequential processing...\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files):\n",
    "            result = classify_pdf_zero_shot(\n",
    "                pdf_path, model, processor, model_type,\n",
    "                reference_embeddings, reference_prototype,\n",
    "                threshold=threshold,\n",
    "                use_prototype=use_prototype\n",
    "            )\n",
    "            \n",
    "            # Flatten results for CSV\n",
    "            results.append({\n",
    "                'filename': result['filename'],\n",
    "                'contains_form': result['contains_form'],\n",
    "                'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "                'num_form_pages': len(result['form_pages']),\n",
    "                'total_pages': result.get('total_pages', 0),\n",
    "                'max_similarity': result['max_similarity'],\n",
    "                'error': result['error']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    print(f\"Documents with forms: {df_results['contains_form'].sum()}\")\n",
    "    print(f\"Documents without forms: {(~df_results['contains_form']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['error'].notna().sum()}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached reference embeddings from /Users/admin-tascott/Documents/GitHub/chehalis/code/preprocessing/cached_embeddings/clip_reference_embeddings.pkl\n",
      "\n",
      "Reference set contains 111 form pages\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Check if cached embeddings exist\n",
    "cache_file = EMBEDDINGS_CACHE_PATH / f\"{MODEL_TYPE}_reference_embeddings.pkl\"\n",
    "\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading cached reference embeddings from {cache_file}\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "        reference_embeddings = cache_data['embeddings']\n",
    "        reference_metadata = cache_data['metadata']\n",
    "else:\n",
    "    print(\"Building reference embeddings from example forms...\")\n",
    "    reference_embeddings = []\n",
    "    reference_metadata = []\n",
    "    \n",
    "    pdf_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "    print(f\"Processing {len(pdf_files)} example form PDFs\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing example forms\"):\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        \n",
    "        if images:\n",
    "            # Extract embeddings for all pages\n",
    "            embeddings = extract_embeddings(images, model, processor, MODEL_TYPE)\n",
    "            \n",
    "            # Store embeddings and metadata\n",
    "            for i, embedding in enumerate(embeddings):\n",
    "                reference_embeddings.append(embedding)\n",
    "                reference_metadata.append({\n",
    "                    'file_path': str(pdf_path),\n",
    "                    'filename': pdf_path.name,\n",
    "                    'page_num': i + 1,\n",
    "                    'total_pages': len(images)\n",
    "                })\n",
    "    \n",
    "    reference_embeddings = np.array(reference_embeddings)\n",
    "    \n",
    "    # Cache the embeddings\n",
    "    print(f\"Caching reference embeddings to {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': reference_embeddings,\n",
    "            'metadata': reference_metadata,\n",
    "            'model_type': MODEL_TYPE\n",
    "        }, f)\n",
    "\n",
    "print(f\"\\nReference set contains {len(reference_embeddings)} form pages\")\n",
    "print(f\"Embedding dimension: {reference_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Similarity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing reference representation...\n",
      "Reference prototype shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Compute reference representation\n",
    "print(\"Computing reference representation...\")\n",
    "\n",
    "# Option 1: Mean embedding (prototype)\n",
    "reference_prototype = reference_embeddings.mean(axis=0)\n",
    "print(f\"Reference prototype shape: {reference_prototype.shape}\")\n",
    "\n",
    "# Option 2: Keep all embeddings for k-NN style matching\n",
    "# This is more flexible but slower\n",
    "\n",
    "def compute_similarity_to_prototype(embedding: np.ndarray, prototype: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity to prototype\"\"\"\n",
    "    return cosine_similarity(embedding.reshape(1, -1), prototype.reshape(1, -1))[0, 0]\n",
    "\n",
    "def compute_similarity_to_references(embedding: np.ndarray, references: np.ndarray, \n",
    "                                   method: str = 'max') -> float:\n",
    "    \"\"\"Compute similarity to reference set\n",
    "    \n",
    "    Args:\n",
    "        embedding: Single embedding to compare\n",
    "        references: Array of reference embeddings\n",
    "        method: 'max', 'mean', or 'top_k'\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity(embedding.reshape(1, -1), references)[0]\n",
    "    \n",
    "    if method == 'max':\n",
    "        return similarities.max()\n",
    "    elif method == 'mean':\n",
    "        return similarities.mean()\n",
    "    elif method == 'top_k':\n",
    "        # Average of top 5 most similar\n",
    "        k = min(5, len(similarities))\n",
    "        return np.sort(similarities)[-k:].mean()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance: Sequential vs Parallel processing\n",
    "print(\"Performance Comparison: Sequential vs Parallel Processing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test on a small subset\n",
    "test_path = NON_EXAMPLES_PATH\n",
    "test_files = list(test_path.glob('*.pdf'))[:20]  # Test with 20 files\n",
    "\n",
    "if len(test_files) >= 10:\n",
    "    # Sequential processing\n",
    "    print(\"\\n1. Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    seq_results = process_document_folder(\n",
    "        test_path,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file='sequential_test.csv',\n",
    "        max_files=20,\n",
    "        use_prototype=False,\n",
    "        use_parallel=False\n",
    "    )\n",
    "    seq_time = time.time() - start_time\n",
    "    print(f\"Sequential time: {seq_time:.1f} seconds\")\n",
    "    print(f\"Speed: {len(test_files)/seq_time:.1f} PDFs/second\")\n",
    "    \n",
    "    # Parallel processing\n",
    "    print(\"\\n2. Parallel Processing:\")\n",
    "    start_time = time.time()\n",
    "    par_results = process_document_folder(\n",
    "        test_path,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file='parallel_test.csv',\n",
    "        max_files=20,\n",
    "        use_prototype=False,\n",
    "        use_parallel=True\n",
    "    )\n",
    "    par_time = time.time() - start_time\n",
    "    print(f\"Parallel time: {par_time:.1f} seconds\")\n",
    "    print(f\"Speed: {len(test_files)/par_time:.1f} PDFs/second\")\n",
    "    \n",
    "    # Speedup\n",
    "    speedup = seq_time / par_time\n",
    "    print(f\"\\nSpeedup: {speedup:.1f}x faster with {N_WORKERS} workers\")\n",
    "    \n",
    "    # Efficiency\n",
    "    efficiency = speedup / N_WORKERS * 100\n",
    "    print(f\"Parallel efficiency: {efficiency:.0f}%\")\n",
    "    \n",
    "    # Estimate for full corpus\n",
    "    print(f\"\\nEstimated time for 190k documents:\")\n",
    "    print(f\"Sequential: {190000 * seq_time / len(test_files) / 3600:.1f} hours\")\n",
    "    print(f\"Parallel: {190000 * par_time / len(test_files) / 3600:.1f} hours\")\n",
    "else:\n",
    "    print(\"Not enough test files for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 Performance Comparison: Sequential vs Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zero-Shot Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pdf_zero_shot(pdf_path: Path, \n",
    "                          model, \n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float = SIMILARITY_THRESHOLD,\n",
    "                          use_prototype: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Classify pages in a PDF using zero-shot similarity matching\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'filename': pdf_path.name,\n",
    "        'contains_form': False,\n",
    "        'form_pages': [],\n",
    "        'page_scores': [],\n",
    "        'max_similarity': 0.0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        images = pdf_to_images(pdf_path)\n",
    "        if not images:\n",
    "            results['error'] = \"Failed to convert PDF\"\n",
    "            return results\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            # Extract embedding\n",
    "            embedding = extract_embedding_single(image, model, processor, model_type)\n",
    "            \n",
    "            # Compute similarity\n",
    "            if use_prototype:\n",
    "                similarity = compute_similarity_to_prototype(embedding, reference_prototype)\n",
    "            else:\n",
    "                similarity = compute_similarity_to_references(embedding, reference_embeddings, method='max')\n",
    "            \n",
    "            results['page_scores'].append({\n",
    "                'page': page_num,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "            \n",
    "            # Check if page is a form\n",
    "            if similarity >= threshold:\n",
    "                results['form_pages'].append(page_num)\n",
    "                results['contains_form'] = True\n",
    "            \n",
    "            results['max_similarity'] = max(results['max_similarity'], similarity)\n",
    "        \n",
    "        results['total_pages'] = len(images)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on known examples to tune threshold...\n",
      "\n",
      "Computing self-similarity for positive examples...\n",
      "\n",
      "Testing on non-examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing non-examples: 100%|██████████████████| 20/20 [02:09<00:00,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive scores - Mean: 0.982, Std: 0.014\n",
      "Positive scores - Min: 0.953, Max: 1.000\n",
      "\n",
      "Negative scores - Mean: 0.645, Std: 0.059\n",
      "Negative scores - Min: 0.355, Max: 0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on known examples and non-examples to find optimal threshold\n",
    "print(\"Testing on known examples to tune threshold...\")\n",
    "\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "\n",
    "# Test on subset of examples (already processed)\n",
    "print(\"\\nComputing self-similarity for positive examples...\")\n",
    "for i in range(min(50, len(reference_embeddings))):\n",
    "    # Leave-one-out similarity\n",
    "    embedding = reference_embeddings[i]\n",
    "    other_embeddings = np.delete(reference_embeddings, i, axis=0)\n",
    "    similarity = compute_similarity_to_references(embedding, other_embeddings, method='max')\n",
    "    positive_scores.append(similarity)\n",
    "\n",
    "# Test on non-examples\n",
    "print(\"\\nTesting on non-examples...\")\n",
    "non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))[:20]  # Test subset\n",
    "\n",
    "for pdf_path in tqdm(non_example_files, desc=\"Processing non-examples\"):\n",
    "    result = classify_pdf_zero_shot(\n",
    "        pdf_path, model, processor, MODEL_TYPE,\n",
    "        reference_embeddings, reference_prototype,\n",
    "        threshold=0.0,  # Set to 0 to get all scores\n",
    "        use_prototype=False\n",
    "    )\n",
    "    \n",
    "    if result['page_scores']:\n",
    "        for page_score in result['page_scores']:\n",
    "            negative_scores.append(page_score['similarity'])\n",
    "\n",
    "# Analyze scores\n",
    "positive_scores = np.array(positive_scores)\n",
    "negative_scores = np.array(negative_scores)\n",
    "\n",
    "print(f\"\\nPositive scores - Mean: {positive_scores.mean():.3f}, Std: {positive_scores.std():.3f}\")\n",
    "print(f\"Positive scores - Min: {positive_scores.min():.3f}, Max: {positive_scores.max():.3f}\")\n",
    "print(f\"\\nNegative scores - Mean: {negative_scores.mean():.3f}, Std: {negative_scores.std():.3f}\")\n",
    "print(f\"Negative scores - Min: {negative_scores.min():.3f}, Max: {negative_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggested thresholds:\n",
      "Gap-based threshold: 0.908\n",
      "99th percentile threshold: 0.805\n",
      "\n",
      "Selected optimal threshold: 0.908\n"
     ]
    }
   ],
   "source": [
    "# Plot score distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(positive_scores, bins=30, alpha=0.7, label='Form pages', color='green')\n",
    "plt.hist(negative_scores, bins=30, alpha=0.7, label='Non-form pages', color='red')\n",
    "plt.axvline(x=SIMILARITY_THRESHOLD, color='black', linestyle='--', \n",
    "            label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
    "plt.xlabel('Cosine Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold\n",
    "# Simple approach: use value that separates distributions\n",
    "gap_threshold = (positive_scores.min() + negative_scores.max()) / 2\n",
    "percentile_threshold = np.percentile(negative_scores, 99)  # 99th percentile of negatives\n",
    "\n",
    "print(f\"\\nSuggested thresholds:\")\n",
    "print(f\"Gap-based threshold: {gap_threshold:.3f}\")\n",
    "print(f\"99th percentile threshold: {percentile_threshold:.3f}\")\n",
    "\n",
    "# Update threshold\n",
    "OPTIMAL_THRESHOLD = max(gap_threshold, percentile_threshold)\n",
    "print(f\"\\nSelected optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_folder(folder_path: Path,\n",
    "                          model,\n",
    "                          processor,\n",
    "                          model_type: str,\n",
    "                          reference_embeddings: np.ndarray,\n",
    "                          reference_prototype: np.ndarray,\n",
    "                          threshold: float,\n",
    "                          output_file: str = 'zero_shot_results.csv',\n",
    "                          max_files: Optional[int] = None,\n",
    "                          use_prototype: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all PDFs in a folder using zero-shot classification\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    pdf_files = list(folder_path.glob('*.pdf'))\n",
    "    \n",
    "    if max_files:\n",
    "        pdf_files = pdf_files[:max_files]\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files):\n",
    "        result = classify_pdf_zero_shot(\n",
    "            pdf_path, model, processor, model_type,\n",
    "            reference_embeddings, reference_prototype,\n",
    "            threshold=threshold,\n",
    "            use_prototype=use_prototype\n",
    "        )\n",
    "        \n",
    "        # Flatten results for CSV\n",
    "        results.append({\n",
    "            'filename': result['filename'],\n",
    "            'contains_form': result['contains_form'],\n",
    "            'form_pages': ','.join(map(str, result['form_pages'])),\n",
    "            'num_form_pages': len(result['form_pages']),\n",
    "            'total_pages': result.get('total_pages', 0),\n",
    "            'max_similarity': result['max_similarity'],\n",
    "            'error': result['error']\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    print(f\"Documents with forms: {df_results['contains_form'].sum()}\")\n",
    "    print(f\"Documents without forms: {(~df_results['contains_form']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['error'].notna().sum()}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sample documents...\n",
      "Processing 50 PDF files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [04:57<00:00,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to zero_shot_test_results.csv\n",
      "Total documents processed: 50\n",
      "Documents with forms: 0\n",
      "Documents without forms: 50\n",
      "Processing errors: 0\n",
      "\n",
      "Sample results:\n",
      "                            filename  contains_form form_pages  \\\n",
      "0                      25581-000.pdf          False              \n",
      "1  0000000000000000000062223-001.pdf          False              \n",
      "2                       1197-000.pdf          False              \n",
      "3                      67419-000.pdf          False              \n",
      "4                     104473-000.pdf          False              \n",
      "5        F1-10-FSSA-DDRS-495-000.pdf          False              \n",
      "6         F1-8-FSSA-DMHA-564-003.pdf          False              \n",
      "7                     104303-001.pdf          False              \n",
      "8          F1-9-DCS-43810SAS-001.pdf          False              \n",
      "9            F1-9-DCS-UNION2-000.pdf          False              \n",
      "\n",
      "   num_form_pages  total_pages  max_similarity error  \n",
      "0               0           25        0.782379  None  \n",
      "1               0            5        0.745903  None  \n",
      "2               0           15        0.858441  None  \n",
      "3               0           26        0.842313  None  \n",
      "4               0           42        0.779016  None  \n",
      "5               0            3        0.718957  None  \n",
      "6               0            3        0.693395  None  \n",
      "7               0            3        0.659438  None  \n",
      "8               0            5        0.748454  None  \n",
      "9               0           22        0.718677  None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on a sample first\n",
    "print(\"Testing on sample documents...\")\n",
    "\n",
    "# Create a test folder with mixed examples\n",
    "test_results = process_document_folder(\n",
    "    NON_EXAMPLES_PATH,\n",
    "    model,\n",
    "    processor,\n",
    "    MODEL_TYPE,\n",
    "    reference_embeddings,\n",
    "    reference_prototype,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='zero_shot_test_results.csv',\n",
    "    max_files=50,\n",
    "    use_prototype=False  # Use full reference set for better accuracy\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "print(test_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Process Full Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 193450 documents in 194 batches...\n",
      "Processing 1000 PDF files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                      | 23/1000 [02:08<1:30:48,  5.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     (batch_folder \u001b[38;5;241m/\u001b[39m pdf\u001b[38;5;241m.\u001b[39mname)\u001b[38;5;241m.\u001b[39msymlink_to(pdf)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Process batch\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m process_document_folder(\n\u001b[1;32m     28\u001b[0m     batch_folder,\n\u001b[1;32m     29\u001b[0m     model,\n\u001b[1;32m     30\u001b[0m     processor,\n\u001b[1;32m     31\u001b[0m     MODEL_TYPE,\n\u001b[1;32m     32\u001b[0m     reference_embeddings,\n\u001b[1;32m     33\u001b[0m     reference_prototype,\n\u001b[1;32m     34\u001b[0m     threshold\u001b[38;5;241m=\u001b[39mOPTIMAL_THRESHOLD,\n\u001b[1;32m     35\u001b[0m     output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero_shot_results_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     36\u001b[0m     use_prototype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m batch_folder\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[57], line 23\u001b[0m, in \u001b[0;36mprocess_document_folder\u001b[0;34m(folder_path, model, processor, model_type, reference_embeddings, reference_prototype, threshold, output_file, max_files, use_prototype)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pdf_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m PDF files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf_path \u001b[38;5;129;01min\u001b[39;00m tqdm(pdf_files):\n\u001b[0;32m---> 23\u001b[0m     result \u001b[38;5;241m=\u001b[39m classify_pdf_zero_shot(\n\u001b[1;32m     24\u001b[0m         pdf_path, model, processor, model_type,\n\u001b[1;32m     25\u001b[0m         reference_embeddings, reference_prototype,\n\u001b[1;32m     26\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[1;32m     27\u001b[0m         use_prototype\u001b[38;5;241m=\u001b[39muse_prototype\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Flatten results for CSV\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontains_form\u001b[39m\u001b[38;5;124m'\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontains_form\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     39\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[43], line 34\u001b[0m, in \u001b[0;36mclassify_pdf_zero_shot\u001b[0;34m(pdf_path, model, processor, model_type, reference_embeddings, reference_prototype, threshold, use_prototype)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Process each page\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_num, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Extract embedding\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m extract_embedding_single(image, model, processor, model_type)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Compute similarity\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_prototype:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[37], line 41\u001b[0m, in \u001b[0;36mextract_embedding_single\u001b[0;34m(image, model, processor, model_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdonut\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     40\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 41\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# dinov2\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:757\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m    755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 757\u001b[0m encoder_outputs: BaseModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    758\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    759\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    760\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    761\u001b[0m )\n\u001b[1;32m    763\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    764\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:557\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    556\u001b[0m     encoder_states \u001b[38;5;241m=\u001b[39m encoder_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 557\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m encoder_layer(\n\u001b[1;32m    558\u001b[0m     hidden_states,\n\u001b[1;32m    559\u001b[0m     attention_mask,\n\u001b[1;32m    560\u001b[0m     causal_attention_mask,\n\u001b[1;32m    561\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    562\u001b[0m )\n\u001b[1;32m    564\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:404\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    403\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 404\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    405\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    406\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    407\u001b[0m     causal_attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[1;32m    408\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    409\u001b[0m )\n\u001b[1;32m    410\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    412\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example code for processing the full 190k corpus\n",
    "# Uncomment and modify the path as needed\n",
    "\n",
    "\n",
    "CORPUS_PATH = BASE_PATH / 'data' / 'raw' / '_contracts'  # Adjust path\n",
    "\n",
    "# Process in batches to save memory and allow interruption\n",
    "batch_size = 1000\n",
    "all_pdfs = list(CORPUS_PATH.glob('*.pdf'))\n",
    "total_batches = (len(all_pdfs) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Processing {len(all_pdfs)} documents in {total_batches} batches...\")\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(all_pdfs))\n",
    "    \n",
    "    batch_pdfs = all_pdfs[start_idx:end_idx]\n",
    "    batch_folder = Path('/tmp/batch_pdfs')  # Temporary folder\n",
    "    batch_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create symlinks for batch\n",
    "    for pdf in batch_pdfs:\n",
    "        (batch_folder / pdf.name).symlink_to(pdf)\n",
    "    \n",
    "    # Process batch\n",
    "    batch_results = process_document_folder(\n",
    "        batch_folder,\n",
    "        model,\n",
    "        processor,\n",
    "        MODEL_TYPE,\n",
    "        reference_embeddings,\n",
    "        reference_prototype,\n",
    "        threshold=OPTIMAL_THRESHOLD,\n",
    "        output_file=f'zero_shot_results_batch_{batch_idx}.csv',\n",
    "        use_prototype=False\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    for link in batch_folder.glob('*.pdf'):\n",
    "        link.unlink()\n",
    "    \n",
    "    print(f\"Completed batch {batch_idx + 1}/{total_batches}\")\n",
    "\n",
    "# Combine all batch results\n",
    "all_results = []\n",
    "for batch_idx in range(total_batches):\n",
    "    batch_df = pd.read_csv(f'zero_shot_results_batch_{batch_idx}.csv')\n",
    "    all_results.append(batch_df)\n",
    "\n",
    "final_results = pd.concat(all_results, ignore_index=True)\n",
    "final_results.to_csv('zero_shot_results_complete.csv', index=False)\n",
    "print(f\"\\nComplete results saved to zero_shot_results_complete.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with trained model performance (if available)\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"Model type: {MODEL_TYPE}\")\n",
    "print(f\"Reference examples: {len(reference_embeddings)}\")\n",
    "print(f\"Optimal threshold: {OPTIMAL_THRESHOLD:.3f}\")\n",
    "print(f\"\\nProcessing speed: ~{len(test_results) / 60:.1f} documents per minute\")\n",
    "\n",
    "# Estimate time for full corpus\n",
    "docs_per_minute = len(test_results) / 60  # Rough estimate\n",
    "total_minutes = 190000 / docs_per_minute\n",
    "print(f\"\\nEstimated time for 190k documents: {total_minutes/60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### For production use:\n",
    "\n",
    "1. **Model Selection**: Try different models (CLIP, DINOv2) to see which works best\n",
    "2. **Threshold Tuning**: Use more examples/non-examples to fine-tune threshold\n",
    "3. **Optimization**: \n",
    "   - Use GPU for faster processing\n",
    "   - Process in parallel with multiprocessing\n",
    "   - Cache embeddings for documents you process repeatedly\n",
    "\n",
    "### Advantages over training:\n",
    "- **Immediate deployment** - No training time\n",
    "- **Easy updates** - Just add new reference examples\n",
    "- **Interpretable** - Can inspect which references match\n",
    "- **Flexible threshold** - Adjust precision/recall trade-off easily\n",
    "\n",
    "### When to use training instead:\n",
    "- If zero-shot accuracy is insufficient\n",
    "- If you need to learn subtle patterns\n",
    "- If you have many labeled examples\n",
    "- If inference speed is critical (trained models can be faster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
