{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administrative Form Detection using Donut\n",
    "\n",
    "This notebook implements a classifier to detect standardized administrative forms in PDF documents using the Donut transformer model.\n",
    "\n",
    "## Objectives:\n",
    "1. Detect if a PDF document contains a specific standardized administrative form\n",
    "2. Identify which page number the form appears on\n",
    "\n",
    "## Approach:\n",
    "- Use Donut (Document Understanding Transformer) for OCR-free document classification\n",
    "- Process PDFs page by page to identify the administrative form\n",
    "\n",
    "## Future Options:\n",
    "- Consider LayoutLMv3 for combining visual and text features if higher accuracy is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch torchvision pdf2image pillow numpy pandas tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "from transformers import (\n",
    "    DonutProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Check if paths exist\n",
    "print(f\"Example forms path exists: {EXAMPLE_FORMS_PATH.exists()}\")\n",
    "print(f\"Non-examples path exists: {NON_EXAMPLES_PATH.exists()}\")\n",
    "\n",
    "# List example files\n",
    "if EXAMPLE_FORMS_PATH.exists():\n",
    "    example_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "    print(f\"\\nFound {len(example_files)} example form files\")\n",
    "    for f in example_files[:5]:  # Show first 5\n",
    "        print(f\"  - {f.name}\")\n",
    "\n",
    "# List non-example files\n",
    "if NON_EXAMPLES_PATH.exists():\n",
    "    non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))\n",
    "    print(f\"\\nFound {len(non_example_files)} non-example files\")\n",
    "    for f in non_example_files[:5]:  # Show first 5\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: Path, dpi: int = 200) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Convert PDF to list of PIL Images (one per page)\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        dpi: Resolution for conversion\n",
    "    \n",
    "    Returns:\n",
    "        List of PIL Images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image_for_donut(image: Image.Image, size: Tuple[int, int] = (1280, 960)) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Preprocess image for Donut model\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        size: Target size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed PIL Image\n",
    "    \"\"\"\n",
    "    # Convert to RGB if necessary\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Resize while maintaining aspect ratio\n",
    "    image.thumbnail(size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for administrative form classification\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list: List[Dict], processor):\n",
    "        self.data = data_list\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        \n",
    "        # Process image\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values.squeeze(),\n",
    "            'labels': label,\n",
    "            'metadata': {\n",
    "                'file_path': item.get('file_path', ''),\n",
    "                'page_num': item.get('page_num', -1)\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_pdfs(example_path: Path, non_example_path: Optional[Path] = None, \n",
    "                           max_samples_per_class: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create dataset from PDF files\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with 'image', 'label', 'file_path', 'page_num'\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Process positive examples (forms)\n",
    "    if example_path.exists():\n",
    "        pdf_files = list(example_path.glob('*.pdf'))[:max_samples_per_class]\n",
    "        print(f\"Processing {len(pdf_files)} positive example PDFs...\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Positive examples\"):\n",
    "            images = pdf_to_images(pdf_path)\n",
    "            for page_num, image in enumerate(images):\n",
    "                dataset.append({\n",
    "                    'image': preprocess_image_for_donut(image),\n",
    "                    'label': 1,  # 1 for administrative form\n",
    "                    'file_path': str(pdf_path),\n",
    "                    'page_num': page_num + 1\n",
    "                })\n",
    "    \n",
    "    # Process negative examples (non-forms)\n",
    "    if non_example_path and non_example_path.exists():\n",
    "        pdf_files = list(non_example_path.glob('*.pdf'))[:max_samples_per_class]\n",
    "        print(f\"\\nProcessing {len(pdf_files)} negative example PDFs...\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Negative examples\"):\n",
    "            images = pdf_to_images(pdf_path)\n",
    "            for page_num, image in enumerate(images):\n",
    "                dataset.append({\n",
    "                    'image': preprocess_image_for_donut(image),\n",
    "                    'label': 0,  # 0 for non-form\n",
    "                    'file_path': str(pdf_path),\n",
    "                    'page_num': page_num + 1\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nTotal dataset size: {len(dataset)} pages\")\n",
    "    print(f\"Positive examples (forms): {sum(1 for d in dataset if d['label'] == 1)}\")\n",
    "    print(f\"Negative examples (non-forms): {sum(1 for d in dataset if d['label'] == 0)}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use pre-trained Donut for document classification\n",
    "# We'll fine-tune it for binary classification (form vs non-form)\n",
    "\n",
    "MODEL_NAME = \"naver-clova-ix/donut-base\"\n",
    "\n",
    "# Load processor and model\n",
    "processor = DonutProcessor.from_pretrained(MODEL_NAME)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for classification task\n",
    "# Donut uses a decoder, so we'll set up special tokens for our classification task\n",
    "\n",
    "# Add special tokens for our task\n",
    "processor.tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<admin_form>\",\n",
    "        \"<not_admin_form>\",\n",
    "        \"<classification>\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Resize model embeddings to accommodate new tokens\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Set up decoder start token\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([\"<classification>\"])[0]\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels_for_training(batch, processor):\n",
    "    \"\"\"\n",
    "    Prepare decoder labels for training\n",
    "    \"\"\"\n",
    "    labels_list = []\n",
    "    for label in batch['labels']:\n",
    "        if label == 1:\n",
    "            text = \"<classification> <admin_form>\"\n",
    "        else:\n",
    "            text = \"<classification> <not_admin_form>\"\n",
    "        \n",
    "        # Tokenize the label\n",
    "        encoding = processor.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=10,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels_list.append(encoding.input_ids.squeeze())\n",
    "    \n",
    "    return torch.stack(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, processor, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = prepare_labels_for_training(batch, processor).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, processor, device):\n",
    "    \"\"\"\n",
    "    Evaluate model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            true_labels.extend(batch['labels'].numpy())\n",
    "            \n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                pixel_values,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "                max_length=10,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                num_beams=1,\n",
    "                bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "            \n",
    "            # Decode predictions\n",
    "            prediction_text = processor.batch_decode(outputs.sequences)\n",
    "            \n",
    "            # Convert text predictions to binary labels\n",
    "            for pred_text in prediction_text:\n",
    "                if \"<admin_form>\" in pred_text:\n",
    "                    predictions.append(1)\n",
    "                else:\n",
    "                    predictions.append(0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "# Limit samples for initial testing\n",
    "dataset_list = create_dataset_from_pdfs(\n",
    "    EXAMPLE_FORMS_PATH,\n",
    "    NON_EXAMPLES_PATH,\n",
    "    max_samples_per_class=10  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# Split into train and validation\n",
    "train_data, val_data = train_test_split(dataset_list, test_size=0.2, random_state=42, stratify=[d['label'] for d in dataset_list])\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = FormDataset(train_data, processor)\n",
    "val_dataset = FormDataset(val_data, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 5\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, processor, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, processor, device)\n",
    "    print(f\"Validation accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Validation recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"Validation F1: {val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['accuracy'] > best_val_accuracy:\n",
    "        best_val_accuracy = val_metrics['accuracy']\n",
    "        torch.save(model.state_dict(), 'best_form_classifier.pth')\n",
    "        print(\"Saved best model!\")\n",
    "    \n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_accuracy': val_metrics['accuracy'],\n",
    "        'val_precision': val_metrics['precision'],\n",
    "        'val_recall': val_metrics['recall'],\n",
    "        'val_f1': val_metrics['f1']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_form_in_pdf(pdf_path: Path, model, processor, device, batch_size: int = 8) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect if a PDF contains the administrative form and on which page(s)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'contains_form': boolean\n",
    "        - 'form_pages': list of page numbers where form is detected\n",
    "        - 'confidence_scores': confidence score for each page\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    if not images:\n",
    "        return {'contains_form': False, 'form_pages': [], 'confidence_scores': []}\n",
    "    \n",
    "    form_pages = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        batch_processed = [preprocess_image_for_donut(img) for img in batch_images]\n",
    "        \n",
    "        # Process images\n",
    "        pixel_values = torch.stack([\n",
    "            processor(img, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "            for img in batch_processed\n",
    "        ]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                pixel_values,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "                max_length=10,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                num_beams=1,\n",
    "                bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "            \n",
    "            # Decode predictions\n",
    "            predictions = processor.batch_decode(outputs.sequences)\n",
    "            \n",
    "            # Process predictions\n",
    "            for j, pred_text in enumerate(predictions):\n",
    "                page_num = i + j + 1\n",
    "                \n",
    "                if \"<admin_form>\" in pred_text:\n",
    "                    form_pages.append(page_num)\n",
    "                    # Simple confidence based on presence of token\n",
    "                    confidence_scores.append((page_num, 1.0))\n",
    "                else:\n",
    "                    confidence_scores.append((page_num, 0.0))\n",
    "    \n",
    "    return {\n",
    "        'contains_form': len(form_pages) > 0,\n",
    "        'form_pages': form_pages,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'total_pages': len(images)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_corpus(pdf_folder: Path, model, processor, device, \n",
    "                          output_file: str = 'form_detection_results.csv') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a corpus of PDF documents and save results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    pdf_files = list(pdf_folder.glob('*.pdf'))\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files):\n",
    "        try:\n",
    "            detection_result = detect_form_in_pdf(pdf_path, model, processor, device)\n",
    "            \n",
    "            results.append({\n",
    "                'filename': pdf_path.name,\n",
    "                'filepath': str(pdf_path),\n",
    "                'contains_form': detection_result['contains_form'],\n",
    "                'form_pages': ','.join(map(str, detection_result['form_pages'])),\n",
    "                'num_form_pages': len(detection_result['form_pages']),\n",
    "                'total_pages': detection_result['total_pages']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "            results.append({\n",
    "                'filename': pdf_path.name,\n",
    "                'filepath': str(pdf_path),\n",
    "                'contains_form': None,\n",
    "                'form_pages': '',\n",
    "                'num_form_pages': 0,\n",
    "                'total_pages': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Total documents processed: {len(df_results)}\")\n",
    "    print(f\"Documents with forms: {df_results['contains_form'].sum()}\")\n",
    "    print(f\"Documents without forms: {(~df_results['contains_form']).sum()}\")\n",
    "    print(f\"Processing errors: {df_results['contains_form'].isna().sum()}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test on Example Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single example file\n",
    "if len(example_files) > 0:\n",
    "    test_pdf = example_files[0]\n",
    "    print(f\"Testing on: {test_pdf.name}\")\n",
    "    \n",
    "    result = detect_form_in_pdf(test_pdf, model, processor, device)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Contains form: {result['contains_form']}\")\n",
    "    print(f\"Form pages: {result['form_pages']}\")\n",
    "    print(f\"Total pages: {result['total_pages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process Full Corpus (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process a folder of documents\n",
    "# Uncomment and modify the path to process your full corpus\n",
    "\n",
    "# CORPUS_PATH = BASE_PATH / 'data' / 'raw' / 'contracts'  # Adjust to your corpus location\n",
    "# results_df = process_document_corpus(CORPUS_PATH, model, processor, device)\n",
    "\n",
    "# # Display some results\n",
    "# print(\"\\nSample results:\")\n",
    "# print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "save_directory = \"./form_classifier_model\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save model and processor\n",
    "model.save_pretrained(save_directory)\n",
    "processor.save_pretrained(save_directory)\n",
    "\n",
    "# Save training configuration\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'task': 'administrative_form_detection',\n",
    "    'num_epochs': num_epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'best_validation_accuracy': best_val_accuracy,\n",
    "    'training_history': training_history\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_directory, 'training_config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### To use the trained model on new documents:\n",
    "\n",
    "```python\n",
    "# Load the saved model\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained('./form_classifier_model')\n",
    "processor = DonutProcessor.from_pretrained('./form_classifier_model')\n",
    "\n",
    "# Detect form in a PDF\n",
    "result = detect_form_in_pdf(pdf_path, model, processor, device)\n",
    "print(f\"Form found on pages: {result['form_pages']}\")\n",
    "```\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "1. **Use LayoutLMv3** if you need to combine visual and text features for higher accuracy\n",
    "2. **Data Augmentation**: Add rotations, noise, etc. to training images\n",
    "3. **Confidence Calibration**: Implement proper probability scores for predictions\n",
    "4. **Multi-GPU Training**: For processing the full 190k document corpus\n",
    "5. **Active Learning**: Use model uncertainty to identify which documents to manually label next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}