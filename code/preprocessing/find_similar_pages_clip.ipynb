{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Most Similar Pages Using CLIP\n",
    "\n",
    "This notebook uses CLIP embeddings to find pages in the non-examples folder that are most similar to the administrative forms in the examples folder.\n",
    "\n",
    "## Why CLIP?\n",
    "- Better visual discrimination than document-specific models\n",
    "- Trained on diverse image-text pairs\n",
    "- Should show clearer separation between forms and non-forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Paths and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = Path('/Users/admin-tascott/Documents/GitHub/chehalis')\n",
    "EXAMPLE_FORMS_PATH = BASE_PATH / 'data' / 'raw' / '_exampleforms'\n",
    "NON_EXAMPLES_PATH = BASE_PATH / 'data' / 'raw' / '_nonexamples'\n",
    "\n",
    "# Check if paths exist\n",
    "print(f\"Example forms path exists: {EXAMPLE_FORMS_PATH.exists()}\")\n",
    "print(f\"Non-examples path exists: {NON_EXAMPLES_PATH.exists()}\")\n",
    "\n",
    "# Load CLIP model\n",
    "print(\"\\nLoading CLIP model...\")\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Use only the vision model for embeddings\n",
    "vision_model = model.vision_model.to(device)\n",
    "vision_model.eval()\n",
    "\n",
    "print(f\"CLIP model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: Path, dpi: int = 150) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Convert PDF to list of PIL Images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_image(image: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Preprocess image for CLIP (224x224 RGB)\n",
    "    \"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    # CLIP expects 224x224 images\n",
    "    return image\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embedding(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract CLIP embedding for a single image\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    image = preprocess_image(image)\n",
    "    \n",
    "    # Process with CLIP processor\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values.to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    outputs = vision_model(pixel_values=pixel_values)\n",
    "    \n",
    "    # Get pooled output (CLS token)\n",
    "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "        embedding = outputs.pooler_output\n",
    "    else:\n",
    "        # Use last hidden state and pool\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Example Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for all example form pages\n",
    "example_embeddings = []\n",
    "example_metadata = []\n",
    "\n",
    "print(\"Processing example forms...\")\n",
    "example_files = list(EXAMPLE_FORMS_PATH.glob('*.pdf'))\n",
    "print(f\"Found {len(example_files)} example PDF files\")\n",
    "\n",
    "for pdf_path in tqdm(example_files, desc=\"Example PDFs\"):\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    \n",
    "    for page_num, image in enumerate(images):\n",
    "        # Extract embedding\n",
    "        embedding = extract_embedding(image)\n",
    "        example_embeddings.append(embedding)\n",
    "        \n",
    "        # Store metadata\n",
    "        example_metadata.append({\n",
    "            'file_path': str(pdf_path),\n",
    "            'filename': pdf_path.name,\n",
    "            'page_num': page_num + 1,\n",
    "            'total_pages': len(images)\n",
    "        })\n",
    "\n",
    "# Convert to numpy array\n",
    "example_embeddings = np.array(example_embeddings)\n",
    "print(f\"\\nExtracted embeddings for {len(example_embeddings)} example form pages\")\n",
    "print(f\"Embedding shape: {example_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Non-Example Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for non-example pages\n",
    "non_example_embeddings = []\n",
    "non_example_metadata = []\n",
    "non_example_images = []  # Store for visualization\n",
    "\n",
    "print(\"\\nProcessing non-example documents...\")\n",
    "non_example_files = list(NON_EXAMPLES_PATH.glob('*.pdf'))\n",
    "\n",
    "# Limit number of PDFs for memory management\n",
    "max_pdfs = 50  # Adjust as needed\n",
    "non_example_files = non_example_files[:max_pdfs]\n",
    "print(f\"Processing {len(non_example_files)} non-example PDF files\")\n",
    "\n",
    "for pdf_path in tqdm(non_example_files, desc=\"Non-example PDFs\"):\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    \n",
    "    for page_num, image in enumerate(images):\n",
    "        # Extract embedding\n",
    "        embedding = extract_embedding(image)\n",
    "        non_example_embeddings.append(embedding)\n",
    "        \n",
    "        # Store metadata\n",
    "        non_example_metadata.append({\n",
    "            'file_path': str(pdf_path),\n",
    "            'filename': pdf_path.name,\n",
    "            'page_num': page_num + 1,\n",
    "            'total_pages': len(images)\n",
    "        })\n",
    "        \n",
    "        # Store resized image for visualization\n",
    "        non_example_images.append(preprocess_image(image))\n",
    "\n",
    "# Convert to numpy array\n",
    "non_example_embeddings = np.array(non_example_embeddings)\n",
    "print(f\"\\nExtracted embeddings for {len(non_example_embeddings)} non-example pages\")\n",
    "print(f\"Embedding shape: {non_example_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between all example and non-example pages\n",
    "print(\"\\nComputing similarity scores...\")\n",
    "similarity_matrix = cosine_similarity(non_example_embeddings, example_embeddings)\n",
    "\n",
    "# Find the maximum similarity for each non-example page\n",
    "max_similarities = similarity_matrix.max(axis=1)\n",
    "most_similar_example_idx = similarity_matrix.argmax(axis=1)\n",
    "\n",
    "# Calculate statistics\n",
    "print(f\"\\nSimilarity Statistics:\")\n",
    "print(f\"Mean similarity: {max_similarities.mean():.4f}\")\n",
    "print(f\"Std deviation: {max_similarities.std():.4f}\")\n",
    "print(f\"Min similarity: {max_similarities.min():.4f}\")\n",
    "print(f\"Max similarity: {max_similarities.max():.4f}\")\n",
    "\n",
    "# Find the overall most similar non-example page\n",
    "most_similar_idx = max_similarities.argmax()\n",
    "most_similar_score = max_similarities[most_similar_idx]\n",
    "most_similar_example = most_similar_example_idx[most_similar_idx]\n",
    "\n",
    "print(f\"\\nMost similar non-example page:\")\n",
    "print(f\"  File: {non_example_metadata[most_similar_idx]['filename']}\")\n",
    "print(f\"  Page: {non_example_metadata[most_similar_idx]['page_num']}\")\n",
    "print(f\"  Similarity score: {most_similar_score:.4f}\")\n",
    "print(f\"  Most similar to example: {example_metadata[most_similar_example]['filename']}, page {example_metadata[most_similar_example]['page_num']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze High Similarity Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ALL non-example pages with similarity > 0.9\n",
    "high_similarity_threshold = 0.9\n",
    "high_similarity_indices = np.where(max_similarities > high_similarity_threshold)[0]\n",
    "\n",
    "print(f\"\\nFound {len(high_similarity_indices)} non-example pages with similarity > {high_similarity_threshold}\")\n",
    "print(f\"That's {len(high_similarity_indices) / len(max_similarities) * 100:.1f}% of all non-example pages!\")\n",
    "\n",
    "if len(high_similarity_indices) > 0:\n",
    "    print(\"\\nHigh similarity non-example pages:\")\n",
    "    # Sort by similarity score (highest first)\n",
    "    sorted_indices = high_similarity_indices[np.argsort(max_similarities[high_similarity_indices])[::-1]]\n",
    "    \n",
    "    # Show details for top matches (limit to 20 for readability)\n",
    "    for idx in sorted_indices[:20]:\n",
    "        similar_example_idx = most_similar_example_idx[idx]\n",
    "        print(f\"\\n  File: {non_example_metadata[idx]['filename']}, Page: {non_example_metadata[idx]['page_num']}\")\n",
    "        print(f\"  Similarity: {max_similarities[idx]:.4f}\")\n",
    "        print(f\"  Similar to: {example_metadata[similar_example_idx]['filename']}, page {example_metadata[similar_example_idx]['page_num']}\")\n",
    "    \n",
    "    if len(high_similarity_indices) > 20:\n",
    "        print(f\"\\n  ... and {len(high_similarity_indices) - 20} more pages with similarity > {high_similarity_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find Top K Most Similar Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top K most similar non-example pages\n",
    "K = 10\n",
    "top_k_indices = np.argsort(max_similarities)[-K:][::-1]\n",
    "\n",
    "print(f\"\\nTop {K} most similar non-example pages:\")\n",
    "results = []\n",
    "\n",
    "for rank, idx in enumerate(top_k_indices):\n",
    "    metadata = non_example_metadata[idx]\n",
    "    similar_example_idx = most_similar_example_idx[idx]\n",
    "    similar_example = example_metadata[similar_example_idx]\n",
    "    \n",
    "    result = {\n",
    "        'rank': rank + 1,\n",
    "        'filename': metadata['filename'],\n",
    "        'page': metadata['page_num'],\n",
    "        'similarity_score': max_similarities[idx],\n",
    "        'similar_to_example': similar_example['filename'],\n",
    "        'similar_to_page': similar_example['page_num']\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\n{rank + 1}. File: {metadata['filename']}, Page: {metadata['page_num']}\")\n",
    "    print(f\"   Similarity: {max_similarities[idx]:.4f}\")\n",
    "    print(f\"   Most similar to: {similar_example['filename']}, page {similar_example['page_num']}\")\n",
    "\n",
    "# Create DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Most Similar Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 5 most similar non-example pages\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "fig.suptitle('Top 5 Most Similar Non-Example Pages (Using CLIP)', fontsize=16)\n",
    "\n",
    "for i, idx in enumerate(top_k_indices[:5]):\n",
    "    # Non-example page\n",
    "    ax_non = axes[0, i]\n",
    "    # Resize for display\n",
    "    display_img = non_example_images[idx]\n",
    "    if display_img.size[0] > 400 or display_img.size[1] > 400:\n",
    "        display_img.thumbnail((400, 400), Image.Resampling.LANCZOS)\n",
    "    ax_non.imshow(display_img)\n",
    "    ax_non.set_title(f\"Non-Example\\n{non_example_metadata[idx]['filename']}\\nPage {non_example_metadata[idx]['page_num']}\\nScore: {max_similarities[idx]:.3f}\")\n",
    "    ax_non.axis('off')\n",
    "    \n",
    "    # Most similar example page\n",
    "    similar_example_idx = most_similar_example_idx[idx]\n",
    "    example_meta = example_metadata[similar_example_idx]\n",
    "    \n",
    "    # Load the example image for visualization\n",
    "    example_pdf_path = Path(example_meta['file_path'])\n",
    "    example_images = pdf_to_images(example_pdf_path)\n",
    "    if example_meta['page_num'] <= len(example_images):\n",
    "        example_image = preprocess_image(example_images[example_meta['page_num'] - 1])\n",
    "        if example_image.size[0] > 400 or example_image.size[1] > 400:\n",
    "            example_image.thumbnail((400, 400), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        ax_ex = axes[1, i]\n",
    "        ax_ex.imshow(example_image)\n",
    "        ax_ex.set_title(f\"Similar Example\\n{example_meta['filename']}\\nPage {example_meta['page_num']}\")\n",
    "        ax_ex.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Similarity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of similarity scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(max_similarities, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=most_similar_score, color='red', linestyle='--', label=f'Most similar: {most_similar_score:.3f}')\n",
    "plt.axvline(x=0.9, color='orange', linestyle='--', label='Threshold: 0.9')\n",
    "plt.xlabel('Maximum Cosine Similarity Score')\n",
    "plt.ylabel('Number of Non-Example Pages')\n",
    "plt.title('Distribution of Similarity Scores (CLIP)\\n(Non-Example Pages vs Most Similar Example Form)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print percentile statistics\n",
    "print(\"\\nPercentile Statistics:\")\n",
    "for p in [50, 75, 90, 95, 99]:\n",
    "    print(f\"{p}th percentile: {np.percentile(max_similarities, p):.4f}\")\n",
    "\n",
    "print(f\"\\nPages with similarity > 0.9: {(max_similarities > 0.9).sum()}\")\n",
    "print(f\"Pages with similarity > 0.8: {(max_similarities > 0.8).sum()}\")\n",
    "print(f\"Pages with similarity > 0.7: {(max_similarities > 0.7).sum()}\")\n",
    "print(f\"Pages with similarity < 0.5: {(max_similarities < 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to CSV\n",
    "all_results = []\n",
    "\n",
    "for idx, (score, example_idx) in enumerate(zip(max_similarities, most_similar_example_idx)):\n",
    "    metadata = non_example_metadata[idx]\n",
    "    similar_example = example_metadata[example_idx]\n",
    "    \n",
    "    all_results.append({\n",
    "        'non_example_file': metadata['filename'],\n",
    "        'non_example_page': metadata['page_num'],\n",
    "        'non_example_total_pages': metadata['total_pages'],\n",
    "        'similarity_score': score,\n",
    "        'most_similar_example_file': similar_example['filename'],\n",
    "        'most_similar_example_page': similar_example['page_num']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values('similarity_score', ascending=False)\n",
    "results_df.to_csv('clip_similarity_results.csv', index=False)\n",
    "\n",
    "print(f\"\\nResults saved to clip_similarity_results.csv\")\n",
    "print(f\"\\nTop 10 most similar pages:\")\n",
    "display(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Identify Potential Misclassified Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pages with very high similarity might be misclassified\n",
    "threshold = 0.85  # Adjust based on your results\n",
    "potential_misclassified = results_df[results_df['similarity_score'] > threshold]\n",
    "\n",
    "print(f\"\\nPotential misclassified pages (similarity > {threshold}):\")\n",
    "print(f\"Found {len(potential_misclassified)} pages\")\n",
    "\n",
    "if len(potential_misclassified) > 0:\n",
    "    print(\"\\nThese non-example pages are very similar to the administrative forms:\")\n",
    "    for _, row in potential_misclassified.iterrows():\n",
    "        print(f\"\\n- File: {row['non_example_file']}, Page: {row['non_example_page']}\")\n",
    "        print(f\"  Similarity: {row['similarity_score']:.4f}\")\n",
    "        print(f\"  Similar to: {row['most_similar_example_file']}, page {row['most_similar_example_page']}\")\n",
    "    \n",
    "    # Save potential misclassified pages\n",
    "    potential_misclassified.to_csv('potential_misclassified_pages_clip.csv', index=False)\n",
    "    print(\"\\nSaved to potential_misclassified_pages_clip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook uses CLIP to find similar pages. If you're still seeing high similarity scores (>0.9) for many non-example pages, consider:\n",
    "\n",
    "1. **Visual Inspection**: Look at the high-scoring pages - they might actually be forms\n",
    "2. **Different Models**: Try other vision models like DINOv2 or newer CLIP variants\n",
    "3. **Feature Engineering**: Extract specific features (text density, layout structure)\n",
    "4. **Fine-tuning**: Fine-tune CLIP on your specific document types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}